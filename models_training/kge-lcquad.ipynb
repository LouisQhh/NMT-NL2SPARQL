{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w2wNk0167DQ"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1688940686577,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "6ZH1UZrpjtPE",
    "outputId": "ce741db7-131d-487b-8756-9748a28cdea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'nmt'\n",
    "!mkdir nmt\n",
    "\n",
    "# Change the current working directory to the newly created 'nmt' directory\n",
    "%cd nmt\n",
    "\n",
    "# Create a subdirectory named 'nmtmodel' inside the 'nmt' directory\n",
    "!mkdir nmtmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98193,
     "status": "ok",
     "timestamp": 1688940790614,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "df6W9IIWmlQL",
    "outputId": "833ea8be-93fe-498a-d862-2abca368baa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install OpenNMT-py along with specific versions of torchvision and torchaudio\n",
    "! pip install OpenNMT-py torchvision==0.14.1 torchaudio==0.13.1 > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1688940790615,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "_3WnsZ-1nUSj",
    "outputId": "f182e6bf-0703-41c6-de81-cfa998e17c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QypsRuOq7IiR"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1430,
     "status": "ok",
     "timestamp": 1688940792041,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "Jsc7THqZnYCI",
    "outputId": "8314780b-2aba-4f02-9997-dba3e636a90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./lcquad.zip\n",
      "   creating: ./lcquad/\n",
      "  inflating: ./__MACOSX/._lcquad     \n",
      "  inflating: ./lcquad/dev.en         \n",
      "  inflating: ./__MACOSX/lcquad/._dev.en  \n",
      "  inflating: ./lcquad/dev.sparql     \n",
      "  inflating: ./__MACOSX/lcquad/._dev.sparql  \n",
      "  inflating: ./lcquad/.DS_Store      \n",
      "  inflating: ./__MACOSX/lcquad/._.DS_Store  \n",
      "  inflating: ./lcquad/train.sparql   \n",
      "  inflating: ./__MACOSX/lcquad/._train.sparql  \n",
      "  inflating: ./lcquad/train.en       \n",
      "  inflating: ./__MACOSX/lcquad/._train.en  \n",
      "  inflating: ./lcquad/test.sparql    \n",
      "  inflating: ./__MACOSX/lcquad/._test.sparql  \n",
      "  inflating: ./lcquad/test.en        \n",
      "  inflating: ./__MACOSX/lcquad/._test.en  \n"
     ]
    }
   ],
   "source": [
    "# Copy the file 'lcquad.zip' from '/content/drive/MyDrive/' to the current directory\n",
    "!cp /content/drive/MyDrive/lcquad.zip ./\n",
    "\n",
    "# Unzip the file 'lcquad.zip' and extract its contents to the current directory\n",
    "!unzip ./lcquad.zip -d ./\n",
    "\n",
    "# Remove the original zip file 'lcquad.zip' from '/content/nmt/'\n",
    "!rm /content/nmt/lcquad.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688940792041,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "dbGofKann5xB",
    "outputId": "d5d95707-fcea-42cf-9f27-a30bb4d27520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcquad\t__MACOSX  nmtmodel\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the current directory\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZiY1mojsEkl"
   },
   "source": [
    "# Knowledge Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 459201,
     "status": "ok",
     "timestamp": 1688941251239,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "YTD24q1UsEPM"
   },
   "outputs": [],
   "source": [
    "# Create a directory named \"graph_embedding_dir\" using the mkdir command\n",
    "!mkdir \"graph_embedding_dir\"\n",
    "\n",
    "# Copy the file \"embedding.vec\" from the specified source path in Google Drive\n",
    "# to the destination path \"/content/nmt/graph_embedding_dir\"\n",
    "!cp /content/drive/MyDrive/embedding.vec /content/nmt/graph_embedding_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Nfw0l77P8e"
   },
   "source": [
    "# Create the Training Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688941251240,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9uBY92Mf8vE6"
   },
   "outputs": [],
   "source": [
    "model_root = '/content/nmt/nmtmodel'\n",
    "\n",
    "# Create a directory named 'nmtmodel' using the model_root path\n",
    "!mkdir -p '{model_root}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688941251240,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "xAJ686rIvLRn"
   },
   "outputs": [],
   "source": [
    "# Define the configuration as a formatted string\n",
    "config = f'''# config.yaml\n",
    "# GloVe:\n",
    "# this means embeddings will be used for both encoder and decoder sides\n",
    "both_embeddings: /content/nmt/graph_embedding_dir/embedding.vec\n",
    "\n",
    "# supported types: GloVe, word2vec\n",
    "embeddings_type: \"word2vec\"\n",
    "\n",
    "# word_vec_size need to match with the pretrained embeddings dimensions\n",
    "word_vec_size: 300\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: {model_root}\n",
    "\n",
    "## Where the vocab(s) will be written\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: {model_root}/src.vocab\n",
    "tgt_vocab: {model_root}/src.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 11000\n",
    "tgt_vocab_size: 11000\n",
    "share_vocab: true\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    train:\n",
    "        path_src: /content/nmt/lcquad/train.en\n",
    "        path_tgt: /content/nmt/lcquad/train.sparql\n",
    "    valid:\n",
    "        path_src: /content/nmt/lcquad/dev.en\n",
    "        path_tgt: /content/nmt/lcquad/dev.sparql\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: {model_root}/model\n",
    "log_file: {model_root}/train.log\n",
    "save_checkpoint_steps: 100\n",
    "train_steps: 1200\n",
    "valid_steps: 400\n",
    "\n",
    "# Stop training if it does not imporve after n validations\n",
    "early_stopping: 4\n",
    "\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 4242\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "# queue_size: 100\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "# world_size: 1\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "# model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "# learning_rate: 2\n",
    "warmup_steps: 500\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.98\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "# dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "'''\n",
    "\n",
    "# Write the configuration to a \"config.yaml\" file\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1ZGev3N7ao3"
   },
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4301,
     "status": "ok",
     "timestamp": 1688941255536,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "F4jDz0Dr7sne",
    "outputId": "118888e7-9b48-4bc0-ef97-f3e1c5015bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 22:20:53,882 INFO] Counter vocab from -1 samples.\n",
      "[2023-07-09 22:20:53,882 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2023-07-09 22:20:54,012 INFO] Counters src: 6405\n",
      "[2023-07-09 22:20:54,012 INFO] Counters tgt: 4412\n",
      "[2023-07-09 22:20:54,014 INFO] Counters after share:10760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the source vocabulary file doesn't exist in the 'model_root' directory\n",
    "if not os.path.exists(os.path.join(model_root, 'src.vocab')):\n",
    "    # Build the source vocabulary using the onmt_build_vocab command and the provided config.yaml\n",
    "    # The --n_sample option is used to indicate the number of samples to consider for building the vocabulary\n",
    "    # The \"|| true\" at the end ensures that the command won't stop the script even if there's an error\n",
    "    !onmt_build_vocab -config config.yaml --n_sample -1 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V312rtxK7pd7"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2831,
     "status": "ok",
     "timestamp": 1688941258364,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tJ4dYNPtvQ5Q",
    "outputId": "3e3b9ff0-8ca8-4f32-f29a-64e3c1cc415a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  9 22:20:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "GPU:\n",
      "GPU 0: Tesla T4 (UUID: GPU-3ed68eca-bfa6-c94d-a066-00d50364d937)\n",
      "$*****************************************************************************$\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "True\n",
      "Tesla T4\n",
      "Free GPU memory: 14998.8125 out of: 15101.8125\n",
      "$*****************************************************************************$\n",
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 20.04.6 LTS\n",
      "Release:\t20.04\n",
      "Codename:\tfocal\n",
      "$*****************************************************************************$\n",
      "5.15.107+\n",
      "$*****************************************************************************$\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "$*****************************************************************************$\n",
      "1.13.1+cu117\n",
      "$*****************************************************************************$\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "$*****************************************************************************$\n",
      "MemTotal:       26687700 kB\n"
     ]
    }
   ],
   "source": [
    "# Check NVIDIA GPU information using the 'nvidia-smi' command\n",
    "!nvidia-smi\n",
    "\n",
    "# Print a separator line and heading for the GPU information\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "print('GPU:')\n",
    "\n",
    "# Check and display the GPU devices using 'nvidia-smi -L'\n",
    "!nvidia-smi -L\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print a separator line and heading for GPU-related checks\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "\n",
    "# Check if CUDA-enabled GPU is available for PyTorch\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Print the name of the first CUDA-enabled GPU\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Get GPU memory information\n",
    "gpu_memory = torch.cuda.mem_get_info(0)\n",
    "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print system information using 'lsb_release -a'\n",
    "!lsb_release -a\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print kernel version using 'uname -r'\n",
    "!uname -r\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print NVCC (NVIDIA CUDA Compiler) version using 'nvcc --version'\n",
    "!nvcc --version\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Check Torch version using Python import\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display CPU information using 'cat /proc/cpuinfo | grep model\\ name'\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display total memory information using 'cat /proc/meminfo | grep MemTotal'\n",
    "!cat /proc/meminfo | grep MemTotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmaMw_sc7gvP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2275318,
     "status": "ok",
     "timestamp": 1688943533680,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "5iMmswvHvta_",
    "outputId": "831c7806-533c-4ee7-d015-a012c99d14c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 22:20:58,832 INFO] Missing transforms field for train data, set to default: [].\n",
      "[2023-07-09 22:20:58,832 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 22:20:58,832 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2023-07-09 22:20:58,833 INFO] Parsed 2 corpora from -data.\n",
      "[2023-07-09 22:20:58,833 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2023-07-09 22:20:58,881 INFO] Reading encoder and decoder embeddings from /content/nmt/graph_embedding_dir/embedding.vec\n",
      "[2023-07-09 22:23:53,524 INFO] \tFound 8541203 total vectors in file\n",
      "[2023-07-09 22:23:53,524 INFO] After filtering to vectors in vocab:\n",
      "[2023-07-09 22:23:53,528 INFO] \t* enc: 4655 match, 6113 missing, (43.23%)\n",
      "[2023-07-09 22:23:53,530 INFO] \t* dec: 4655 match, 6113 missing, (43.23%)\n",
      "[2023-07-09 22:23:53,530 INFO] \n",
      "Saving encoder embeddings as:\n",
      "\t* enc: /content/nmt/nmtmodel.enc_embeddings.pt\n",
      "[2023-07-09 22:24:01,667 INFO] \n",
      "Saving decoder embeddings as:\n",
      "\t* dec: /content/nmt/nmtmodel.dec_embeddings.pt\n",
      "[2023-07-09 22:24:09,814 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'var_uri', 'sep_dot', 'WHERE', 'brack_open', 'brack_close', 'the']\n",
      "[2023-07-09 22:24:09,814 INFO] The decoder start token is: <s>\n",
      "[2023-07-09 22:24:09,814 INFO] Building model...\n",
      "[2023-07-09 22:24:10,521 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2023-07-09 22:24:10,521 INFO] Non quantized layer compute is fp32\n",
      "[2023-07-09 22:24:14,052 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10768, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10768, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=10768, bias=True)\n",
      ")\n",
      "[2023-07-09 22:24:14,148 INFO] encoder: 24400896\n",
      "[2023-07-09 22:24:14,148 INFO] decoder: 36222480\n",
      "[2023-07-09 22:24:14,148 INFO] * number of parameters: 60623376\n",
      "[2023-07-09 22:24:14,148 INFO] Trainable parameters = {'torch.float32': 60623376, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 22:24:14,148 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 22:24:14,148 INFO]  * src vocab size = 10768\n",
      "[2023-07-09 22:24:14,148 INFO]  * tgt vocab size = 10768\n",
      "[2023-07-09 22:24:14,151 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 1\n",
      "[2023-07-09 22:24:14,151 INFO] Starting training on GPU: [0]\n",
      "[2023-07-09 22:24:14,151 INFO] Start training loop and validate every 400 steps...\n",
      "[2023-07-09 22:24:14,151 INFO] Scoring with: TransformPipe()\n",
      "[2023-07-09 22:24:14,216 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 2\n",
      "[2023-07-09 22:24:14,277 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 3\n",
      "[2023-07-09 22:24:14,339 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 4\n",
      "[2023-07-09 22:24:14,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 5\n",
      "[2023-07-09 22:24:14,533 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 6\n",
      "[2023-07-09 22:24:14,597 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 7\n",
      "[2023-07-09 22:24:14,776 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 8\n",
      "[2023-07-09 22:24:14,842 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 9\n",
      "[2023-07-09 22:24:14,916 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 10\n",
      "[2023-07-09 22:24:14,981 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 11\n",
      "[2023-07-09 22:24:15,212 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 12\n",
      "[2023-07-09 22:24:15,276 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 13\n",
      "[2023-07-09 22:24:15,344 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 14\n",
      "[2023-07-09 22:24:15,407 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 15\n",
      "[2023-07-09 22:24:15,473 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 16\n",
      "[2023-07-09 22:24:15,754 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 17\n",
      "[2023-07-09 22:24:15,823 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 18\n",
      "[2023-07-09 22:24:15,889 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 19\n",
      "[2023-07-09 22:24:15,953 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 20\n",
      "[2023-07-09 22:24:16,021 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 21\n",
      "[2023-07-09 22:24:16,371 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 22\n",
      "[2023-07-09 22:24:16,434 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 23\n",
      "[2023-07-09 22:24:16,498 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 24\n",
      "[2023-07-09 22:24:16,562 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 25\n",
      "[2023-07-09 22:24:16,626 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 26\n",
      "[2023-07-09 22:24:16,692 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 27\n",
      "[2023-07-09 22:24:16,758 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 28\n",
      "[2023-07-09 22:24:17,200 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 29\n",
      "[2023-07-09 22:24:17,271 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 30\n",
      "[2023-07-09 22:24:17,334 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 31\n",
      "[2023-07-09 22:24:17,405 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 32\n",
      "[2023-07-09 22:24:17,475 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 33\n",
      "[2023-07-09 22:24:17,541 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 34\n",
      "[2023-07-09 22:24:17,612 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 35\n",
      "[2023-07-09 22:24:17,680 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 36\n",
      "[2023-07-09 22:24:18,237 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 37\n",
      "[2023-07-09 22:24:18,301 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 38\n",
      "[2023-07-09 22:24:18,363 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 39\n",
      "[2023-07-09 22:24:18,426 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 40\n",
      "[2023-07-09 22:24:18,489 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 41\n",
      "[2023-07-09 22:24:18,551 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 42\n",
      "[2023-07-09 22:24:18,615 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 43\n",
      "[2023-07-09 22:24:18,677 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 44\n",
      "[2023-07-09 22:24:18,741 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 45\n",
      "[2023-07-09 22:24:18,805 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 46\n",
      "[2023-07-09 22:24:19,461 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 47\n",
      "[2023-07-09 22:24:19,524 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 48\n",
      "[2023-07-09 22:24:19,587 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 49\n",
      "[2023-07-09 22:24:19,649 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 50\n",
      "[2023-07-09 22:24:19,714 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 51\n",
      "[2023-07-09 22:24:19,777 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 52\n",
      "[2023-07-09 22:24:19,840 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 53\n",
      "[2023-07-09 22:24:19,903 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 54\n",
      "[2023-07-09 22:24:19,965 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 55\n",
      "[2023-07-09 22:24:20,029 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 56\n",
      "[2023-07-09 22:24:20,095 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 57\n",
      "[2023-07-09 22:24:20,160 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 58\n",
      "[2023-07-09 22:24:20,970 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 59\n",
      "[2023-07-09 22:24:21,033 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 60\n",
      "[2023-07-09 22:24:21,096 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 61\n",
      "[2023-07-09 22:24:21,160 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 62\n",
      "[2023-07-09 22:24:21,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 63\n",
      "[2023-07-09 22:24:21,290 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 64\n",
      "[2023-07-09 22:24:21,357 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 65\n",
      "[2023-07-09 22:24:21,420 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 66\n",
      "[2023-07-09 22:25:43,860 INFO] Step 50/ 1200; acc: 20.8; ppl: 1149.1; xent: 7.0; lr: 0.00020; sents:   49691; bsz: 2987/3809/248; 6659/8491 tok/s;     90 sec;\n",
      "[2023-07-09 22:27:00,482 INFO] Step 100/ 1200; acc: 63.6; ppl:  30.7; xent: 3.4; lr: 0.00040; sents:   51329; bsz: 3008/3808/257; 7853/9940 tok/s;    166 sec;\n",
      "[2023-07-09 22:27:00,485 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_100.pt\n",
      "[2023-07-09 22:28:20,319 INFO] Step 150/ 1200; acc: 75.7; ppl:  13.7; xent: 2.6; lr: 0.00060; sents:   50756; bsz: 2998/3786/254; 7510/9485 tok/s;    246 sec;\n",
      "[2023-07-09 22:29:22,870 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 67\n",
      "[2023-07-09 22:29:22,936 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 68\n",
      "[2023-07-09 22:29:23,000 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 69\n",
      "[2023-07-09 22:29:23,064 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 70\n",
      "[2023-07-09 22:29:23,128 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 71\n",
      "[2023-07-09 22:29:23,199 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 72\n",
      "[2023-07-09 22:29:23,264 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 73\n",
      "[2023-07-09 22:29:23,329 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 74\n",
      "[2023-07-09 22:29:23,393 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 75\n",
      "[2023-07-09 22:29:23,459 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 76\n",
      "[2023-07-09 22:29:23,525 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 77\n",
      "[2023-07-09 22:29:23,588 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 78\n",
      "[2023-07-09 22:29:23,658 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 79\n",
      "[2023-07-09 22:29:23,727 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 80\n",
      "[2023-07-09 22:29:23,810 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 81\n",
      "[2023-07-09 22:29:23,877 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 82\n",
      "[2023-07-09 22:29:23,944 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 83\n",
      "[2023-07-09 22:29:24,015 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 84\n",
      "[2023-07-09 22:29:24,088 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 85\n",
      "[2023-07-09 22:29:24,160 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 86\n",
      "[2023-07-09 22:29:24,222 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 87\n",
      "[2023-07-09 22:29:26,447 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 88\n",
      "[2023-07-09 22:29:26,509 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 89\n",
      "[2023-07-09 22:29:26,570 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 90\n",
      "[2023-07-09 22:29:26,631 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 91\n",
      "[2023-07-09 22:29:26,695 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 92\n",
      "[2023-07-09 22:29:26,756 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 93\n",
      "[2023-07-09 22:29:26,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 94\n",
      "[2023-07-09 22:29:26,879 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 95\n",
      "[2023-07-09 22:29:26,941 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 96\n",
      "[2023-07-09 22:29:27,006 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 97\n",
      "[2023-07-09 22:29:27,070 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 98\n",
      "[2023-07-09 22:29:27,133 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 99\n",
      "[2023-07-09 22:29:27,203 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 100\n",
      "[2023-07-09 22:29:27,268 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 101\n",
      "[2023-07-09 22:29:27,333 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 102\n",
      "[2023-07-09 22:29:27,399 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 103\n",
      "[2023-07-09 22:29:27,472 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 104\n",
      "[2023-07-09 22:29:27,542 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 105\n",
      "[2023-07-09 22:29:27,608 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 106\n",
      "[2023-07-09 22:29:27,670 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 107\n",
      "[2023-07-09 22:29:27,732 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 108\n",
      "[2023-07-09 22:29:27,802 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 109\n",
      "[2023-07-09 22:29:27,873 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 110\n",
      "[2023-07-09 22:29:27,937 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 111\n",
      "[2023-07-09 22:29:28,008 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 112\n",
      "[2023-07-09 22:29:30,605 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 113\n",
      "[2023-07-09 22:29:30,665 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 114\n",
      "[2023-07-09 22:29:30,727 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 115\n",
      "[2023-07-09 22:29:30,789 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 116\n",
      "[2023-07-09 22:29:30,852 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 117\n",
      "[2023-07-09 22:29:30,915 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 118\n",
      "[2023-07-09 22:29:30,977 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 119\n",
      "[2023-07-09 22:29:31,040 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 120\n",
      "[2023-07-09 22:29:31,102 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 121\n",
      "[2023-07-09 22:29:31,165 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 122\n",
      "[2023-07-09 22:29:31,228 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 123\n",
      "[2023-07-09 22:29:31,291 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 124\n",
      "[2023-07-09 22:29:31,356 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 125\n",
      "[2023-07-09 22:29:31,420 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 126\n",
      "[2023-07-09 22:29:31,483 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 127\n",
      "[2023-07-09 22:29:31,546 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 128\n",
      "[2023-07-09 22:29:31,609 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 129\n",
      "[2023-07-09 22:29:31,674 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 130\n",
      "[2023-07-09 22:29:31,738 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 131\n",
      "[2023-07-09 22:29:31,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 132\n",
      "[2023-07-09 22:29:56,880 INFO] Step 200/ 1200; acc: 82.4; ppl:   8.6; xent: 2.2; lr: 0.00079; sents:   52013; bsz: 3013/3799/260; 6240/7869 tok/s;    343 sec;\n",
      "[2023-07-09 22:29:56,883 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_200.pt\n",
      "[2023-07-09 22:31:17,647 INFO] Step 250/ 1200; acc: 94.9; ppl:   4.9; xent: 1.6; lr: 0.00099; sents:   49398; bsz: 2988/3797/247; 7399/9402 tok/s;    423 sec;\n",
      "[2023-07-09 22:32:38,771 INFO] Step 300/ 1200; acc: 98.8; ppl:   3.9; xent: 1.4; lr: 0.00119; sents:   50742; bsz: 3009/3808/254; 7419/9387 tok/s;    505 sec;\n",
      "[2023-07-09 22:32:38,774 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_300.pt\n",
      "[2023-07-09 22:34:01,571 INFO] Step 350/ 1200; acc: 99.7; ppl:   3.7; xent: 1.3; lr: 0.00139; sents:   50411; bsz: 2987/3811/252; 7214/9204 tok/s;    587 sec;\n",
      "[2023-07-09 22:34:45,455 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 133\n",
      "[2023-07-09 22:34:45,516 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 134\n",
      "[2023-07-09 22:34:45,575 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 135\n",
      "[2023-07-09 22:34:45,637 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 136\n",
      "[2023-07-09 22:34:47,772 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 137\n",
      "[2023-07-09 22:34:47,834 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 138\n",
      "[2023-07-09 22:34:47,896 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 139\n",
      "[2023-07-09 22:34:47,958 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 140\n",
      "[2023-07-09 22:34:48,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 141\n",
      "[2023-07-09 22:34:48,082 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 142\n",
      "[2023-07-09 22:34:48,144 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 143\n",
      "[2023-07-09 22:34:48,206 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 144\n",
      "[2023-07-09 22:34:48,270 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 145\n",
      "[2023-07-09 22:34:48,332 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 146\n",
      "[2023-07-09 22:34:48,396 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 147\n",
      "[2023-07-09 22:34:48,460 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 148\n",
      "[2023-07-09 22:34:48,519 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 149\n",
      "[2023-07-09 22:34:48,580 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 150\n",
      "[2023-07-09 22:34:48,641 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 151\n",
      "[2023-07-09 22:34:48,700 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 152\n",
      "[2023-07-09 22:34:48,760 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 153\n",
      "[2023-07-09 22:34:48,824 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 154\n",
      "[2023-07-09 22:34:48,883 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 155\n",
      "[2023-07-09 22:34:48,941 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 156\n",
      "[2023-07-09 22:34:49,000 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 157\n",
      "[2023-07-09 22:34:51,510 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 158\n",
      "[2023-07-09 22:34:51,570 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 159\n",
      "[2023-07-09 22:34:51,629 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 160\n",
      "[2023-07-09 22:34:51,688 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 161\n",
      "[2023-07-09 22:34:51,746 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 162\n",
      "[2023-07-09 22:34:51,806 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 163\n",
      "[2023-07-09 22:34:51,865 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 164\n",
      "[2023-07-09 22:34:51,925 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 165\n",
      "[2023-07-09 22:34:51,984 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 166\n",
      "[2023-07-09 22:34:52,053 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 167\n",
      "[2023-07-09 22:34:52,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 168\n",
      "[2023-07-09 22:34:52,177 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 169\n",
      "[2023-07-09 22:34:52,238 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 170\n",
      "[2023-07-09 22:34:52,296 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 171\n",
      "[2023-07-09 22:34:52,355 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 172\n",
      "[2023-07-09 22:34:52,415 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 173\n",
      "[2023-07-09 22:34:52,476 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 174\n",
      "[2023-07-09 22:34:52,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 175\n",
      "[2023-07-09 22:34:52,593 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 176\n",
      "[2023-07-09 22:34:52,651 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 177\n",
      "[2023-07-09 22:34:52,711 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 178\n",
      "[2023-07-09 22:34:52,771 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 179\n",
      "[2023-07-09 22:34:52,831 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 180\n",
      "[2023-07-09 22:34:52,890 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 181\n",
      "[2023-07-09 22:34:52,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 182\n",
      "[2023-07-09 22:34:53,007 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 183\n",
      "[2023-07-09 22:34:53,067 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 184\n",
      "[2023-07-09 22:34:55,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 185\n",
      "[2023-07-09 22:34:55,991 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 186\n",
      "[2023-07-09 22:34:56,052 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 187\n",
      "[2023-07-09 22:34:56,112 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 188\n",
      "[2023-07-09 22:34:56,171 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 189\n",
      "[2023-07-09 22:34:56,231 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 190\n",
      "[2023-07-09 22:34:56,293 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 191\n",
      "[2023-07-09 22:34:56,358 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 192\n",
      "[2023-07-09 22:34:56,421 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 193\n",
      "[2023-07-09 22:34:56,501 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 194\n",
      "[2023-07-09 22:34:56,563 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 195\n",
      "[2023-07-09 22:34:56,620 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 196\n",
      "[2023-07-09 22:34:56,682 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 197\n",
      "[2023-07-09 22:35:42,981 INFO] Step 400/ 1200; acc: 96.3; ppl:   4.4; xent: 1.5; lr: 0.00159; sents:   50562; bsz: 2985/3780/253; 5887/7454 tok/s;    689 sec;\n",
      "[2023-07-09 22:35:43,469 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.48650336265563965 s.\n",
      "[2023-07-09 22:35:43,469 INFO] Train perplexity: 13.7643\n",
      "[2023-07-09 22:35:43,470 INFO] Train accuracy: 79.0208\n",
      "[2023-07-09 22:35:43,470 INFO] Sentences processed: 404902\n",
      "[2023-07-09 22:35:43,470 INFO] Average bsz: 2997/3800/253\n",
      "[2023-07-09 22:35:43,470 INFO] Validation perplexity: 14.0132\n",
      "[2023-07-09 22:35:43,470 INFO] Validation accuracy: 77.6983\n",
      "[2023-07-09 22:35:43,470 INFO] Model is improving ppl: inf --> 14.0132.\n",
      "[2023-07-09 22:35:43,470 INFO] Model is improving acc: -inf --> 77.6983.\n",
      "[2023-07-09 22:35:43,472 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_400.pt\n",
      "[2023-07-09 22:37:05,874 INFO] Step 450/ 1200; acc: 99.5; ppl:   3.7; xent: 1.3; lr: 0.00178; sents:   49962; bsz: 2993/3808/250; 7221/9189 tok/s;    772 sec;\n",
      "[2023-07-09 22:38:26,811 INFO] Step 500/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00197; sents:   51049; bsz: 3000/3804/255; 7414/9399 tok/s;    853 sec;\n",
      "[2023-07-09 22:38:26,814 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_500.pt\n",
      "[2023-07-09 22:39:49,516 INFO] Step 550/ 1200; acc: 99.5; ppl:   3.7; xent: 1.3; lr: 0.00188; sents:   51418; bsz: 3006/3804/257; 7268/9200 tok/s;    935 sec;\n",
      "[2023-07-09 22:40:15,463 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 198\n",
      "[2023-07-09 22:40:15,526 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 199\n",
      "[2023-07-09 22:40:15,588 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 200\n",
      "[2023-07-09 22:40:15,650 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 201\n",
      "[2023-07-09 22:40:15,713 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 202\n",
      "[2023-07-09 22:40:15,777 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 203\n",
      "[2023-07-09 22:40:15,839 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 204\n",
      "[2023-07-09 22:40:15,901 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 205\n",
      "[2023-07-09 22:40:15,963 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 206\n",
      "[2023-07-09 22:40:16,026 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 207\n",
      "[2023-07-09 22:40:16,089 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 208\n",
      "[2023-07-09 22:40:16,153 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 209\n",
      "[2023-07-09 22:40:16,216 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 210\n",
      "[2023-07-09 22:40:18,541 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 211\n",
      "[2023-07-09 22:40:18,611 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 212\n",
      "[2023-07-09 22:40:18,677 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 213\n",
      "[2023-07-09 22:40:18,750 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 214\n",
      "[2023-07-09 22:40:18,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 215\n",
      "[2023-07-09 22:40:18,882 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 216\n",
      "[2023-07-09 22:40:18,953 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 217\n",
      "[2023-07-09 22:40:19,026 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 218\n",
      "[2023-07-09 22:40:19,095 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 219\n",
      "[2023-07-09 22:40:19,163 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 220\n",
      "[2023-07-09 22:40:19,237 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 221\n",
      "[2023-07-09 22:40:19,305 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 222\n",
      "[2023-07-09 22:40:19,378 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 223\n",
      "[2023-07-09 22:40:19,445 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 224\n",
      "[2023-07-09 22:40:19,511 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 225\n",
      "[2023-07-09 22:40:19,585 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 226\n",
      "[2023-07-09 22:40:19,647 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 227\n",
      "[2023-07-09 22:40:19,709 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 228\n",
      "[2023-07-09 22:40:19,774 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 229\n",
      "[2023-07-09 22:40:19,840 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 230\n",
      "[2023-07-09 22:40:19,906 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 231\n",
      "[2023-07-09 22:40:19,982 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 232\n",
      "[2023-07-09 22:40:20,046 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 233\n",
      "[2023-07-09 22:40:20,106 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 234\n",
      "[2023-07-09 22:40:22,753 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 235\n",
      "[2023-07-09 22:40:22,813 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 236\n",
      "[2023-07-09 22:40:22,873 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 237\n",
      "[2023-07-09 22:40:22,934 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 238\n",
      "[2023-07-09 22:40:22,995 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 239\n",
      "[2023-07-09 22:40:23,053 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 240\n",
      "[2023-07-09 22:40:23,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 241\n",
      "[2023-07-09 22:40:23,174 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 242\n",
      "[2023-07-09 22:40:23,235 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 243\n",
      "[2023-07-09 22:40:23,296 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 244\n",
      "[2023-07-09 22:40:23,355 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 245\n",
      "[2023-07-09 22:40:23,417 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 246\n",
      "[2023-07-09 22:40:23,476 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 247\n",
      "[2023-07-09 22:40:23,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 248\n",
      "[2023-07-09 22:40:23,595 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 249\n",
      "[2023-07-09 22:40:23,658 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 250\n",
      "[2023-07-09 22:40:23,718 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 251\n",
      "[2023-07-09 22:40:23,780 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 252\n",
      "[2023-07-09 22:40:23,841 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 253\n",
      "[2023-07-09 22:40:23,900 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 254\n",
      "[2023-07-09 22:40:23,960 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 255\n",
      "[2023-07-09 22:40:24,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 256\n",
      "[2023-07-09 22:40:24,083 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 257\n",
      "[2023-07-09 22:40:24,144 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 258\n",
      "[2023-07-09 22:40:24,209 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 259\n",
      "[2023-07-09 22:40:24,271 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 260\n",
      "[2023-07-09 22:40:24,329 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 261\n",
      "[2023-07-09 22:40:24,389 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 262\n",
      "[2023-07-09 22:40:24,449 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 263\n",
      "[2023-07-09 22:41:32,160 INFO] Step 600/ 1200; acc: 99.2; ppl:   3.7; xent: 1.3; lr: 0.00180; sents:   51661; bsz: 3009/3798/258; 5863/7401 tok/s;   1038 sec;\n",
      "[2023-07-09 22:41:32,164 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_600.pt\n",
      "[2023-07-09 22:42:53,975 INFO] Step 650/ 1200; acc: 99.7; ppl:   3.6; xent: 1.3; lr: 0.00173; sents:   50003; bsz: 2979/3792/250; 7283/9269 tok/s;   1120 sec;\n",
      "[2023-07-09 22:44:14,698 INFO] Step 700/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00167; sents:   50102; bsz: 3003/3810/251; 7441/9439 tok/s;   1201 sec;\n",
      "[2023-07-09 22:44:14,702 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_700.pt\n",
      "[2023-07-09 22:45:37,115 INFO] Step 750/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00161; sents:   51090; bsz: 3004/3809/255; 7290/9243 tok/s;   1283 sec;\n",
      "[2023-07-09 22:45:45,304 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 264\n",
      "[2023-07-09 22:45:45,368 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 265\n",
      "[2023-07-09 22:45:45,432 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 266\n",
      "[2023-07-09 22:45:45,493 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 267\n",
      "[2023-07-09 22:45:45,552 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 268\n",
      "[2023-07-09 22:45:45,615 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 269\n",
      "[2023-07-09 22:45:45,677 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 270\n",
      "[2023-07-09 22:45:45,738 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 271\n",
      "[2023-07-09 22:45:45,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 272\n",
      "[2023-07-09 22:45:45,863 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 273\n",
      "[2023-07-09 22:45:45,925 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 274\n",
      "[2023-07-09 22:45:45,986 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 275\n",
      "[2023-07-09 22:45:46,050 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 276\n",
      "[2023-07-09 22:45:46,113 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 277\n",
      "[2023-07-09 22:45:46,174 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 278\n",
      "[2023-07-09 22:45:46,239 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 279\n",
      "[2023-07-09 22:45:46,305 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 280\n",
      "[2023-07-09 22:45:46,368 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 281\n",
      "[2023-07-09 22:45:46,431 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 282\n",
      "[2023-07-09 22:45:46,492 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 283\n",
      "[2023-07-09 22:45:46,555 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 284\n",
      "[2023-07-09 22:45:46,618 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 285\n",
      "[2023-07-09 22:45:46,680 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 286\n",
      "[2023-07-09 22:45:46,742 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 287\n",
      "[2023-07-09 22:45:46,806 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 288\n",
      "[2023-07-09 22:45:46,866 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 289\n",
      "[2023-07-09 22:45:46,930 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 290\n",
      "[2023-07-09 22:45:46,992 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 291\n",
      "[2023-07-09 22:45:47,055 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 292\n",
      "[2023-07-09 22:45:49,655 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 293\n",
      "[2023-07-09 22:45:49,716 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 294\n",
      "[2023-07-09 22:45:49,774 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 295\n",
      "[2023-07-09 22:45:49,836 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 296\n",
      "[2023-07-09 22:45:49,897 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 297\n",
      "[2023-07-09 22:45:49,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 298\n",
      "[2023-07-09 22:45:50,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 299\n",
      "[2023-07-09 22:45:50,084 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 300\n",
      "[2023-07-09 22:45:50,146 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 301\n",
      "[2023-07-09 22:45:50,209 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 302\n",
      "[2023-07-09 22:45:50,282 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 303\n",
      "[2023-07-09 22:45:50,357 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 304\n",
      "[2023-07-09 22:45:50,423 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 305\n",
      "[2023-07-09 22:45:50,485 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 306\n",
      "[2023-07-09 22:45:50,549 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 307\n",
      "[2023-07-09 22:45:50,610 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 308\n",
      "[2023-07-09 22:45:50,670 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 309\n",
      "[2023-07-09 22:45:50,739 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 310\n",
      "[2023-07-09 22:45:50,801 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 311\n",
      "[2023-07-09 22:45:50,865 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 312\n",
      "[2023-07-09 22:45:50,926 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 313\n",
      "[2023-07-09 22:45:50,990 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 314\n",
      "[2023-07-09 22:45:51,055 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 315\n",
      "[2023-07-09 22:45:51,123 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 316\n",
      "[2023-07-09 22:45:51,187 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 317\n",
      "[2023-07-09 22:45:51,252 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 318\n",
      "[2023-07-09 22:45:51,315 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 319\n",
      "[2023-07-09 22:45:51,376 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 320\n",
      "[2023-07-09 22:45:54,473 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 321\n",
      "[2023-07-09 22:45:54,531 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 322\n",
      "[2023-07-09 22:45:54,593 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 323\n",
      "[2023-07-09 22:45:54,656 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 324\n",
      "[2023-07-09 22:45:54,717 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 325\n",
      "[2023-07-09 22:45:54,778 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 326\n",
      "[2023-07-09 22:45:54,838 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 327\n",
      "[2023-07-09 22:45:54,896 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 328\n",
      "[2023-07-09 22:47:17,219 INFO] Step 800/ 1200; acc: 99.7; ppl:   3.6; xent: 1.3; lr: 0.00156; sents:   51229; bsz: 3002/3803/256; 5998/7598 tok/s;   1383 sec;\n",
      "[2023-07-09 22:47:17,648 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.42783451080322266 s.\n",
      "[2023-07-09 22:47:17,649 INFO] Train perplexity: 7.08408\n",
      "[2023-07-09 22:47:17,649 INFO] Train accuracy: 89.3348\n",
      "[2023-07-09 22:47:17,649 INFO] Sentences processed: 811416\n",
      "[2023-07-09 22:47:17,649 INFO] Average bsz: 2998/3802/254\n",
      "[2023-07-09 22:47:17,649 INFO] Validation perplexity: 13.194\n",
      "[2023-07-09 22:47:17,650 INFO] Validation accuracy: 79.314\n",
      "[2023-07-09 22:47:17,650 INFO] Model is improving ppl: 14.0132 --> 13.194.\n",
      "[2023-07-09 22:47:17,650 INFO] Model is improving acc: 77.6983 --> 79.314.\n",
      "[2023-07-09 22:47:17,652 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_800.pt\n",
      "[2023-07-09 22:48:39,720 INFO] Step 850/ 1200; acc: 99.7; ppl:   3.6; xent: 1.3; lr: 0.00151; sents:   50505; bsz: 2996/3809/253; 7263/9235 tok/s;   1466 sec;\n",
      "[2023-07-09 22:50:00,229 INFO] Step 900/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00147; sents:   50615; bsz: 3007/3810/253; 7470/9464 tok/s;   1546 sec;\n",
      "[2023-07-09 22:50:00,233 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_900.pt\n",
      "[2023-07-09 22:51:12,637 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 329\n",
      "[2023-07-09 22:51:12,708 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 330\n",
      "[2023-07-09 22:51:12,774 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 331\n",
      "[2023-07-09 22:51:12,838 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 332\n",
      "[2023-07-09 22:51:12,905 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 333\n",
      "[2023-07-09 22:51:12,981 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 334\n",
      "[2023-07-09 22:51:13,052 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 335\n",
      "[2023-07-09 22:51:13,126 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 336\n",
      "[2023-07-09 22:51:13,196 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 337\n",
      "[2023-07-09 22:51:13,264 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 338\n",
      "[2023-07-09 22:51:13,327 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 339\n",
      "[2023-07-09 22:51:13,397 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 340\n",
      "[2023-07-09 22:51:13,467 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 341\n",
      "[2023-07-09 22:51:13,536 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 342\n",
      "[2023-07-09 22:51:13,608 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 343\n",
      "[2023-07-09 22:51:13,682 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 344\n",
      "[2023-07-09 22:51:13,753 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 345\n",
      "[2023-07-09 22:51:13,826 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 346\n",
      "[2023-07-09 22:51:13,894 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 347\n",
      "[2023-07-09 22:51:13,964 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 348\n",
      "[2023-07-09 22:51:14,036 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 349\n",
      "[2023-07-09 22:51:14,109 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 350\n",
      "[2023-07-09 22:51:16,577 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 351\n",
      "[2023-07-09 22:51:16,640 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 352\n",
      "[2023-07-09 22:51:16,700 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 353\n",
      "[2023-07-09 22:51:16,762 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 354\n",
      "[2023-07-09 22:51:16,825 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 355\n",
      "[2023-07-09 22:51:16,889 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 356\n",
      "[2023-07-09 22:51:16,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 357\n",
      "[2023-07-09 22:51:17,013 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 358\n",
      "[2023-07-09 22:51:17,077 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 359\n",
      "[2023-07-09 22:51:17,139 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 360\n",
      "[2023-07-09 22:51:17,204 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 361\n",
      "[2023-07-09 22:51:17,268 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 362\n",
      "[2023-07-09 22:51:17,331 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 363\n",
      "[2023-07-09 22:51:17,394 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 364\n",
      "[2023-07-09 22:51:17,458 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 365\n",
      "[2023-07-09 22:51:17,521 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 366\n",
      "[2023-07-09 22:51:17,585 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 367\n",
      "[2023-07-09 22:51:17,649 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 368\n",
      "[2023-07-09 22:51:17,712 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 369\n",
      "[2023-07-09 22:51:17,773 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 370\n",
      "[2023-07-09 22:51:17,843 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 371\n",
      "[2023-07-09 22:51:17,904 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 372\n",
      "[2023-07-09 22:51:17,966 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 373\n",
      "[2023-07-09 22:51:18,024 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 374\n",
      "[2023-07-09 22:51:18,086 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 375\n",
      "[2023-07-09 22:51:18,149 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 376\n",
      "[2023-07-09 22:51:20,991 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 377\n",
      "[2023-07-09 22:51:21,053 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 378\n",
      "[2023-07-09 22:51:21,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 379\n",
      "[2023-07-09 22:51:21,172 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 380\n",
      "[2023-07-09 22:51:21,233 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 381\n",
      "[2023-07-09 22:51:21,294 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 382\n",
      "[2023-07-09 22:51:21,355 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 383\n",
      "[2023-07-09 22:51:21,416 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 384\n",
      "[2023-07-09 22:51:21,478 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 385\n",
      "[2023-07-09 22:51:21,538 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 386\n",
      "[2023-07-09 22:51:21,599 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 387\n",
      "[2023-07-09 22:51:21,661 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 388\n",
      "[2023-07-09 22:51:21,724 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 389\n",
      "[2023-07-09 22:51:21,786 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 390\n",
      "[2023-07-09 22:51:21,846 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 391\n",
      "[2023-07-09 22:51:21,909 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 392\n",
      "[2023-07-09 22:51:21,969 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 393\n",
      "[2023-07-09 22:51:22,029 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 394\n",
      "[2023-07-09 22:51:40,135 INFO] Step 950/ 1200; acc: 99.6; ppl:   3.6; xent: 1.3; lr: 0.00143; sents:   50138; bsz: 2987/3786/251; 5980/7580 tok/s;   1646 sec;\n",
      "[2023-07-09 22:53:01,496 INFO] Step 1000/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00140; sents:   50683; bsz: 3006/3807/253; 7389/9359 tok/s;   1727 sec;\n",
      "[2023-07-09 22:53:01,499 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1000.pt\n",
      "[2023-07-09 22:54:23,925 INFO] Step 1050/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00136; sents:   49981; bsz: 2992/3809/250; 7259/9241 tok/s;   1810 sec;\n",
      "[2023-07-09 22:55:44,690 INFO] Step 1100/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00133; sents:   51013; bsz: 3001/3806/255; 7432/9426 tok/s;   1891 sec;\n",
      "[2023-07-09 22:55:44,693 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1100.pt\n",
      "[2023-07-09 22:56:39,157 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 395\n",
      "[2023-07-09 22:56:39,223 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 396\n",
      "[2023-07-09 22:56:39,287 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 397\n",
      "[2023-07-09 22:56:39,350 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 398\n",
      "[2023-07-09 22:56:39,413 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 399\n",
      "[2023-07-09 22:56:39,477 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 400\n",
      "[2023-07-09 22:56:41,698 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 401\n",
      "[2023-07-09 22:56:41,760 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 402\n",
      "[2023-07-09 22:56:41,823 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 403\n",
      "[2023-07-09 22:56:41,888 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 404\n",
      "[2023-07-09 22:56:41,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 405\n",
      "[2023-07-09 22:56:42,026 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 406\n",
      "[2023-07-09 22:56:42,089 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 407\n",
      "[2023-07-09 22:56:42,151 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 408\n",
      "[2023-07-09 22:56:42,214 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 409\n",
      "[2023-07-09 22:56:42,276 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 410\n",
      "[2023-07-09 22:56:42,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 411\n",
      "[2023-07-09 22:56:42,404 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 412\n",
      "[2023-07-09 22:56:42,466 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 413\n",
      "[2023-07-09 22:56:42,529 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 414\n",
      "[2023-07-09 22:56:42,591 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 415\n",
      "[2023-07-09 22:56:42,653 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 416\n",
      "[2023-07-09 22:56:42,719 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 417\n",
      "[2023-07-09 22:56:42,784 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 418\n",
      "[2023-07-09 22:56:42,846 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 419\n",
      "[2023-07-09 22:56:42,908 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 420\n",
      "[2023-07-09 22:56:42,973 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 421\n",
      "[2023-07-09 22:56:43,034 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 422\n",
      "[2023-07-09 22:56:45,642 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 423\n",
      "[2023-07-09 22:56:45,704 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 424\n",
      "[2023-07-09 22:56:45,768 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 425\n",
      "[2023-07-09 22:56:45,829 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 426\n",
      "[2023-07-09 22:56:45,894 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 427\n",
      "[2023-07-09 22:56:45,960 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 428\n",
      "[2023-07-09 22:56:46,024 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 429\n",
      "[2023-07-09 22:56:46,092 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 430\n",
      "[2023-07-09 22:56:46,155 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 431\n",
      "[2023-07-09 22:56:46,220 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 432\n",
      "[2023-07-09 22:56:46,284 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 433\n",
      "[2023-07-09 22:56:46,347 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 434\n",
      "[2023-07-09 22:56:46,416 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 435\n",
      "[2023-07-09 22:56:46,481 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 436\n",
      "[2023-07-09 22:56:46,546 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 437\n",
      "[2023-07-09 22:56:46,614 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 438\n",
      "[2023-07-09 22:56:46,684 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 439\n",
      "[2023-07-09 22:56:46,752 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 440\n",
      "[2023-07-09 22:56:46,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 441\n",
      "[2023-07-09 22:56:46,876 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 442\n",
      "[2023-07-09 22:56:46,939 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 443\n",
      "[2023-07-09 22:56:47,007 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 444\n",
      "[2023-07-09 22:56:47,077 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 445\n",
      "[2023-07-09 22:56:47,147 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 446\n",
      "[2023-07-09 22:56:47,217 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 447\n",
      "[2023-07-09 22:56:47,285 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 448\n",
      "[2023-07-09 22:56:47,346 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 449\n",
      "[2023-07-09 22:56:47,412 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 450\n",
      "[2023-07-09 22:56:50,440 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 451\n",
      "[2023-07-09 22:56:50,502 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 452\n",
      "[2023-07-09 22:56:50,563 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 453\n",
      "[2023-07-09 22:56:50,628 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 454\n",
      "[2023-07-09 22:56:50,693 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 455\n",
      "[2023-07-09 22:56:50,754 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 456\n",
      "[2023-07-09 22:56:50,815 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 457\n",
      "[2023-07-09 22:56:50,876 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 458\n",
      "[2023-07-09 22:56:50,937 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 459\n",
      "[2023-07-09 22:57:27,374 INFO] Step 1150/ 1200; acc: 99.6; ppl:   3.6; xent: 1.3; lr: 0.00130; sents:   51923; bsz: 3000/3777/260; 5844/7357 tok/s;   1993 sec;\n",
      "[2023-07-09 22:58:48,393 INFO] Step 1200/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00128; sents:   49673; bsz: 2997/3813/248; 7398/9413 tok/s;   2074 sec;\n",
      "[2023-07-09 22:58:48,861 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.4666919708251953 s.\n",
      "[2023-07-09 22:58:48,862 INFO] Train perplexity: 5.6493\n",
      "[2023-07-09 22:58:48,862 INFO] Train accuracy: 92.8179\n",
      "[2023-07-09 22:58:48,862 INFO] Sentences processed: 1.21595e+06\n",
      "[2023-07-09 22:58:48,862 INFO] Average bsz: 2998/3802/253\n",
      "[2023-07-09 22:58:48,862 INFO] Validation perplexity: 12.9291\n",
      "[2023-07-09 22:58:48,862 INFO] Validation accuracy: 79.8305\n",
      "[2023-07-09 22:58:48,863 INFO] Model is improving ppl: 13.194 --> 12.9291.\n",
      "[2023-07-09 22:58:48,863 INFO] Model is improving acc: 79.314 --> 79.8305.\n",
      "[2023-07-09 22:58:48,865 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1200.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the NMT model using the configuration defined in 'config.yaml'\n",
    "!onmt_train -config config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1688943533680,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "ro-21ODLnBNU",
    "outputId": "37f23680-79bc-4aaf-c5a7-dc24b4085ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step_1000.pt  model_step_200.pt  model_step_600.pt  src.vocab\n",
      "model_step_100.pt   model_step_300.pt  model_step_700.pt  train.log\n",
      "model_step_1100.pt  model_step_400.pt  model_step_800.pt\n",
      "model_step_1200.pt  model_step_500.pt  model_step_900.pt\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the 'model_root' directory\n",
    "!ls '{model_root}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRou9WfsDbh"
   },
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37368,
     "status": "ok",
     "timestamp": 1688943571045,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tStHVtbNmipI",
    "outputId": "9015dd0a-4c98-4f1f-fbaa-5af9205c89bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 22:58:55,071 INFO] Loading checkpoint from /content/nmt/nmtmodel/model_step_1200.pt\n",
      "[2023-07-09 22:58:55,745 INFO] Loading data into the model\n",
      "[2023-07-09 22:59:30,236 INFO] PRED SCORE: -0.3685, PRED PPL: 1.45 NB SENTENCES: 500\n"
     ]
    }
   ],
   "source": [
    "# Perform translation using the trained NMT model\n",
    "# --model specifies the path to the trained model checkpoint\n",
    "# --src specifies the path to the source text file to be translated\n",
    "# --output specifies the path to save the translated output\n",
    "# -beam_size specifies the beam size for beam search\n",
    "!onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src /content/nmt/lcquad/test.en --output /content/nmt/lcquad/trans_test.sparql -beam_size 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1688943571046,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "UYszCfIbssof",
    "outputId": "06c7bb54-c1c0-45fc-c674-bf597ebd038d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list the products of the company which published tweenies: game time .\n",
      "name the common sports played at polytechnic university of philippines san juan and islamic azad university ?\n",
      "which company developed both dart and go ?\n",
      "count the total number of launch site of the rockets which have been launched form cape canaveral air force station ?\n",
      "to which settlement does elliot bay belong to ?\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.en'\n",
    "!head -n 5 /content/nmt/lcquad/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1688943571046,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "nnTRp6eas_p7",
    "outputId": "b8db8b79-f3a3-42ca-91ff-b690014941dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_computingPlatform> <dbr_PC-9800_Series> sep_dot var_x <dbp_genre> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Advocate_Nasiruddin> <dbo_knownFor> var_uri sep_dot <dbr_Polytechnic_University_of_the_Philippines_San_Juan> <dbo_sport> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Google_Web_Toolkit> <dbo_author> var_uri sep_dot <dbr_PlayN> <dbo_author> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_manufacturer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_GetTV> <dbo_parentOrganisation> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'trans_test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/trans_test.sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1688943571530,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "azi6x-EstHYN",
    "outputId": "824cdf97-5dee-4b67-f051-1a2caa2f6527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Polytechnic_University_of_the_Philippines_San_Juan> <dbo_sport> var_uri sep_dot <dbr_Islamic_Azad_University_Central_Tehran_Branch> <dbo_sport> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Dart_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot <dbr_Go_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_launchSite> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Elliott_Bay> <dbp_cities> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/test.sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vANuMTJUtfUb"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1688943571530,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "kj1AtrSDtjfq",
    "outputId": "34b20ba2-143e-4afe-d06d-34300790ccb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1688943572288,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9OhvK6N3ecsm",
    "outputId": "552924ae-d5fd-4407-89f1-8a645cd9cdf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_computingPlatform> <dbr_PC-9800_Series> sep_dot var_x <dbp_genre> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Accuracy:  0.639729691679572\n"
     ]
    }
   ],
   "source": [
    "# Copy the file \"compute-accuracy.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-accuracy.py ./\n",
    "\n",
    "# Evaluate the translation using the provided accuracy computation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-accuracy.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5308,
     "status": "ok",
     "timestamp": 1688943577594,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "CBxRt66trVw5",
    "outputId": "0d1e8118-7c76-4f1e-fb82-e029d35f1754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_computingPlatform> <dbr_PC-9800_Series> sep_dot var_x <dbp_genre> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "BLEU:  68.69045724113383\n"
     ]
    }
   ],
   "source": [
    "# Install the sacrebleu library using pip\n",
    "!pip install sacrebleu > /dev/null\n",
    "\n",
    "# Copy the file \"compute-bleu.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-bleu.py ./\n",
    "\n",
    "# Evaluate the translation using BLEU score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-bleu.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4855,
     "status": "ok",
     "timestamp": 1688943582445,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "m8OCPRSAxQ_I",
    "outputId": "0de6c439-f3cf-47e8-e734-82d8b7beb59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_computingPlatform> <dbr_PC-9800_Series> sep_dot var_x <dbp_genre> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Rouge-L:  0.7318797353069093\n"
     ]
    }
   ],
   "source": [
    "# Install the rouge library using pip\n",
    "!pip install rouge > /dev/null\n",
    "\n",
    "# Copy the file \"compute-rouge-l.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-rouge-l.py ./\n",
    "\n",
    "# Evaluate the translation using Rouge-L score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-rouge-l.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 45075,
     "status": "ok",
     "timestamp": 1688943627516,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "wCH-C-xQCGOv"
   },
   "outputs": [],
   "source": [
    "# Copy the directory 'nmt' and its contents from '/content/nmt' to '/content/drive/MyDrive'\n",
    "!cp -r /content/nmt /content/drive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L_nKJQMr2nO"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu7lspbw1Gxw"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ''' ''',\n",
    "    ''' '''\n",
    "]\n",
    "with open('questions.en', 'w') as fp:\n",
    "    t = [''.join(x) for x in sentences]\n",
    "    t = '\\n'.join(t)\n",
    "    fp.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src questions.en --output pred.sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat pred.sparql"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOjcpSZOEmTaoXTsSSAwYiu",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1wyB1O17-Q8ysEA3ZrBm8TxVwAlhhv8zh",
   "provenance": [
    {
     "file_id": "1qwBCXsrX0UD96jZU-P9vIhFRTnCZYO6m",
     "timestamp": 1686823688936
    },
    {
     "file_id": "1sYi4GQuLKBmhhiOJvNHDacHYPI39LsG-",
     "timestamp": 1686759059262
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
