{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w2wNk0167DQ"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1689004605024,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "6ZH1UZrpjtPE",
    "outputId": "3b709230-e44d-42a7-b79a-9ed04d104428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'nmt'\n",
    "!mkdir nmt\n",
    "\n",
    "# Change the current working directory to the newly created 'nmt' directory\n",
    "%cd nmt\n",
    "\n",
    "# Inside the 'nmt' directory, create another directory named 'nmtmodel'\n",
    "!mkdir nmtmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113176,
     "status": "ok",
     "timestamp": 1689004718199,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "df6W9IIWmlQL",
    "outputId": "9fa8fbfb-0703-4896-cac2-4f5a127bf01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install specific versions of OpenNMT-py, torchvision, and torchaudio using pip\n",
    "# The output of the installation process is redirected to /dev/null to suppress output\n",
    "!pip install OpenNMT-py torchvision==0.14.1 torchaudio==0.13.1 > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1689004718199,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "_3WnsZ-1nUSj",
    "outputId": "ced20f0b-dd59-4dd6-8368-2bb4bea58b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory (current path)\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QypsRuOq7IiR"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1689004718766,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "Jsc7THqZnYCI",
    "outputId": "77f5f516-91e4-4614-a16b-08d64498af67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./monument50.zip\n",
      "   creating: ./monument50/\n",
      "  inflating: ./__MACOSX/._monument50  \n",
      "  inflating: ./monument50/dev.en     \n",
      "  inflating: ./__MACOSX/monument50/._dev.en  \n",
      "  inflating: ./monument50/dev.sparql  \n",
      "  inflating: ./__MACOSX/monument50/._dev.sparql  \n",
      "  inflating: ./monument50/.DS_Store  \n",
      "  inflating: ./__MACOSX/monument50/._.DS_Store  \n",
      "  inflating: ./monument50/train.sparql  \n",
      "  inflating: ./__MACOSX/monument50/._train.sparql  \n",
      "  inflating: ./monument50/train.en   \n",
      "  inflating: ./__MACOSX/monument50/._train.en  \n",
      "  inflating: ./monument50/test.sparql  \n",
      "  inflating: ./__MACOSX/monument50/._test.sparql  \n",
      "  inflating: ./monument50/test.en    \n",
      "  inflating: ./__MACOSX/monument50/._test.en  \n"
     ]
    }
   ],
   "source": [
    "# Copy the 'monument50.zip' file from Google Drive to the current directory\n",
    "!cp /content/drive/MyDrive/monument50.zip ./\n",
    "\n",
    "# Unzip the contents of the 'monument50.zip' file into the current directory\n",
    "# The '-d' flag specifies the destination directory for the extracted files\n",
    "!unzip ./monument50.zip -d ./\n",
    "\n",
    "# Remove the original 'monument50.zip' file from the 'nmt' directory\n",
    "!rm /content/nmt/monument50.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1689004719194,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "dbGofKann5xB",
    "outputId": "7e3a188f-e967-4692-9797-12d152d5b533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__MACOSX  monument50  nmtmodel\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the current directory using the 'ls' command\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZiY1mojsEkl"
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159514,
     "status": "ok",
     "timestamp": 1689004878705,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "YTD24q1UsEPM",
    "outputId": "fed94290-f707-4a80-a33a-7409decd0431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-10 15:58:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-07-10 15:58:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-07-10 15:58:39--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2023-07-10 16:01:18 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'glove_dir'\n",
    "!mkdir \"glove_dir\"\n",
    "\n",
    "# Download the GloVe pre-trained word vectors dataset (glove.6B.zip) from Stanford NLP\n",
    "# The '--no-check-certificate' flag is used to bypass SSL certificate verification\n",
    "!wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22131,
     "status": "ok",
     "timestamp": 1689004900818,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "uksk5MCmq1kL",
    "outputId": "2d62151b-4ffb-47ff-9727-1d58a4c4dbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove_dir/glove.6B.50d.txt  \n",
      "  inflating: glove_dir/glove.6B.100d.txt  \n",
      "  inflating: glove_dir/glove.6B.200d.txt  \n",
      "  inflating: glove_dir/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# Unzip the contents of the 'glove.6B.zip' file into the 'glove_dir' directory\n",
    "# The '-d' flag specifies the destination directory for the extracted files\n",
    "!unzip glove.6B.zip -d \"glove_dir\"\n",
    "\n",
    "# Remove the original 'glove.6B.zip' file from the 'nmt' directory\n",
    "!rm /content/nmt/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Nfw0l77P8e"
   },
   "source": [
    "# Create the Training Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689004900820,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9uBY92Mf8vE6"
   },
   "outputs": [],
   "source": [
    "# Define the path for the model root directory\n",
    "model_root = '/content/nmt/nmtmodel'\n",
    "\n",
    "# Create the model root directory if it doesn't exist\n",
    "# The '-p' flag ensures that intermediate directories are also created if needed\n",
    "!mkdir -p '{model_root}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1689007077353,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "xAJ686rIvLRn"
   },
   "outputs": [],
   "source": [
    "# Define the content of the configuration file as a formatted string\n",
    "config = f'''# config.yaml\n",
    "# GloVe:\n",
    "# This indicates that GloVe embeddings will be used for both encoder and decoder sides\n",
    "both_embeddings: /content/nmt/glove_dir/glove.6B.300d.txt\n",
    "\n",
    "# Supported types of word embeddings: GloVe, word2vec\n",
    "embeddings_type: \"GloVe\"\n",
    "\n",
    "# Set the word embedding dimension to 300 to match the pretrained embeddings dimensions\n",
    "word_vec_size: 300\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: {model_root}\n",
    "\n",
    "## Where the vocab(s) will be written\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: {model_root}/src.vocab\n",
    "tgt_vocab: {model_root}/src.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 5000\n",
    "tgt_vocab_size: 5000\n",
    "share_vocab: true\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    train:\n",
    "        path_src: /content/nmt/monument50/train.en\n",
    "        path_tgt: /content/nmt/monument50/train.sparql\n",
    "    valid:\n",
    "        path_src: /content/nmt/monument50/dev.en\n",
    "        path_tgt: /content/nmt/monument50/dev.sparql\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: {model_root}/model\n",
    "log_file: {model_root}/train.log\n",
    "save_checkpoint_steps: 100\n",
    "train_steps: 1200\n",
    "valid_steps: 400\n",
    "\n",
    "# Stop training if it does not improve after n validations\n",
    "early_stopping: 4\n",
    "\n",
    "# To save space, limit checkpoints to the last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 4242\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "# queue_size: 100\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM is out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA is out of memory\n",
    "valid_batch_size: 2048\n",
    "# world_size: 1\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "# model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "# learning_rate: 2\n",
    "warmup_steps: 500 ######\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.98\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "# dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "'''\n",
    "\n",
    "# Write the configuration string to a file named \"config.yaml\"\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1ZGev3N7ao3"
   },
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1689007080044,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "F4jDz0Dr7sne"
   },
   "outputs": [],
   "source": [
    "# Import the 'os' module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Check if the source vocabulary file does not exist in the model root directory\n",
    "if not os.path.exists(os.path.join(model_root, 'src.vocab')):\n",
    "    # Use the 'onmt_build_vocab' command to generate vocabulary files\n",
    "    # The '-config' flag specifies the configuration file\n",
    "    # The '--n_sample -1' flag ensures that all training data samples are used\n",
    "    # The '|| true' part ensures that the command continues running even if it encounters an error\n",
    "    ! onmt_build_vocab -config config.yaml --n_sample -1 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V312rtxK7pd7"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1689007083851,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tJ4dYNPtvQ5Q",
    "outputId": "b5b05175-ae1b-4fc3-c360-89b4a38ad569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 10 16:38:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P0    29W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "GPU:\n",
      "GPU 0: Tesla T4 (UUID: GPU-aa47db5d-ac5b-9cd4-3edc-837f0b5d4d71)\n",
      "$*****************************************************************************$\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "True\n",
      "Tesla T4\n",
      "Free GPU memory: 14998.8125 out of: 15101.8125\n",
      "$*****************************************************************************$\n",
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 20.04.6 LTS\n",
      "Release:\t20.04\n",
      "Codename:\tfocal\n",
      "$*****************************************************************************$\n",
      "5.15.107+\n",
      "$*****************************************************************************$\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "$*****************************************************************************$\n",
      "1.13.1+cu117\n",
      "$*****************************************************************************$\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "$*****************************************************************************$\n",
      "MemTotal:       26687700 kB\n"
     ]
    }
   ],
   "source": [
    "# Display GPU information using the 'nvidia-smi' command\n",
    "!nvidia-smi\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "\n",
    "# Display information about the available GPUs using the 'nvidia-smi -L' command\n",
    "print('GPU:')\n",
    "!nvidia-smi -L\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Check if the GPU is available for PyTorch by importing the 'torch' module\n",
    "import torch\n",
    "\n",
    "# Print whether CUDA (GPU support) is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Print the name of the GPU device (if available)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Get GPU memory information\n",
    "gpu_memory = torch.cuda.mem_get_info(0)\n",
    "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display information about the Linux distribution using 'lsb_release -a'\n",
    "!lsb_release -a\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display the Linux kernel version using 'uname -r'\n",
    "!uname -r\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display the version information for the CUDA toolkit using 'nvcc --version'\n",
    "!nvcc --version\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display the version information for PyTorch using 'torch.__version__'\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display CPU information using 'cat /proc/cpuinfo | grep model\\ name'\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Print a separator for better readability\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display total system memory information using 'cat /proc/meminfo | grep MemTotal'\n",
    "!cat /proc/meminfo | grep MemTotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmaMw_sc7gvP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1725343,
     "status": "ok",
     "timestamp": 1689008811445,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "5iMmswvHvta_",
    "outputId": "856e8f53-35e1-45ce-96a3-62fa024c395b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-10 16:38:07,451 INFO] Missing transforms field for train data, set to default: [].\n",
      "[2023-07-10 16:38:07,451 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-10 16:38:07,451 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2023-07-10 16:38:07,451 INFO] Parsed 2 corpora from -data.\n",
      "[2023-07-10 16:38:07,452 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2023-07-10 16:38:07,465 INFO] Reading encoder and decoder embeddings from /content/nmt/glove_dir/glove.6B.300d.txt\n",
      "[2023-07-10 16:38:14,649 INFO] \tFound 400000 total vectors in file\n",
      "[2023-07-10 16:38:14,650 INFO] After filtering to vectors in vocab:\n",
      "[2023-07-10 16:38:14,651 INFO] \t* enc: 1836 match, 1828 missing, (50.11%)\n",
      "[2023-07-10 16:38:14,652 INFO] \t* dec: 1836 match, 1828 missing, (50.11%)\n",
      "[2023-07-10 16:38:14,652 INFO] \n",
      "Saving encoder embeddings as:\n",
      "\t* enc: /content/nmt/nmtmodel.enc_embeddings.pt\n",
      "[2023-07-10 16:38:16,011 INFO] \n",
      "Saving decoder embeddings as:\n",
      "\t* dec: /content/nmt/nmtmodel.dec_embeddings.pt\n",
      "[2023-07-10 16:38:17,365 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'var_a', 'where', 'brack_open', 'brack_close', 'select', 'is']\n",
      "[2023-07-10 16:38:17,365 INFO] The decoder start token is: <s>\n",
      "[2023-07-10 16:38:17,365 INFO] Building model...\n",
      "[2023-07-10 16:38:17,899 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2023-07-10 16:38:17,900 INFO] Non quantized layer compute is fp32\n",
      "[2023-07-10 16:38:19,012 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(3664, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(3664, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=3664, bias=True)\n",
      ")\n",
      "[2023-07-10 16:38:19,074 INFO] encoder: 20763648\n",
      "[2023-07-10 16:38:19,074 INFO] decoder: 28940880\n",
      "[2023-07-10 16:38:19,074 INFO] * number of parameters: 49704528\n",
      "[2023-07-10 16:38:19,075 INFO] Trainable parameters = {'torch.float32': 49704528, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-10 16:38:19,075 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-10 16:38:19,075 INFO]  * src vocab size = 3664\n",
      "[2023-07-10 16:38:19,075 INFO]  * tgt vocab size = 3664\n",
      "[2023-07-10 16:38:19,078 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 1\n",
      "[2023-07-10 16:38:19,078 INFO] Starting training on GPU: [0]\n",
      "[2023-07-10 16:38:19,078 INFO] Start training loop and validate every 400 steps...\n",
      "[2023-07-10 16:38:19,078 INFO] Scoring with: TransformPipe()\n",
      "[2023-07-10 16:38:19,182 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 2\n",
      "[2023-07-10 16:38:19,369 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 3\n",
      "[2023-07-10 16:38:19,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 4\n",
      "[2023-07-10 16:38:19,708 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 5\n",
      "[2023-07-10 16:38:19,813 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 6\n",
      "[2023-07-10 16:38:20,095 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 7\n",
      "[2023-07-10 16:38:20,201 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 8\n",
      "[2023-07-10 16:38:20,300 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 9\n",
      "[2023-07-10 16:38:20,643 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 10\n",
      "[2023-07-10 16:38:20,746 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 11\n",
      "[2023-07-10 16:38:20,857 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 12\n",
      "[2023-07-10 16:38:21,283 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 13\n",
      "[2023-07-10 16:38:21,391 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 14\n",
      "[2023-07-10 16:38:21,502 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 15\n",
      "[2023-07-10 16:38:21,987 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 16\n",
      "[2023-07-10 16:38:22,092 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 17\n",
      "[2023-07-10 16:38:22,198 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 18\n",
      "[2023-07-10 16:38:22,309 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 19\n",
      "[2023-07-10 16:38:22,411 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 20\n",
      "[2023-07-10 16:38:23,004 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 21\n",
      "[2023-07-10 16:38:23,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 22\n",
      "[2023-07-10 16:38:23,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 23\n",
      "[2023-07-10 16:38:23,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 24\n",
      "[2023-07-10 16:38:23,458 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 25\n",
      "[2023-07-10 16:38:24,183 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 26\n",
      "[2023-07-10 16:38:24,301 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 27\n",
      "[2023-07-10 16:38:24,417 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 28\n",
      "[2023-07-10 16:38:24,528 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 29\n",
      "[2023-07-10 16:38:24,647 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 30\n",
      "[2023-07-10 16:38:24,764 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 31\n",
      "[2023-07-10 16:38:24,871 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 32\n",
      "[2023-07-10 16:38:25,736 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 33\n",
      "[2023-07-10 16:38:25,837 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 34\n",
      "[2023-07-10 16:38:25,939 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 35\n",
      "[2023-07-10 16:38:26,044 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 36\n",
      "[2023-07-10 16:39:40,341 INFO] Step 50/ 1200; acc: 34.1; ppl: 201.4; xent: 5.3; lr: 0.00020; sents:   57891; bsz: 1983/3794/289; 4881/9337 tok/s;     81 sec;\n",
      "[2023-07-10 16:40:46,521 INFO] Step 100/ 1200; acc: 81.3; ppl:   8.7; xent: 2.2; lr: 0.00040; sents:   57624; bsz: 1973/3791/288; 5963/11456 tok/s;    147 sec;\n",
      "[2023-07-10 16:40:46,524 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_100.pt\n",
      "[2023-07-10 16:41:56,141 INFO] Step 150/ 1200; acc: 92.4; ppl:   5.0; xent: 1.6; lr: 0.00060; sents:   57193; bsz: 1987/3788/286; 5708/10882 tok/s;    217 sec;\n",
      "[2023-07-10 16:43:02,560 INFO] Step 200/ 1200; acc: 97.1; ppl:   3.8; xent: 1.3; lr: 0.00079; sents:   57706; bsz: 1957/3793/289; 5892/11421 tok/s;    283 sec;\n",
      "[2023-07-10 16:43:02,563 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_200.pt\n",
      "[2023-07-10 16:43:23,531 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 37\n",
      "[2023-07-10 16:43:23,635 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 38\n",
      "[2023-07-10 16:43:23,741 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 39\n",
      "[2023-07-10 16:43:23,847 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 40\n",
      "[2023-07-10 16:43:23,949 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 41\n",
      "[2023-07-10 16:43:24,051 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 42\n",
      "[2023-07-10 16:43:24,157 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 43\n",
      "[2023-07-10 16:43:24,261 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 44\n",
      "[2023-07-10 16:43:24,366 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 45\n",
      "[2023-07-10 16:43:24,477 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 46\n",
      "[2023-07-10 16:43:24,584 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 47\n",
      "[2023-07-10 16:43:26,690 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 48\n",
      "[2023-07-10 16:43:26,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 49\n",
      "[2023-07-10 16:43:26,908 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 50\n",
      "[2023-07-10 16:43:27,012 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 51\n",
      "[2023-07-10 16:43:27,115 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 52\n",
      "[2023-07-10 16:43:27,216 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 53\n",
      "[2023-07-10 16:43:27,325 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 54\n",
      "[2023-07-10 16:43:27,430 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 55\n",
      "[2023-07-10 16:43:27,539 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 56\n",
      "[2023-07-10 16:43:27,649 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 57\n",
      "[2023-07-10 16:43:27,751 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 58\n",
      "[2023-07-10 16:43:27,855 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 59\n",
      "[2023-07-10 16:43:27,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 60\n",
      "[2023-07-10 16:43:28,065 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 61\n",
      "[2023-07-10 16:43:30,523 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 62\n",
      "[2023-07-10 16:43:30,635 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 63\n",
      "[2023-07-10 16:43:30,742 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 64\n",
      "[2023-07-10 16:43:30,849 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 65\n",
      "[2023-07-10 16:43:30,956 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 66\n",
      "[2023-07-10 16:43:31,068 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 67\n",
      "[2023-07-10 16:43:31,181 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 68\n",
      "[2023-07-10 16:43:31,296 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 69\n",
      "[2023-07-10 16:43:31,415 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 70\n",
      "[2023-07-10 16:43:31,521 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 71\n",
      "[2023-07-10 16:44:26,468 INFO] Step 250/ 1200; acc: 98.9; ppl:   3.4; xent: 1.2; lr: 0.00099; sents:   58419; bsz: 1978/3792/292; 4715/9038 tok/s;    367 sec;\n",
      "[2023-07-10 16:45:32,486 INFO] Step 300/ 1200; acc: 99.6; ppl:   3.2; xent: 1.2; lr: 0.00119; sents:   54768; bsz: 1913/3807/274; 5797/11535 tok/s;    433 sec;\n",
      "[2023-07-10 16:45:32,489 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_300.pt\n",
      "[2023-07-10 16:46:40,017 INFO] Step 350/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00139; sents:   58697; bsz: 1991/3790/293; 5897/11223 tok/s;    501 sec;\n",
      "[2023-07-10 16:47:46,652 INFO] Step 400/ 1200; acc: 95.6; ppl:   4.0; xent: 1.4; lr: 0.00159; sents:   58350; bsz: 1992/3787/292; 5979/11368 tok/s;    568 sec;\n",
      "[2023-07-10 16:47:47,852 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 1.1985068321228027 s.\n",
      "[2023-07-10 16:47:47,853 INFO] Train perplexity: 6.79924\n",
      "[2023-07-10 16:47:47,853 INFO] Train accuracy: 87.354\n",
      "[2023-07-10 16:47:47,853 INFO] Sentences processed: 460648\n",
      "[2023-07-10 16:47:47,853 INFO] Average bsz: 1972/3793/288\n",
      "[2023-07-10 16:47:47,853 INFO] Validation perplexity: 4.06498\n",
      "[2023-07-10 16:47:47,853 INFO] Validation accuracy: 96.6111\n",
      "[2023-07-10 16:47:47,853 INFO] Model is improving ppl: inf --> 4.06498.\n",
      "[2023-07-10 16:47:47,853 INFO] Model is improving acc: -inf --> 96.6111.\n",
      "[2023-07-10 16:47:47,855 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_400.pt\n",
      "[2023-07-10 16:48:28,781 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 72\n",
      "[2023-07-10 16:48:28,879 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 73\n",
      "[2023-07-10 16:48:28,977 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 74\n",
      "[2023-07-10 16:48:30,975 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 75\n",
      "[2023-07-10 16:48:31,075 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 76\n",
      "[2023-07-10 16:48:31,174 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 77\n",
      "[2023-07-10 16:48:31,277 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 78\n",
      "[2023-07-10 16:48:31,378 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 79\n",
      "[2023-07-10 16:48:31,483 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 80\n",
      "[2023-07-10 16:48:31,583 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 81\n",
      "[2023-07-10 16:48:31,682 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 82\n",
      "[2023-07-10 16:48:31,784 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 83\n",
      "[2023-07-10 16:48:31,883 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 84\n",
      "[2023-07-10 16:48:31,980 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 85\n",
      "[2023-07-10 16:48:34,273 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 86\n",
      "[2023-07-10 16:48:34,371 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 87\n",
      "[2023-07-10 16:48:34,468 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 88\n",
      "[2023-07-10 16:48:34,579 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 89\n",
      "[2023-07-10 16:48:34,679 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 90\n",
      "[2023-07-10 16:48:34,781 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 91\n",
      "[2023-07-10 16:48:34,879 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 92\n",
      "[2023-07-10 16:48:34,974 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 93\n",
      "[2023-07-10 16:48:35,077 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 94\n",
      "[2023-07-10 16:48:35,174 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 95\n",
      "[2023-07-10 16:48:35,281 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 96\n",
      "[2023-07-10 16:48:35,388 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 97\n",
      "[2023-07-10 16:48:35,487 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 98\n",
      "[2023-07-10 16:48:35,589 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 99\n",
      "[2023-07-10 16:48:35,687 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 100\n",
      "[2023-07-10 16:48:38,434 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 101\n",
      "[2023-07-10 16:48:38,527 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 102\n",
      "[2023-07-10 16:48:38,626 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 103\n",
      "[2023-07-10 16:48:38,723 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 104\n",
      "[2023-07-10 16:48:38,825 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 105\n",
      "[2023-07-10 16:48:38,921 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 106\n",
      "[2023-07-10 16:48:39,016 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 107\n",
      "[2023-07-10 16:49:13,741 INFO] Step 450/ 1200; acc: 99.4; ppl:   3.3; xent: 1.2; lr: 0.00178; sents:   57158; bsz: 1975/3792/286; 4536/8708 tok/s;    655 sec;\n",
      "[2023-07-10 16:50:19,898 INFO] Step 500/ 1200; acc: 99.9; ppl:   3.2; xent: 1.2; lr: 0.00197; sents:   58368; bsz: 1970/3793/292; 5957/11467 tok/s;    721 sec;\n",
      "[2023-07-10 16:50:19,901 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_500.pt\n",
      "[2023-07-10 16:51:27,328 INFO] Step 550/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00188; sents:   57909; bsz: 1984/3794/290; 5885/11253 tok/s;    788 sec;\n",
      "[2023-07-10 16:52:33,280 INFO] Step 600/ 1200; acc: 99.6; ppl:   3.2; xent: 1.2; lr: 0.00180; sents:   57514; bsz: 1981/3795/288; 6007/11509 tok/s;    854 sec;\n",
      "[2023-07-10 16:52:33,282 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_600.pt\n",
      "[2023-07-10 16:53:34,120 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 108\n",
      "[2023-07-10 16:53:34,221 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 109\n",
      "[2023-07-10 16:53:34,324 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 110\n",
      "[2023-07-10 16:53:34,425 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 111\n",
      "[2023-07-10 16:53:34,519 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 112\n",
      "[2023-07-10 16:53:34,615 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 113\n",
      "[2023-07-10 16:53:34,717 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 114\n",
      "[2023-07-10 16:53:34,814 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 115\n",
      "[2023-07-10 16:53:37,042 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 116\n",
      "[2023-07-10 16:53:37,139 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 117\n",
      "[2023-07-10 16:53:37,240 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 118\n",
      "[2023-07-10 16:53:37,341 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 119\n",
      "[2023-07-10 16:53:37,441 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 120\n",
      "[2023-07-10 16:53:37,544 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 121\n",
      "[2023-07-10 16:53:37,644 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 122\n",
      "[2023-07-10 16:53:37,757 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 123\n",
      "[2023-07-10 16:53:37,865 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 124\n",
      "[2023-07-10 16:53:37,969 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 125\n",
      "[2023-07-10 16:53:38,072 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 126\n",
      "[2023-07-10 16:53:38,168 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 127\n",
      "[2023-07-10 16:53:38,281 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 128\n",
      "[2023-07-10 16:53:40,892 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 129\n",
      "[2023-07-10 16:53:40,995 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 130\n",
      "[2023-07-10 16:53:41,096 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 131\n",
      "[2023-07-10 16:53:41,195 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 132\n",
      "[2023-07-10 16:53:41,291 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 133\n",
      "[2023-07-10 16:53:41,400 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 134\n",
      "[2023-07-10 16:53:41,500 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 135\n",
      "[2023-07-10 16:53:41,606 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 136\n",
      "[2023-07-10 16:53:41,702 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 137\n",
      "[2023-07-10 16:53:41,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 138\n",
      "[2023-07-10 16:53:41,902 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 139\n",
      "[2023-07-10 16:53:42,000 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 140\n",
      "[2023-07-10 16:53:42,095 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 141\n",
      "[2023-07-10 16:53:42,190 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 142\n",
      "[2023-07-10 16:53:59,529 INFO] Step 650/ 1200; acc: 98.3; ppl:   3.5; xent: 1.2; lr: 0.00173; sents:   56631; bsz: 1955/3790/283; 4533/8789 tok/s;    940 sec;\n",
      "[2023-07-10 16:55:05,564 INFO] Step 700/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00167; sents:   56032; bsz: 1926/3803/280; 5834/11520 tok/s;   1006 sec;\n",
      "[2023-07-10 16:55:05,567 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_700.pt\n",
      "[2023-07-10 16:56:12,649 INFO] Step 750/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00161; sents:   56717; bsz: 1954/3798/284; 5826/11322 tok/s;   1074 sec;\n",
      "[2023-07-10 16:57:18,703 INFO] Step 800/ 1200; acc: 99.3; ppl:   3.3; xent: 1.2; lr: 0.00156; sents:   59533; bsz: 2023/3776/298; 6126/11433 tok/s;   1140 sec;\n",
      "[2023-07-10 16:57:19,917 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 1.212068796157837 s.\n",
      "[2023-07-10 16:57:19,918 INFO] Train perplexity: 4.70629\n",
      "[2023-07-10 16:57:19,918 INFO] Train accuracy: 93.4218\n",
      "[2023-07-10 16:57:19,918 INFO] Sentences processed: 920510\n",
      "[2023-07-10 16:57:19,918 INFO] Average bsz: 1971/3793/288\n",
      "[2023-07-10 16:57:19,918 INFO] Validation perplexity: 3.74296\n",
      "[2023-07-10 16:57:19,918 INFO] Validation accuracy: 96.7443\n",
      "[2023-07-10 16:57:19,918 INFO] Model is improving ppl: 4.06498 --> 3.74296.\n",
      "[2023-07-10 16:57:19,918 INFO] Model is improving acc: 96.6111 --> 96.7443.\n",
      "[2023-07-10 16:57:19,921 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_800.pt\n",
      "[2023-07-10 16:58:27,719 INFO] Step 850/ 1200; acc: 99.5; ppl:   3.3; xent: 1.2; lr: 0.00151; sents:   58878; bsz: 2013/3787/294; 5834/10974 tok/s;   1209 sec;\n",
      "[2023-07-10 16:58:40,698 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 143\n",
      "[2023-07-10 16:58:40,808 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 144\n",
      "[2023-07-10 16:58:40,911 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 145\n",
      "[2023-07-10 16:58:41,018 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 146\n",
      "[2023-07-10 16:58:41,121 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 147\n",
      "[2023-07-10 16:58:41,227 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 148\n",
      "[2023-07-10 16:58:41,332 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 149\n",
      "[2023-07-10 16:58:41,437 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 150\n",
      "[2023-07-10 16:58:41,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 151\n",
      "[2023-07-10 16:58:41,640 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 152\n",
      "[2023-07-10 16:58:41,743 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 153\n",
      "[2023-07-10 16:58:41,852 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 154\n",
      "[2023-07-10 16:58:41,952 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 155\n",
      "[2023-07-10 16:58:42,048 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 156\n",
      "[2023-07-10 16:58:42,146 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 157\n",
      "[2023-07-10 16:58:42,240 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 158\n",
      "[2023-07-10 16:58:42,342 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 159\n",
      "[2023-07-10 16:58:42,441 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 160\n",
      "[2023-07-10 16:58:42,537 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 161\n",
      "[2023-07-10 16:58:45,121 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 162\n",
      "[2023-07-10 16:58:45,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 163\n",
      "[2023-07-10 16:58:45,336 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 164\n",
      "[2023-07-10 16:58:45,443 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 165\n",
      "[2023-07-10 16:58:45,542 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 166\n",
      "[2023-07-10 16:58:45,650 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 167\n",
      "[2023-07-10 16:58:45,748 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 168\n",
      "[2023-07-10 16:58:45,849 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 169\n",
      "[2023-07-10 16:58:45,960 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 170\n",
      "[2023-07-10 16:58:46,058 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 171\n",
      "[2023-07-10 16:58:46,157 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 172\n",
      "[2023-07-10 16:58:46,259 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 173\n",
      "[2023-07-10 16:58:46,371 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 174\n",
      "[2023-07-10 16:58:46,482 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 175\n",
      "[2023-07-10 16:58:46,589 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 176\n",
      "[2023-07-10 16:58:49,537 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 177\n",
      "[2023-07-10 16:58:49,630 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 178\n",
      "[2023-07-10 16:59:51,116 INFO] Step 900/ 1200; acc: 99.7; ppl:   3.2; xent: 1.2; lr: 0.00147; sents:   58333; bsz: 1965/3784/292; 4713/9074 tok/s;   1292 sec;\n",
      "[2023-07-10 16:59:51,118 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_900.pt\n",
      "[2023-07-10 17:00:57,645 INFO] Step 950/ 1200; acc: 99.9; ppl:   3.2; xent: 1.2; lr: 0.00143; sents:   57007; bsz: 1959/3794/285; 5888/11405 tok/s;   1359 sec;\n",
      "[2023-07-10 17:02:03,514 INFO] Step 1000/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00140; sents:   55715; bsz: 1934/3804/279; 5873/11549 tok/s;   1424 sec;\n",
      "[2023-07-10 17:02:03,517 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1000.pt\n",
      "[2023-07-10 17:03:10,465 INFO] Step 1050/ 1200; acc: 99.6; ppl:   3.2; xent: 1.2; lr: 0.00136; sents:   57253; bsz: 1973/3795/286; 5895/11338 tok/s;   1491 sec;\n",
      "[2023-07-10 17:03:45,223 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 179\n",
      "[2023-07-10 17:03:45,322 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 180\n",
      "[2023-07-10 17:03:45,424 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 181\n",
      "[2023-07-10 17:03:45,518 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 182\n",
      "[2023-07-10 17:03:45,617 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 183\n",
      "[2023-07-10 17:03:45,716 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 184\n",
      "[2023-07-10 17:03:45,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 185\n",
      "[2023-07-10 17:03:45,913 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 186\n",
      "[2023-07-10 17:03:46,009 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 187\n",
      "[2023-07-10 17:03:46,120 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 188\n",
      "[2023-07-10 17:03:46,231 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 189\n",
      "[2023-07-10 17:03:46,334 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 190\n",
      "[2023-07-10 17:03:46,440 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 191\n",
      "[2023-07-10 17:03:46,542 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 192\n",
      "[2023-07-10 17:03:46,641 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 193\n",
      "[2023-07-10 17:03:46,743 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 194\n",
      "[2023-07-10 17:03:49,294 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 195\n",
      "[2023-07-10 17:03:49,393 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 196\n",
      "[2023-07-10 17:03:49,508 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 197\n",
      "[2023-07-10 17:03:49,616 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 198\n",
      "[2023-07-10 17:03:49,729 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 199\n",
      "[2023-07-10 17:03:49,836 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 200\n",
      "[2023-07-10 17:03:49,931 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 201\n",
      "[2023-07-10 17:03:50,029 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 202\n",
      "[2023-07-10 17:03:50,130 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 203\n",
      "[2023-07-10 17:03:50,230 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 204\n",
      "[2023-07-10 17:03:50,329 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 205\n",
      "[2023-07-10 17:03:50,430 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 206\n",
      "[2023-07-10 17:03:50,529 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 207\n",
      "[2023-07-10 17:03:50,625 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 208\n",
      "[2023-07-10 17:03:50,723 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 209\n",
      "[2023-07-10 17:03:53,597 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 210\n",
      "[2023-07-10 17:03:53,690 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 211\n",
      "[2023-07-10 17:03:53,782 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 212\n",
      "[2023-07-10 17:03:53,883 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 213\n",
      "[2023-07-10 17:04:34,133 INFO] Step 1100/ 1200; acc: 99.8; ppl:   3.2; xent: 1.2; lr: 0.00133; sents:   59798; bsz: 2032/3785/299; 4858/9048 tok/s;   1575 sec;\n",
      "[2023-07-10 17:04:34,136 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1100.pt\n",
      "[2023-07-10 17:05:41,520 INFO] Step 1150/ 1200; acc: 99.9; ppl:   3.2; xent: 1.2; lr: 0.00130; sents:   58505; bsz: 1996/3787/293; 5923/11238 tok/s;   1642 sec;\n",
      "[2023-07-10 17:06:47,185 INFO] Step 1200/ 1200; acc: 99.9; ppl:   3.2; xent: 1.2; lr: 0.00128; sents:   56748; bsz: 1953/3801/284; 5947/11577 tok/s;   1708 sec;\n",
      "[2023-07-10 17:06:48,451 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 1.2644214630126953 s.\n",
      "[2023-07-10 17:06:48,452 INFO] Train perplexity: 4.1378\n",
      "[2023-07-10 17:06:48,452 INFO] Train accuracy: 95.5324\n",
      "[2023-07-10 17:06:48,452 INFO] Sentences processed: 1.38275e+06\n",
      "[2023-07-10 17:06:48,452 INFO] Average bsz: 1974/3792/288\n",
      "[2023-07-10 17:06:48,452 INFO] Validation perplexity: 3.70034\n",
      "[2023-07-10 17:06:48,452 INFO] Validation accuracy: 97.2474\n",
      "[2023-07-10 17:06:48,452 INFO] Model is improving ppl: 3.74296 --> 3.70034.\n",
      "[2023-07-10 17:06:48,452 INFO] Model is improving acc: 96.7443 --> 97.2474.\n",
      "[2023-07-10 17:06:48,455 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1200.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the NMT model using the 'onmt_train' command and provide the configuration file 'config.yaml'\n",
    "!onmt_train -config config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1689009342758,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "ro-21ODLnBNU",
    "outputId": "6829bfba-df2e-4f30-c0b4-0cab90438c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step_1000.pt  model_step_200.pt  model_step_600.pt  src.vocab\n",
      "model_step_100.pt   model_step_300.pt  model_step_700.pt  train.log\n",
      "model_step_1100.pt  model_step_400.pt  model_step_800.pt\n",
      "model_step_1200.pt  model_step_500.pt  model_step_900.pt\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the 'model_root' directory where the NMT model and related files are saved\n",
    "!ls '{model_root}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRou9WfsDbh"
   },
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387816,
     "status": "ok",
     "timestamp": 1689009199259,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tStHVtbNmipI",
    "outputId": "462207d7-bb31-4cb9-d07e-b4f1620acbd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-10 17:06:52,498 INFO] Loading checkpoint from /content/nmt/nmtmodel/model_step_1200.pt\n",
      "[2023-07-10 17:06:53,066 INFO] Loading data into the model\n",
      "[2023-07-10 17:13:18,625 INFO] PRED SCORE: -0.1120, PRED PPL: 1.12 NB SENTENCES: 5916\n"
     ]
    }
   ],
   "source": [
    "# Perform translation using the trained NMT model\n",
    "# Arguments:\n",
    "# --model: Path to the trained NMT model checkpoint file\n",
    "# --src: Path to the source input file containing sentences to be translated\n",
    "# --output: Path to the output file to save translated sentences\n",
    "# -beam_size: Beam size used during translation\n",
    "!onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' \\\n",
    "                --src /content/nmt/monument50/test.en \\\n",
    "                --output /content/nmt/monument50/trans_test.sparql \\\n",
    "                -beam_size 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1689009199260,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "UYszCfIbssof",
    "outputId": "6df9fc4f-5026-467c-9ee6-591210108712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's the tallest cristo del otero\n",
      "latitude of hagia sophia\n",
      "what do nelson's column and charaxes sidamo have in common\n",
      "what do nelson's column and historic centre of cienfuegos have in common\n",
      "where is bourguiba mausoleum located in\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the English test data file\n",
    "# Arguments:\n",
    "# -n 5: Display the first 5 lines\n",
    "# /content/nmt/monument50/test.en: Path to the test data file in English\n",
    "!head -n 5 /content/nmt/monument50/test.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1689009199580,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "nnTRp6eas_p7",
    "outputId": "56d5400d-97e7-47b0-86f2-5438f92bf7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select var_a where brack_open var_a rdf_type dbo_Monument sep_dot var_a dbo_location dbr_Cristo_del_Otero sep_dot var_a dbp_complete var_c brack_close order by var_c limit 1\n",
      "select var_a where brack_open dbr_Hagia_Sophia geo_lat var_a brack_close\n",
      "select wildcard where brack_open brack_open dbr_Nelson's_Column,_Montreal var_a var_b sep_dot dbr_Vulvodynia var_a var_b brack_close UNION brack_open var_c var_d dbr_Nelson's_Column,_Montreal sep_dot var_c var_d dbr_Graves'_disease brack_close brack_close\n",
      "select wildcard where brack_open brack_open dbr_Nelson's_Column,_Montreal var_a var_b sep_dot dbr_Cienfuegos_Bay var_a var_b brack_close UNION brack_open var_c var_d dbr_Nelson's_Column,_Montreal sep_dot var_c var_d dbr_Cienfuegos_Bay brack_close brack_close\n",
      "select var_a where brack_open dbr_Bourguiba_mausoleum dbo_location var_a brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the translated output file\n",
    "# Arguments:\n",
    "# -n 5: Display the first 5 lines\n",
    "# /content/nmt/monument50/trans_test.sparql: Path to the translated output file\n",
    "!head -n 5 /content/nmt/monument50/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1689009199580,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "azi6x-EstHYN",
    "outputId": "51a0e503-e05e-4f04-fefb-7fe7958f7dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select var_a where brack_open var_a rdf_type dbr_Cristo_del_Otero sep_dot var_a dbp_height var_b brack_close order by desc par_open var_b par_close  limit 1\n",
      "select var_a where brack_open dbr_Hagia_Sophia geo_lat var_a brack_close\n",
      "select wildcard where brack_open brack_open dbr_Nelson's_Column,_Montreal var_a var_b sep_dot dbr_Charaxes_sidamo var_a var_b brack_close UNION brack_open var_c var_d dbr_Nelson's_Column,_Montreal sep_dot var_c var_d dbr_Charaxes_sidamo brack_close brack_close\n",
      "select wildcard where brack_open brack_open dbr_Nelson's_Column,_Montreal var_a var_b sep_dot dbr_Historic_Centre_of_Cienfuegos var_a var_b brack_close UNION brack_open var_c var_d dbr_Nelson's_Column,_Montreal sep_dot var_c var_d dbr_Historic_Centre_of_Cienfuegos brack_close brack_close\n",
      "select var_a where brack_open dbr_Bourguiba_mausoleum dbo_location var_a brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the reference target file\n",
    "# Arguments:\n",
    "# -n 5: Display the first 5 lines\n",
    "# /content/nmt/monument50/test.sparql: Path to the reference target file\n",
    "!head -n 5 /content/nmt/monument50/test.sparql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vANuMTJUtfUb"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1689009199581,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "kj1AtrSDtjfq",
    "outputId": "9c859e32-26ba-4fb2-8c7c-ee4c67fbeb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1689009205771,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9OhvK6N3ecsm",
    "outputId": "623a0c9e-15b5-45ff-8d9d-fb1f83094b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: select var_a where brack_open var_a rdf_type dbr_Cristo_del_Otero sep_dot var_a dbp_height var_b brack_close order by desc par_open var_b par_close  limit 1\n",
      "MTed 1st sentence: select var_a where brack_open var_a rdf_type dbo_Monument sep_dot var_a dbo_location dbr_Cristo_del_Otero sep_dot var_a dbp_complete var_c brack_close order by var_c limit 1\n",
      "Accuracy:  0.9352229951716524\n"
     ]
    }
   ],
   "source": [
    "# Copy the Compute-accuracy.py script from the Google Drive to the current directory\n",
    "!cp /content/drive/MyDrive/compute-accuracy.py ./\n",
    "\n",
    "# Evaluate the translation using accuracy\n",
    "# Arguments:\n",
    "# /content/nmt/monument50/test.sparql: Path to the reference target file\n",
    "# /content/nmt/monument50/trans_test.sparql: Path to the translated output file\n",
    "!python compute-accuracy.py /content/nmt/monument50/test.sparql /content/nmt/monument50/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6190,
     "status": "ok",
     "timestamp": 1689009214749,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "CBxRt66trVw5",
    "outputId": "11924d7e-6e77-43fd-ca29-0d2edfccea68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: select var_a where brack_open var_a rdf_type dbr_Cristo_del_Otero sep_dot var_a dbp_height var_b brack_close order by desc par_open var_b par_close  limit 1\n",
      "MTed 1st sentence: select var_a where brack_open var_a rdf_type dbo_Monument sep_dot var_a dbo_location dbr_Cristo_del_Otero sep_dot var_a dbp_complete var_c brack_close order by var_c limit 1\n",
      "BLEU:  95.48139598263148\n"
     ]
    }
   ],
   "source": [
    "# Install the sacrebleu library using pip\n",
    "!pip install sacrebleu > /dev/null\n",
    "\n",
    "# Copy the Compute-bleu.py script from Google Drive to the current directory\n",
    "!cp /content/drive/MyDrive/compute-bleu.py ./\n",
    "\n",
    "# Evaluate the translation using BLEU score\n",
    "# Arguments:\n",
    "# /content/nmt/monument50/test.sparql: Path to the reference target file\n",
    "# /content/nmt/monument50/trans_test.sparql: Path to the translated output file\n",
    "!python compute-bleu.py /content/nmt/monument50/test.sparql /content/nmt/monument50/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6419,
     "status": "ok",
     "timestamp": 1689009222977,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "m8OCPRSAxQ_I",
    "outputId": "a5de29ab-5f06-4e0d-b603-b34a71bfd8d8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: select var_a where brack_open var_a rdf_type dbr_Cristo_del_Otero sep_dot var_a dbp_height var_b brack_close order by desc par_open var_b par_close  limit 1\n",
      "MTed 1st sentence: select var_a where brack_open var_a rdf_type dbo_Monument sep_dot var_a dbo_location dbr_Cristo_del_Otero sep_dot var_a dbp_complete var_c brack_close order by var_c limit 1\n",
      "Rouge-L:  0.9787737703029831\n"
     ]
    }
   ],
   "source": [
    "# Install the Rouge library using pip\n",
    "!pip install rouge > /dev/null\n",
    "\n",
    "# Copy the compute-rouge-l.py script from Google Drive to the current directory\n",
    "!cp /content/drive/MyDrive/compute-rouge-l.py ./\n",
    "\n",
    "# Evaluate the translation using Rouge-L score\n",
    "# Arguments:\n",
    "# /content/nmt/monument50/test.sparql: Path to the reference target file\n",
    "# /content/nmt/monument50/trans_test.sparql: Path to the translated output file\n",
    "!python compute-rouge-l.py /content/nmt/monument50/test.sparql /content/nmt/monument50/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 29467,
     "status": "ok",
     "timestamp": 1689009285575,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "_l3Mka9HhB7e"
   },
   "outputs": [],
   "source": [
    "# Copy the trained NMT model directory to Google Drive for backup\n",
    "# Source: /content/nmt/nmtmodel (trained NMT model)\n",
    "# Destination: /content/drive/MyDrive/NMT_models (Google Drive directory)\n",
    "!cp -r /content/nmt/nmtmodel /content/drive/MyDrive/NMT_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L_nKJQMr2nO"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu7lspbw1Gxw"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ''' ''',\n",
    "    ''' '''\n",
    "]\n",
    "with open('questions.en', 'w') as fp:\n",
    "    t = [''.join(x) for x in sentences]\n",
    "    t = '\\n'.join(t)\n",
    "    fp.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src questions.en --output pred.sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat pred.sparql"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsh6uvH63ZzolY2AKVSUiV",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1gXeczdvSKaaec-3Mrx1eb5-c4sI7KmwT",
   "provenance": [
    {
     "file_id": "1qwBCXsrX0UD96jZU-P9vIhFRTnCZYO6m",
     "timestamp": 1686823688936
    },
    {
     "file_id": "1sYi4GQuLKBmhhiOJvNHDacHYPI39LsG-",
     "timestamp": 1686759059262
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
