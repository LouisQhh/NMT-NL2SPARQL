{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w2wNk0167DQ"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1688943808688,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "6ZH1UZrpjtPE",
    "outputId": "8704d133-3ece-482d-d674-fba8236f97c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'nmt'\n",
    "!mkdir nmt\n",
    "\n",
    "# Change the current working directory to the newly created 'nmt' directory\n",
    "%cd nmt\n",
    "\n",
    "# Create a subdirectory named 'nmtmodel' inside the 'nmt' directory\n",
    "!mkdir nmtmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112002,
     "status": "ok",
     "timestamp": 1688943920689,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "df6W9IIWmlQL",
    "outputId": "2282f52f-54a2-4095-bf3a-5b04fd9ba22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install OpenNMT-py along with specific versions of torchvision and torchaudio\n",
    "! pip install OpenNMT-py torchvision==0.14.1 torchaudio==0.13.1 > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1688943920982,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "_3WnsZ-1nUSj",
    "outputId": "5a6a054c-2693-41dd-87f6-304b37cfa4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QypsRuOq7IiR"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1688943921501,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "Jsc7THqZnYCI",
    "outputId": "1e83f63d-ff50-4063-ba6d-6d15ddc721c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./lcquad.zip\n",
      "   creating: ./lcquad/\n",
      "  inflating: ./__MACOSX/._lcquad     \n",
      "  inflating: ./lcquad/dev.en         \n",
      "  inflating: ./__MACOSX/lcquad/._dev.en  \n",
      "  inflating: ./lcquad/dev.sparql     \n",
      "  inflating: ./__MACOSX/lcquad/._dev.sparql  \n",
      "  inflating: ./lcquad/.DS_Store      \n",
      "  inflating: ./__MACOSX/lcquad/._.DS_Store  \n",
      "  inflating: ./lcquad/train.sparql   \n",
      "  inflating: ./__MACOSX/lcquad/._train.sparql  \n",
      "  inflating: ./lcquad/train.en       \n",
      "  inflating: ./__MACOSX/lcquad/._train.en  \n",
      "  inflating: ./lcquad/test.sparql    \n",
      "  inflating: ./__MACOSX/lcquad/._test.sparql  \n",
      "  inflating: ./lcquad/test.en        \n",
      "  inflating: ./__MACOSX/lcquad/._test.en  \n"
     ]
    }
   ],
   "source": [
    "# Copy the file 'lcquad.zip' from '/content/drive/MyDrive/' to the current directory\n",
    "!cp /content/drive/MyDrive/lcquad.zip ./\n",
    "\n",
    "# Unzip the file 'lcquad.zip' and extract its contents to the current directory\n",
    "!unzip ./lcquad.zip -d ./\n",
    "\n",
    "# Remove the original zip file 'lcquad.zip' from '/content/nmt/'\n",
    "!rm /content/nmt/lcquad.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688943921502,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "dbGofKann5xB",
    "outputId": "37dcfa8a-d86a-446c-cd7e-33999dc32bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcquad\t__MACOSX  nmtmodel\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the current directory\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZiY1mojsEkl"
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160411,
     "status": "ok",
     "timestamp": 1688944081910,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "YTD24q1UsEPM",
    "outputId": "fb9c9be0-8132-44b1-8a28-8a07115e93ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-09 23:05:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-07-09 23:05:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-07-09 23:05:21--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2023-07-09 23:08:01 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'glove_dir'\n",
    "!mkdir \"glove_dir\"\n",
    "\n",
    "# Download the GloVe word vectors zip file from Stanford's website\n",
    "!wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22536,
     "status": "ok",
     "timestamp": 1688944104437,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "uksk5MCmq1kL",
    "outputId": "72e7ebad-88e2-4dbe-f395-d501ece1a4b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove_dir/glove.6B.50d.txt  \n",
      "  inflating: glove_dir/glove.6B.100d.txt  \n",
      "  inflating: glove_dir/glove.6B.200d.txt  \n",
      "  inflating: glove_dir/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# Unzip the downloaded GloVe word vectors zip file and extract its contents to the 'glove_dir' directory\n",
    "!unzip glove.6B.zip -d \"glove_dir\"\n",
    "\n",
    "# Remove the original zip file 'glove.6B.zip' from the '/content/nmt/' directory\n",
    "!rm /content/nmt/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Nfw0l77P8e"
   },
   "source": [
    "# Create the Training Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uBY92Mf8vE6"
   },
   "outputs": [],
   "source": [
    "# Define the root directory path for the NMT model as \"model_root\"\n",
    "model_root = '/content/nmt/nmtmodel'\n",
    "\n",
    "# Create the directory structure for the NMT model using the mkdir command with the -p option\n",
    "# This ensures that the entire directory path is created, including any necessary parent directories\n",
    "!mkdir -p '{model_root}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAJ686rIvLRn"
   },
   "outputs": [],
   "source": [
    "# Define the configuration as a formatted string\n",
    "config = f'''# config.yaml\n",
    "# GloVe:\n",
    "# this means embeddings will be used for both encoder and decoder sides\n",
    "both_embeddings: /content/nmt/glove_dir/glove.6B.300d.txt\n",
    "\n",
    "# supported types: GloVe, word2vec\n",
    "embeddings_type: \"GloVe\"\n",
    "\n",
    "# word_vec_size needs to match with the pretrained embeddings dimensions\n",
    "word_vec_size: 300\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: {model_root}\n",
    "\n",
    "## Where the vocab(s) will be written\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: {model_root}/src.vocab\n",
    "tgt_vocab: {model_root}/src.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 5000\n",
    "tgt_vocab_size: 5000\n",
    "share_vocab: true\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    train:\n",
    "        path_src: /content/nmt/lcquad/train.en\n",
    "        path_tgt: /content/nmt/lcquad/train.sparql\n",
    "    valid:\n",
    "        path_src: /content/nmt/lcquad/dev.en\n",
    "        path_tgt: /content/nmt/lcquad/dev.sparql\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: {model_root}/model\n",
    "log_file: {model_root}/train.log\n",
    "save_checkpoint_steps: 100\n",
    "train_steps: 1200\n",
    "valid_steps: 400\n",
    "\n",
    "# Stop training if it does not improve after n validations\n",
    "early_stopping: 4\n",
    "\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 4242\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "# queue_size: 100\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "# world_size: 1\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "# model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "# learning_rate: 2\n",
    "warmup_steps: 500\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.98\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "# dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "'''\n",
    "\n",
    "# Write the configuration to a \"config.yaml\" file\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "    config_yaml.write(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1ZGev3N7ao3"
   },
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2818,
     "status": "ok",
     "timestamp": 1688944107253,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "F4jDz0Dr7sne",
    "outputId": "775afb92-83cb-4d1e-b1b7-7cb880fab7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 23:08:25,835 INFO] Counter vocab from -1 samples.\n",
      "[2023-07-09 23:08:25,835 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2023-07-09 23:08:25,997 INFO] Counters src: 6405\n",
      "[2023-07-09 23:08:25,998 INFO] Counters tgt: 4412\n",
      "[2023-07-09 23:08:26,000 INFO] Counters after share:10760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the source vocabulary file doesn't exist in the 'model_root' directory\n",
    "if not os.path.exists(os.path.join(model_root, 'src.vocab')):\n",
    "    # Build the source vocabulary using the onmt_build_vocab command and the provided config.yaml\n",
    "    # The --n_sample option is used to indicate the number of samples to consider for building the vocabulary\n",
    "    # The \"|| true\" at the end ensures that the command won't stop the script even if there's an error\n",
    "    !onmt_build_vocab -config config.yaml --n_sample -1 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V312rtxK7pd7"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2236,
     "status": "ok",
     "timestamp": 1688944109460,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tJ4dYNPtvQ5Q",
    "outputId": "ef5c7f49-8866-423b-95d2-bc4e3b71ff5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  9 23:08:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "GPU:\n",
      "GPU 0: Tesla T4 (UUID: GPU-caae6f09-8311-e024-50fb-da5a29d2451e)\n",
      "$*****************************************************************************$\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "True\n",
      "Tesla T4\n",
      "Free GPU memory: 14998.8125 out of: 15101.8125\n",
      "$*****************************************************************************$\n",
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 20.04.6 LTS\n",
      "Release:\t20.04\n",
      "Codename:\tfocal\n",
      "$*****************************************************************************$\n",
      "5.15.107+\n",
      "$*****************************************************************************$\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "$*****************************************************************************$\n",
      "1.13.1+cu117\n",
      "$*****************************************************************************$\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
      "$*****************************************************************************$\n",
      "MemTotal:       26687700 kB\n"
     ]
    }
   ],
   "source": [
    "# Check NVIDIA GPU information using the 'nvidia-smi' command\n",
    "!nvidia-smi\n",
    "\n",
    "# Print a separator line and heading for the GPU information\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "print('GPU:')\n",
    "\n",
    "# Check and display the GPU devices using 'nvidia-smi -L'\n",
    "!nvidia-smi -L\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print a separator line and heading for GPU-related checks\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "\n",
    "# Check if CUDA-enabled GPU is available for PyTorch\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Print the name of the first CUDA-enabled GPU\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Get GPU memory information\n",
    "gpu_memory = torch.cuda.mem_get_info(0)\n",
    "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print system information using 'lsb_release -a'\n",
    "!lsb_release -a\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print kernel version using 'uname -r'\n",
    "!uname -r\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print NVCC (NVIDIA CUDA Compiler) version using 'nvcc --version'\n",
    "!nvcc --version\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Check Torch version using Python import\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display CPU information using 'cat /proc/cpuinfo | grep model\\ name'\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display total memory information using 'cat /proc/meminfo | grep MemTotal'\n",
    "!cat /proc/meminfo | grep MemTotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmaMw_sc7gvP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1955023,
     "status": "ok",
     "timestamp": 1688946064480,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "5iMmswvHvta_",
    "outputId": "e29f497e-1b5a-4a52-b4da-ffb6e340b8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 23:08:30,978 INFO] Missing transforms field for train data, set to default: [].\n",
      "[2023-07-09 23:08:30,978 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 23:08:30,978 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2023-07-09 23:08:30,978 INFO] Parsed 2 corpora from -data.\n",
      "[2023-07-09 23:08:30,978 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2023-07-09 23:08:31,015 INFO] Reading encoder and decoder embeddings from /content/nmt/glove_dir/glove.6B.300d.txt\n",
      "[2023-07-09 23:08:38,213 INFO] \tFound 400000 total vectors in file\n",
      "[2023-07-09 23:08:38,213 INFO] After filtering to vectors in vocab:\n",
      "[2023-07-09 23:08:38,215 INFO] \t* enc: 2979 match, 2021 missing, (59.58%)\n",
      "[2023-07-09 23:08:38,216 INFO] \t* dec: 2979 match, 2021 missing, (59.58%)\n",
      "[2023-07-09 23:08:38,217 INFO] \n",
      "Saving encoder embeddings as:\n",
      "\t* enc: /content/nmt/nmtmodel.enc_embeddings.pt\n",
      "[2023-07-09 23:08:40,937 INFO] \n",
      "Saving decoder embeddings as:\n",
      "\t* dec: /content/nmt/nmtmodel.dec_embeddings.pt\n",
      "[2023-07-09 23:08:43,777 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'var_uri', 'sep_dot', 'WHERE', 'brack_open', 'brack_close', 'the']\n",
      "[2023-07-09 23:08:43,778 INFO] The decoder start token is: <s>\n",
      "[2023-07-09 23:08:43,778 INFO] Building model...\n",
      "[2023-07-09 23:08:44,344 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2023-07-09 23:08:44,344 INFO] Non quantized layer compute is fp32\n",
      "[2023-07-09 23:08:45,465 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(5000, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(5000, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=5000, bias=True)\n",
      ")\n",
      "[2023-07-09 23:08:45,502 INFO] encoder: 21447680\n",
      "[2023-07-09 23:08:45,502 INFO] decoder: 30310280\n",
      "[2023-07-09 23:08:45,502 INFO] * number of parameters: 51757960\n",
      "[2023-07-09 23:08:45,503 INFO] Trainable parameters = {'torch.float32': 51757960, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 23:08:45,503 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 23:08:45,503 INFO]  * src vocab size = 5000\n",
      "[2023-07-09 23:08:45,503 INFO]  * tgt vocab size = 5000\n",
      "[2023-07-09 23:08:45,506 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 1\n",
      "[2023-07-09 23:08:45,507 INFO] Starting training on GPU: [0]\n",
      "[2023-07-09 23:08:45,507 INFO] Start training loop and validate every 400 steps...\n",
      "[2023-07-09 23:08:45,507 INFO] Scoring with: TransformPipe()\n",
      "[2023-07-09 23:08:45,586 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 2\n",
      "[2023-07-09 23:08:45,663 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 3\n",
      "[2023-07-09 23:08:45,741 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 4\n",
      "[2023-07-09 23:08:45,902 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 5\n",
      "[2023-07-09 23:08:45,979 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 6\n",
      "[2023-07-09 23:08:46,054 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 7\n",
      "[2023-07-09 23:08:46,132 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 8\n",
      "[2023-07-09 23:08:46,355 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 9\n",
      "[2023-07-09 23:08:46,432 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 10\n",
      "[2023-07-09 23:08:46,509 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 11\n",
      "[2023-07-09 23:08:46,785 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 12\n",
      "[2023-07-09 23:08:46,869 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 13\n",
      "[2023-07-09 23:08:46,944 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 14\n",
      "[2023-07-09 23:08:47,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 15\n",
      "[2023-07-09 23:08:47,099 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 16\n",
      "[2023-07-09 23:08:47,416 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 17\n",
      "[2023-07-09 23:08:47,494 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 18\n",
      "[2023-07-09 23:08:47,568 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 19\n",
      "[2023-07-09 23:08:47,642 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 20\n",
      "[2023-07-09 23:08:47,721 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 21\n",
      "[2023-07-09 23:08:48,119 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 22\n",
      "[2023-07-09 23:08:48,196 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 23\n",
      "[2023-07-09 23:08:48,272 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 24\n",
      "[2023-07-09 23:08:48,348 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 25\n",
      "[2023-07-09 23:08:48,421 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 26\n",
      "[2023-07-09 23:08:48,499 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 27\n",
      "[2023-07-09 23:08:48,577 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 28\n",
      "[2023-07-09 23:08:49,067 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 29\n",
      "[2023-07-09 23:08:49,143 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 30\n",
      "[2023-07-09 23:08:49,218 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 31\n",
      "[2023-07-09 23:08:49,295 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 32\n",
      "[2023-07-09 23:08:49,372 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 33\n",
      "[2023-07-09 23:08:49,447 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 34\n",
      "[2023-07-09 23:08:49,522 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 35\n",
      "[2023-07-09 23:08:49,601 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 36\n",
      "[2023-07-09 23:08:50,193 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 37\n",
      "[2023-07-09 23:08:50,268 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 38\n",
      "[2023-07-09 23:08:50,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 39\n",
      "[2023-07-09 23:08:50,415 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 40\n",
      "[2023-07-09 23:08:50,492 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 41\n",
      "[2023-07-09 23:08:50,568 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 42\n",
      "[2023-07-09 23:08:50,645 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 43\n",
      "[2023-07-09 23:08:50,721 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 44\n",
      "[2023-07-09 23:08:50,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 45\n",
      "[2023-07-09 23:08:50,873 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 46\n",
      "[2023-07-09 23:08:51,593 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 47\n",
      "[2023-07-09 23:08:51,668 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 48\n",
      "[2023-07-09 23:08:51,749 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 49\n",
      "[2023-07-09 23:08:51,824 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 50\n",
      "[2023-07-09 23:08:51,902 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 51\n",
      "[2023-07-09 23:08:51,978 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 52\n",
      "[2023-07-09 23:08:52,055 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 53\n",
      "[2023-07-09 23:08:52,130 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 54\n",
      "[2023-07-09 23:08:52,207 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 55\n",
      "[2023-07-09 23:08:52,281 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 56\n",
      "[2023-07-09 23:08:52,356 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 57\n",
      "[2023-07-09 23:08:52,435 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 58\n",
      "[2023-07-09 23:08:52,508 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 59\n",
      "[2023-07-09 23:08:53,398 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 60\n",
      "[2023-07-09 23:08:53,473 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 61\n",
      "[2023-07-09 23:08:53,548 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 62\n",
      "[2023-07-09 23:08:53,624 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 63\n",
      "[2023-07-09 23:08:53,702 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 64\n",
      "[2023-07-09 23:08:53,780 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 65\n",
      "[2023-07-09 23:08:53,859 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 66\n",
      "[2023-07-09 23:10:13,521 INFO] Step 50/ 1200; acc: 25.5; ppl: 315.9; xent: 5.8; lr: 0.00020; sents:   49691; bsz: 2987/3809/248; 6787/8655 tok/s;     88 sec;\n",
      "[2023-07-09 23:11:25,222 INFO] Step 100/ 1200; acc: 68.9; ppl:  16.2; xent: 2.8; lr: 0.00040; sents:   51329; bsz: 3008/3808/257; 8392/10622 tok/s;    160 sec;\n",
      "[2023-07-09 23:12:39,226 INFO] Step 150/ 1200; acc: 80.8; ppl:   8.2; xent: 2.1; lr: 0.00060; sents:   50756; bsz: 2998/3786/254; 8102/10233 tok/s;    234 sec;\n",
      "[2023-07-09 23:13:38,758 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 67\n",
      "[2023-07-09 23:13:38,829 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 68\n",
      "[2023-07-09 23:13:38,896 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 69\n",
      "[2023-07-09 23:13:38,965 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 70\n",
      "[2023-07-09 23:13:39,040 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 71\n",
      "[2023-07-09 23:13:39,110 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 72\n",
      "[2023-07-09 23:13:39,180 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 73\n",
      "[2023-07-09 23:13:39,247 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 74\n",
      "[2023-07-09 23:13:39,317 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 75\n",
      "[2023-07-09 23:13:39,388 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 76\n",
      "[2023-07-09 23:13:39,463 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 77\n",
      "[2023-07-09 23:13:39,534 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 78\n",
      "[2023-07-09 23:13:39,606 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 79\n",
      "[2023-07-09 23:13:39,685 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 80\n",
      "[2023-07-09 23:13:39,754 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 81\n",
      "[2023-07-09 23:13:39,829 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 82\n",
      "[2023-07-09 23:13:39,900 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 83\n",
      "[2023-07-09 23:13:39,968 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 84\n",
      "[2023-07-09 23:13:40,041 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 85\n",
      "[2023-07-09 23:13:40,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 86\n",
      "[2023-07-09 23:13:40,183 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 87\n",
      "[2023-07-09 23:13:42,536 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 88\n",
      "[2023-07-09 23:13:42,607 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 89\n",
      "[2023-07-09 23:13:42,677 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 90\n",
      "[2023-07-09 23:13:42,748 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 91\n",
      "[2023-07-09 23:13:42,822 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 92\n",
      "[2023-07-09 23:13:42,899 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 93\n",
      "[2023-07-09 23:13:42,967 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 94\n",
      "[2023-07-09 23:13:43,043 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 95\n",
      "[2023-07-09 23:13:43,115 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 96\n",
      "[2023-07-09 23:13:43,187 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 97\n",
      "[2023-07-09 23:13:43,263 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 98\n",
      "[2023-07-09 23:13:43,335 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 99\n",
      "[2023-07-09 23:13:43,409 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 100\n",
      "[2023-07-09 23:13:43,484 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 101\n",
      "[2023-07-09 23:13:43,558 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 102\n",
      "[2023-07-09 23:13:43,630 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 103\n",
      "[2023-07-09 23:13:43,702 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 104\n",
      "[2023-07-09 23:13:43,778 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 105\n",
      "[2023-07-09 23:13:43,850 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 106\n",
      "[2023-07-09 23:13:43,921 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 107\n",
      "[2023-07-09 23:13:43,995 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 108\n",
      "[2023-07-09 23:13:44,068 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 109\n",
      "[2023-07-09 23:13:44,139 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 110\n",
      "[2023-07-09 23:13:44,213 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 111\n",
      "[2023-07-09 23:13:44,284 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 112\n",
      "[2023-07-09 23:13:47,099 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 113\n",
      "[2023-07-09 23:13:47,172 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 114\n",
      "[2023-07-09 23:13:47,242 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 115\n",
      "[2023-07-09 23:13:47,315 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 116\n",
      "[2023-07-09 23:13:47,392 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 117\n",
      "[2023-07-09 23:13:47,466 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 118\n",
      "[2023-07-09 23:13:47,542 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 119\n",
      "[2023-07-09 23:13:47,620 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 120\n",
      "[2023-07-09 23:13:47,700 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 121\n",
      "[2023-07-09 23:13:47,771 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 122\n",
      "[2023-07-09 23:13:47,849 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 123\n",
      "[2023-07-09 23:13:47,926 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 124\n",
      "[2023-07-09 23:13:48,002 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 125\n",
      "[2023-07-09 23:13:48,079 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 126\n",
      "[2023-07-09 23:13:48,157 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 127\n",
      "[2023-07-09 23:13:48,241 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 128\n",
      "[2023-07-09 23:13:48,326 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 129\n",
      "[2023-07-09 23:13:48,406 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 130\n",
      "[2023-07-09 23:13:48,490 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 131\n",
      "[2023-07-09 23:13:48,570 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 132\n",
      "[2023-07-09 23:14:13,471 INFO] Step 200/ 1200; acc: 92.8; ppl:   4.5; xent: 1.5; lr: 0.00079; sents:   52013; bsz: 3013/3799/260; 6393/8062 tok/s;    328 sec;\n",
      "[2023-07-09 23:15:28,353 INFO] Step 250/ 1200; acc: 98.5; ppl:   3.6; xent: 1.3; lr: 0.00099; sents:   49398; bsz: 2988/3797/247; 7980/10141 tok/s;    403 sec;\n",
      "[2023-07-09 23:16:43,836 INFO] Step 300/ 1200; acc: 97.3; ppl:   3.7; xent: 1.3; lr: 0.00119; sents:   50742; bsz: 3009/3808/254; 7973/10089 tok/s;    478 sec;\n",
      "[2023-07-09 23:16:43,839 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_300.pt\n",
      "[2023-07-09 23:18:00,474 INFO] Step 350/ 1200; acc: 99.7; ppl:   3.4; xent: 1.2; lr: 0.00139; sents:   50411; bsz: 2987/3811/252; 7795/9944 tok/s;    555 sec;\n",
      "[2023-07-09 23:18:41,125 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 133\n",
      "[2023-07-09 23:18:41,195 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 134\n",
      "[2023-07-09 23:18:41,267 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 135\n",
      "[2023-07-09 23:18:41,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 136\n",
      "[2023-07-09 23:18:43,725 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 137\n",
      "[2023-07-09 23:18:43,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 138\n",
      "[2023-07-09 23:18:43,873 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 139\n",
      "[2023-07-09 23:18:43,942 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 140\n",
      "[2023-07-09 23:18:44,017 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 141\n",
      "[2023-07-09 23:18:44,093 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 142\n",
      "[2023-07-09 23:18:44,166 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 143\n",
      "[2023-07-09 23:18:44,243 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 144\n",
      "[2023-07-09 23:18:44,318 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 145\n",
      "[2023-07-09 23:18:44,393 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 146\n",
      "[2023-07-09 23:18:44,465 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 147\n",
      "[2023-07-09 23:18:44,541 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 148\n",
      "[2023-07-09 23:18:44,616 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 149\n",
      "[2023-07-09 23:18:44,691 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 150\n",
      "[2023-07-09 23:18:44,766 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 151\n",
      "[2023-07-09 23:18:44,839 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 152\n",
      "[2023-07-09 23:18:44,910 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 153\n",
      "[2023-07-09 23:18:44,981 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 154\n",
      "[2023-07-09 23:18:45,055 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 155\n",
      "[2023-07-09 23:18:45,127 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 156\n",
      "[2023-07-09 23:18:45,199 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 157\n",
      "[2023-07-09 23:18:45,272 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 158\n",
      "[2023-07-09 23:18:48,012 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 159\n",
      "[2023-07-09 23:18:48,082 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 160\n",
      "[2023-07-09 23:18:48,147 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 161\n",
      "[2023-07-09 23:18:48,217 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 162\n",
      "[2023-07-09 23:18:48,288 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 163\n",
      "[2023-07-09 23:18:48,358 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 164\n",
      "[2023-07-09 23:18:48,427 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 165\n",
      "[2023-07-09 23:18:48,498 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 166\n",
      "[2023-07-09 23:18:48,568 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 167\n",
      "[2023-07-09 23:18:48,636 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 168\n",
      "[2023-07-09 23:18:48,705 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 169\n",
      "[2023-07-09 23:18:48,779 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 170\n",
      "[2023-07-09 23:18:48,848 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 171\n",
      "[2023-07-09 23:18:48,918 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 172\n",
      "[2023-07-09 23:18:48,990 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 173\n",
      "[2023-07-09 23:18:49,065 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 174\n",
      "[2023-07-09 23:18:49,138 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 175\n",
      "[2023-07-09 23:18:49,209 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 176\n",
      "[2023-07-09 23:18:49,285 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 177\n",
      "[2023-07-09 23:18:49,356 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 178\n",
      "[2023-07-09 23:18:49,427 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 179\n",
      "[2023-07-09 23:18:49,500 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 180\n",
      "[2023-07-09 23:18:49,571 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 181\n",
      "[2023-07-09 23:18:49,640 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 182\n",
      "[2023-07-09 23:18:49,712 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 183\n",
      "[2023-07-09 23:18:49,786 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 184\n",
      "[2023-07-09 23:18:49,865 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 185\n",
      "[2023-07-09 23:18:53,108 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 186\n",
      "[2023-07-09 23:18:53,180 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 187\n",
      "[2023-07-09 23:18:53,251 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 188\n",
      "[2023-07-09 23:18:53,325 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 189\n",
      "[2023-07-09 23:18:53,402 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 190\n",
      "[2023-07-09 23:18:53,479 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 191\n",
      "[2023-07-09 23:18:53,555 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 192\n",
      "[2023-07-09 23:18:53,629 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 193\n",
      "[2023-07-09 23:18:53,714 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 194\n",
      "[2023-07-09 23:18:53,795 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 195\n",
      "[2023-07-09 23:18:53,875 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 196\n",
      "[2023-07-09 23:18:53,952 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 197\n",
      "[2023-07-09 23:19:37,320 INFO] Step 400/ 1200; acc: 99.0; ppl:   3.5; xent: 1.2; lr: 0.00159; sents:   50562; bsz: 2985/3780/253; 6165/7806 tok/s;    652 sec;\n",
      "[2023-07-09 23:19:37,757 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.43639135360717773 s.\n",
      "[2023-07-09 23:19:37,759 INFO] Train perplexity: 8.59758\n",
      "[2023-07-09 23:19:37,759 INFO] Train accuracy: 82.8009\n",
      "[2023-07-09 23:19:37,759 INFO] Sentences processed: 404902\n",
      "[2023-07-09 23:19:37,759 INFO] Average bsz: 2997/3800/253\n",
      "[2023-07-09 23:19:37,759 INFO] Validation perplexity: 6.08053\n",
      "[2023-07-09 23:19:37,759 INFO] Validation accuracy: 86.5846\n",
      "[2023-07-09 23:19:37,759 INFO] Model is improving ppl: inf --> 6.08053.\n",
      "[2023-07-09 23:19:37,759 INFO] Model is improving acc: -inf --> 86.5846.\n",
      "[2023-07-09 23:20:53,191 INFO] Step 450/ 1200; acc: 99.7; ppl:   3.3; xent: 1.2; lr: 0.00178; sents:   49962; bsz: 2993/3808/250; 7889/10039 tok/s;    728 sec;\n",
      "[2023-07-09 23:22:08,473 INFO] Step 500/ 1200; acc: 99.6; ppl:   3.3; xent: 1.2; lr: 0.00197; sents:   51049; bsz: 3000/3804/255; 7970/10105 tok/s;    803 sec;\n",
      "[2023-07-09 23:23:23,397 INFO] Step 550/ 1200; acc: 99.1; ppl:   3.4; xent: 1.2; lr: 0.00188; sents:   51418; bsz: 3006/3804/257; 8023/10155 tok/s;    878 sec;\n",
      "[2023-07-09 23:23:47,456 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 198\n",
      "[2023-07-09 23:23:47,529 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 199\n",
      "[2023-07-09 23:23:47,601 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 200\n",
      "[2023-07-09 23:23:47,670 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 201\n",
      "[2023-07-09 23:23:47,739 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 202\n",
      "[2023-07-09 23:23:47,811 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 203\n",
      "[2023-07-09 23:23:47,882 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 204\n",
      "[2023-07-09 23:23:47,952 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 205\n",
      "[2023-07-09 23:23:48,021 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 206\n",
      "[2023-07-09 23:23:48,090 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 207\n",
      "[2023-07-09 23:23:48,164 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 208\n",
      "[2023-07-09 23:23:48,234 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 209\n",
      "[2023-07-09 23:23:48,306 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 210\n",
      "[2023-07-09 23:23:48,378 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 211\n",
      "[2023-07-09 23:23:48,446 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 212\n",
      "[2023-07-09 23:23:48,516 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 213\n",
      "[2023-07-09 23:23:50,983 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 214\n",
      "[2023-07-09 23:23:51,053 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 215\n",
      "[2023-07-09 23:23:51,122 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 216\n",
      "[2023-07-09 23:23:51,189 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 217\n",
      "[2023-07-09 23:23:51,261 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 218\n",
      "[2023-07-09 23:23:51,327 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 219\n",
      "[2023-07-09 23:23:51,394 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 220\n",
      "[2023-07-09 23:23:51,468 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 221\n",
      "[2023-07-09 23:23:51,536 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 222\n",
      "[2023-07-09 23:23:51,605 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 223\n",
      "[2023-07-09 23:23:51,679 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 224\n",
      "[2023-07-09 23:23:51,751 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 225\n",
      "[2023-07-09 23:23:51,815 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 226\n",
      "[2023-07-09 23:23:51,882 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 227\n",
      "[2023-07-09 23:23:51,950 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 228\n",
      "[2023-07-09 23:23:52,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 229\n",
      "[2023-07-09 23:23:52,092 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 230\n",
      "[2023-07-09 23:23:52,161 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 231\n",
      "[2023-07-09 23:23:52,233 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 232\n",
      "[2023-07-09 23:23:52,298 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 233\n",
      "[2023-07-09 23:23:52,369 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 234\n",
      "[2023-07-09 23:23:52,442 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 235\n",
      "[2023-07-09 23:23:52,508 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 236\n",
      "[2023-07-09 23:23:52,576 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 237\n",
      "[2023-07-09 23:23:55,482 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 238\n",
      "[2023-07-09 23:23:55,545 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 239\n",
      "[2023-07-09 23:23:55,612 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 240\n",
      "[2023-07-09 23:23:55,679 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 241\n",
      "[2023-07-09 23:23:55,750 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 242\n",
      "[2023-07-09 23:23:55,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 243\n",
      "[2023-07-09 23:23:55,882 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 244\n",
      "[2023-07-09 23:23:55,952 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 245\n",
      "[2023-07-09 23:23:56,017 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 246\n",
      "[2023-07-09 23:23:56,084 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 247\n",
      "[2023-07-09 23:23:56,153 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 248\n",
      "[2023-07-09 23:23:56,221 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 249\n",
      "[2023-07-09 23:23:56,291 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 250\n",
      "[2023-07-09 23:23:56,359 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 251\n",
      "[2023-07-09 23:23:56,429 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 252\n",
      "[2023-07-09 23:23:56,495 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 253\n",
      "[2023-07-09 23:23:56,565 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 254\n",
      "[2023-07-09 23:23:56,634 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 255\n",
      "[2023-07-09 23:23:56,702 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 256\n",
      "[2023-07-09 23:23:56,776 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 257\n",
      "[2023-07-09 23:23:56,847 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 258\n",
      "[2023-07-09 23:23:56,919 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 259\n",
      "[2023-07-09 23:23:56,988 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 260\n",
      "[2023-07-09 23:23:57,059 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 261\n",
      "[2023-07-09 23:23:57,135 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 262\n",
      "[2023-07-09 23:23:57,203 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 263\n",
      "[2023-07-09 23:25:00,532 INFO] Step 600/ 1200; acc: 99.5; ppl:   3.4; xent: 1.2; lr: 0.00180; sents:   51661; bsz: 3009/3798/258; 6196/7821 tok/s;    975 sec;\n",
      "[2023-07-09 23:25:00,535 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_600.pt\n",
      "[2023-07-09 23:26:16,008 INFO] Step 650/ 1200; acc: 99.8; ppl:   3.3; xent: 1.2; lr: 0.00173; sents:   50003; bsz: 2979/3792/250; 7895/10047 tok/s;   1051 sec;\n",
      "[2023-07-09 23:27:30,688 INFO] Step 700/ 1200; acc: 99.5; ppl:   3.4; xent: 1.2; lr: 0.00167; sents:   50102; bsz: 3003/3810/251; 8043/10203 tok/s;   1125 sec;\n",
      "[2023-07-09 23:28:45,352 INFO] Step 750/ 1200; acc: 99.9; ppl:   3.3; xent: 1.2; lr: 0.00161; sents:   51090; bsz: 3004/3809/255; 8047/10203 tok/s;   1200 sec;\n",
      "[2023-07-09 23:28:52,977 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 264\n",
      "[2023-07-09 23:28:53,050 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 265\n",
      "[2023-07-09 23:28:53,119 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 266\n",
      "[2023-07-09 23:28:53,188 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 267\n",
      "[2023-07-09 23:28:53,257 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 268\n",
      "[2023-07-09 23:28:53,325 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 269\n",
      "[2023-07-09 23:28:53,395 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 270\n",
      "[2023-07-09 23:28:53,465 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 271\n",
      "[2023-07-09 23:28:53,533 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 272\n",
      "[2023-07-09 23:28:53,605 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 273\n",
      "[2023-07-09 23:28:53,675 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 274\n",
      "[2023-07-09 23:28:53,742 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 275\n",
      "[2023-07-09 23:28:53,813 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 276\n",
      "[2023-07-09 23:28:53,886 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 277\n",
      "[2023-07-09 23:28:53,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 278\n",
      "[2023-07-09 23:28:54,022 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 279\n",
      "[2023-07-09 23:28:54,094 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 280\n",
      "[2023-07-09 23:28:54,163 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 281\n",
      "[2023-07-09 23:28:54,237 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 282\n",
      "[2023-07-09 23:28:54,309 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 283\n",
      "[2023-07-09 23:28:54,375 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 284\n",
      "[2023-07-09 23:28:54,442 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 285\n",
      "[2023-07-09 23:28:54,511 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 286\n",
      "[2023-07-09 23:28:54,581 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 287\n",
      "[2023-07-09 23:28:54,655 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 288\n",
      "[2023-07-09 23:28:54,723 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 289\n",
      "[2023-07-09 23:28:54,793 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 290\n",
      "[2023-07-09 23:28:54,861 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 291\n",
      "[2023-07-09 23:28:54,930 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 292\n",
      "[2023-07-09 23:28:54,996 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 293\n",
      "[2023-07-09 23:28:55,067 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 294\n",
      "[2023-07-09 23:28:55,137 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 295\n",
      "[2023-07-09 23:28:55,206 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 296\n",
      "[2023-07-09 23:28:55,280 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 297\n",
      "[2023-07-09 23:28:55,349 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 298\n",
      "[2023-07-09 23:28:58,149 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 299\n",
      "[2023-07-09 23:28:58,218 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 300\n",
      "[2023-07-09 23:28:58,286 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 301\n",
      "[2023-07-09 23:28:58,353 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 302\n",
      "[2023-07-09 23:28:58,423 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 303\n",
      "[2023-07-09 23:28:58,492 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 304\n",
      "[2023-07-09 23:28:58,556 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 305\n",
      "[2023-07-09 23:28:58,627 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 306\n",
      "[2023-07-09 23:28:58,695 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 307\n",
      "[2023-07-09 23:28:58,764 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 308\n",
      "[2023-07-09 23:28:58,835 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 309\n",
      "[2023-07-09 23:28:58,904 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 310\n",
      "[2023-07-09 23:28:58,970 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 311\n",
      "[2023-07-09 23:28:59,042 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 312\n",
      "[2023-07-09 23:28:59,110 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 313\n",
      "[2023-07-09 23:28:59,179 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 314\n",
      "[2023-07-09 23:28:59,250 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 315\n",
      "[2023-07-09 23:28:59,319 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 316\n",
      "[2023-07-09 23:28:59,390 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 317\n",
      "[2023-07-09 23:28:59,459 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 318\n",
      "[2023-07-09 23:28:59,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 319\n",
      "[2023-07-09 23:28:59,604 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 320\n",
      "[2023-07-09 23:28:59,674 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 321\n",
      "[2023-07-09 23:28:59,747 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 322\n",
      "[2023-07-09 23:28:59,817 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 323\n",
      "[2023-07-09 23:28:59,888 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 324\n",
      "[2023-07-09 23:28:59,960 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 325\n",
      "[2023-07-09 23:29:00,030 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 326\n",
      "[2023-07-09 23:29:00,097 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 327\n",
      "[2023-07-09 23:29:03,449 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 328\n",
      "[2023-07-09 23:30:20,306 INFO] Step 800/ 1200; acc: 99.7; ppl:   3.3; xent: 1.2; lr: 0.00156; sents:   51229; bsz: 3002/3803/256; 6324/8010 tok/s;   1295 sec;\n",
      "[2023-07-09 23:30:20,754 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.44590163230895996 s.\n",
      "[2023-07-09 23:30:20,755 INFO] Train perplexity: 5.36524\n",
      "[2023-07-09 23:30:20,755 INFO] Train accuracy: 91.2014\n",
      "[2023-07-09 23:30:20,755 INFO] Sentences processed: 811416\n",
      "[2023-07-09 23:30:20,755 INFO] Average bsz: 2998/3802/254\n",
      "[2023-07-09 23:30:20,755 INFO] Validation perplexity: 6.0483\n",
      "[2023-07-09 23:30:20,755 INFO] Validation accuracy: 86.8097\n",
      "[2023-07-09 23:30:20,755 INFO] Model is improving ppl: 6.08053 --> 6.0483.\n",
      "[2023-07-09 23:30:20,755 INFO] Model is improving acc: 86.5846 --> 86.8097.\n",
      "[2023-07-09 23:31:35,375 INFO] Step 850/ 1200; acc: 99.6; ppl:   3.3; xent: 1.2; lr: 0.00151; sents:   50505; bsz: 2996/3809/253; 7982/10149 tok/s;   1370 sec;\n",
      "[2023-07-09 23:32:50,173 INFO] Step 900/ 1200; acc: 99.8; ppl:   3.3; xent: 1.2; lr: 0.00147; sents:   50615; bsz: 3007/3810/253; 8041/10187 tok/s;   1445 sec;\n",
      "[2023-07-09 23:32:50,176 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_900.pt\n",
      "[2023-07-09 23:33:57,848 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 329\n",
      "[2023-07-09 23:33:57,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 330\n",
      "[2023-07-09 23:33:57,997 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 331\n",
      "[2023-07-09 23:33:58,070 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 332\n",
      "[2023-07-09 23:33:58,143 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 333\n",
      "[2023-07-09 23:33:58,214 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 334\n",
      "[2023-07-09 23:33:58,287 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 335\n",
      "[2023-07-09 23:33:58,358 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 336\n",
      "[2023-07-09 23:33:58,431 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 337\n",
      "[2023-07-09 23:33:58,500 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 338\n",
      "[2023-07-09 23:33:58,574 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 339\n",
      "[2023-07-09 23:33:58,645 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 340\n",
      "[2023-07-09 23:33:58,712 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 341\n",
      "[2023-07-09 23:33:58,788 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 342\n",
      "[2023-07-09 23:33:58,861 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 343\n",
      "[2023-07-09 23:33:58,933 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 344\n",
      "[2023-07-09 23:33:59,006 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 345\n",
      "[2023-07-09 23:33:59,077 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 346\n",
      "[2023-07-09 23:33:59,147 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 347\n",
      "[2023-07-09 23:33:59,218 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 348\n",
      "[2023-07-09 23:33:59,293 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 349\n",
      "[2023-07-09 23:33:59,365 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 350\n",
      "[2023-07-09 23:33:59,437 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 351\n",
      "[2023-07-09 23:33:59,507 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 352\n",
      "[2023-07-09 23:33:59,578 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 353\n",
      "[2023-07-09 23:33:59,652 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 354\n",
      "[2023-07-09 23:33:59,719 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 355\n",
      "[2023-07-09 23:33:59,790 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 356\n",
      "[2023-07-09 23:33:59,861 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 357\n",
      "[2023-07-09 23:33:59,937 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 358\n",
      "[2023-07-09 23:34:00,009 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 359\n",
      "[2023-07-09 23:34:00,083 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 360\n",
      "[2023-07-09 23:34:00,155 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 361\n",
      "[2023-07-09 23:34:03,006 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 362\n",
      "[2023-07-09 23:34:03,078 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 363\n",
      "[2023-07-09 23:34:03,147 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 364\n",
      "[2023-07-09 23:34:03,219 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 365\n",
      "[2023-07-09 23:34:03,292 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 366\n",
      "[2023-07-09 23:34:03,366 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 367\n",
      "[2023-07-09 23:34:03,435 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 368\n",
      "[2023-07-09 23:34:03,507 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 369\n",
      "[2023-07-09 23:34:03,578 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 370\n",
      "[2023-07-09 23:34:03,650 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 371\n",
      "[2023-07-09 23:34:03,723 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 372\n",
      "[2023-07-09 23:34:03,791 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 373\n",
      "[2023-07-09 23:34:03,860 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 374\n",
      "[2023-07-09 23:34:03,926 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 375\n",
      "[2023-07-09 23:34:03,997 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 376\n",
      "[2023-07-09 23:34:04,069 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 377\n",
      "[2023-07-09 23:34:04,139 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 378\n",
      "[2023-07-09 23:34:04,214 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 379\n",
      "[2023-07-09 23:34:04,285 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 380\n",
      "[2023-07-09 23:34:04,354 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 381\n",
      "[2023-07-09 23:34:04,423 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 382\n",
      "[2023-07-09 23:34:04,494 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 383\n",
      "[2023-07-09 23:34:04,564 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 384\n",
      "[2023-07-09 23:34:04,634 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 385\n",
      "[2023-07-09 23:34:04,703 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 386\n",
      "[2023-07-09 23:34:04,774 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 387\n",
      "[2023-07-09 23:34:04,844 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 388\n",
      "[2023-07-09 23:34:04,914 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 389\n",
      "[2023-07-09 23:34:04,988 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 390\n",
      "[2023-07-09 23:34:08,390 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 391\n",
      "[2023-07-09 23:34:08,474 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 392\n",
      "[2023-07-09 23:34:08,544 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 393\n",
      "[2023-07-09 23:34:08,612 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 394\n",
      "[2023-07-09 23:34:27,348 INFO] Step 950/ 1200; acc: 99.5; ppl:   3.3; xent: 1.2; lr: 0.00143; sents:   50138; bsz: 2987/3786/251; 6148/7793 tok/s;   1542 sec;\n",
      "[2023-07-09 23:35:42,417 INFO] Step 1000/ 1200; acc: 99.8; ppl:   3.3; xent: 1.2; lr: 0.00140; sents:   50683; bsz: 3006/3807/253; 8008/10144 tok/s;   1617 sec;\n",
      "[2023-07-09 23:36:57,273 INFO] Step 1050/ 1200; acc: 99.9; ppl:   3.3; xent: 1.2; lr: 0.00136; sents:   49981; bsz: 2992/3809/250; 7993/10176 tok/s;   1692 sec;\n",
      "[2023-07-09 23:38:12,075 INFO] Step 1100/ 1200; acc: 99.9; ppl:   3.3; xent: 1.2; lr: 0.00133; sents:   51013; bsz: 3001/3806/255; 8025/10177 tok/s;   1767 sec;\n",
      "[2023-07-09 23:39:01,263 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 395\n",
      "[2023-07-09 23:39:01,344 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 396\n",
      "[2023-07-09 23:39:01,415 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 397\n",
      "[2023-07-09 23:39:01,492 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 398\n",
      "[2023-07-09 23:39:01,571 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 399\n",
      "[2023-07-09 23:39:01,644 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 400\n",
      "[2023-07-09 23:39:01,718 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 401\n",
      "[2023-07-09 23:39:01,792 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 402\n",
      "[2023-07-09 23:39:01,868 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 403\n",
      "[2023-07-09 23:39:01,944 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 404\n",
      "[2023-07-09 23:39:02,024 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 405\n",
      "[2023-07-09 23:39:02,105 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 406\n",
      "[2023-07-09 23:39:02,179 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 407\n",
      "[2023-07-09 23:39:02,259 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 408\n",
      "[2023-07-09 23:39:02,328 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 409\n",
      "[2023-07-09 23:39:02,401 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 410\n",
      "[2023-07-09 23:39:02,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 411\n",
      "[2023-07-09 23:39:02,544 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 412\n",
      "[2023-07-09 23:39:02,616 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 413\n",
      "[2023-07-09 23:39:02,687 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 414\n",
      "[2023-07-09 23:39:02,761 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 415\n",
      "[2023-07-09 23:39:02,827 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 416\n",
      "[2023-07-09 23:39:02,897 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 417\n",
      "[2023-07-09 23:39:02,968 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 418\n",
      "[2023-07-09 23:39:03,036 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 419\n",
      "[2023-07-09 23:39:03,109 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 420\n",
      "[2023-07-09 23:39:03,183 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 421\n",
      "[2023-07-09 23:39:03,254 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 422\n",
      "[2023-07-09 23:39:05,966 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 423\n",
      "[2023-07-09 23:39:06,036 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 424\n",
      "[2023-07-09 23:39:06,107 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 425\n",
      "[2023-07-09 23:39:06,180 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 426\n",
      "[2023-07-09 23:39:06,255 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 427\n",
      "[2023-07-09 23:39:06,327 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 428\n",
      "[2023-07-09 23:39:06,400 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 429\n",
      "[2023-07-09 23:39:06,470 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 430\n",
      "[2023-07-09 23:39:06,541 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 431\n",
      "[2023-07-09 23:39:06,611 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 432\n",
      "[2023-07-09 23:39:06,684 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 433\n",
      "[2023-07-09 23:39:06,758 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 434\n",
      "[2023-07-09 23:39:06,835 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 435\n",
      "[2023-07-09 23:39:06,908 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 436\n",
      "[2023-07-09 23:39:06,977 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 437\n",
      "[2023-07-09 23:39:07,048 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 438\n",
      "[2023-07-09 23:39:07,123 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 439\n",
      "[2023-07-09 23:39:07,194 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 440\n",
      "[2023-07-09 23:39:07,264 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 441\n",
      "[2023-07-09 23:39:07,331 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 442\n",
      "[2023-07-09 23:39:07,405 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 443\n",
      "[2023-07-09 23:39:07,474 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 444\n",
      "[2023-07-09 23:39:07,543 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 445\n",
      "[2023-07-09 23:39:07,619 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 446\n",
      "[2023-07-09 23:39:07,691 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 447\n",
      "[2023-07-09 23:39:07,760 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 448\n",
      "[2023-07-09 23:39:07,832 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 449\n",
      "[2023-07-09 23:39:07,902 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 450\n",
      "[2023-07-09 23:39:11,133 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 451\n",
      "[2023-07-09 23:39:11,203 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 452\n",
      "[2023-07-09 23:39:11,271 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 453\n",
      "[2023-07-09 23:39:11,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 454\n",
      "[2023-07-09 23:39:11,410 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 455\n",
      "[2023-07-09 23:39:11,480 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 456\n",
      "[2023-07-09 23:39:11,544 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 457\n",
      "[2023-07-09 23:39:11,613 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 458\n",
      "[2023-07-09 23:39:11,682 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 459\n",
      "[2023-07-09 23:39:46,208 INFO] Step 1150/ 1200; acc: 99.6; ppl:   3.3; xent: 1.2; lr: 0.00130; sents:   51923; bsz: 3000/3777/260; 6375/8025 tok/s;   1861 sec;\n",
      "[2023-07-09 23:41:01,133 INFO] Step 1200/ 1200; acc: 99.9; ppl:   3.3; xent: 1.2; lr: 0.00128; sents:   49673; bsz: 2997/3813/248; 8000/10179 tok/s;   1936 sec;\n",
      "[2023-07-09 23:41:01,547 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.41261887550354004 s.\n",
      "[2023-07-09 23:41:01,548 INFO] Train perplexity: 4.56829\n",
      "[2023-07-09 23:41:01,548 INFO] Train accuracy: 94.055\n",
      "[2023-07-09 23:41:01,548 INFO] Sentences processed: 1.21595e+06\n",
      "[2023-07-09 23:41:01,548 INFO] Average bsz: 2998/3802/253\n",
      "[2023-07-09 23:41:01,548 INFO] Validation perplexity: 6.19735\n",
      "[2023-07-09 23:41:01,548 INFO] Validation accuracy: 87.4983\n",
      "[2023-07-09 23:41:01,548 INFO] Stalled patience: 3/4\n",
      "[2023-07-09 23:41:01,551 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1200.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the NMT model using the configuration defined in 'config.yaml'\n",
    "!onmt_train -config config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688946064480,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "ro-21ODLnBNU",
    "outputId": "1f16d45d-1057-4041-ff27-03751e31821e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step_1200.pt  model_step_600.pt  src.vocab\n",
      "model_step_300.pt   model_step_900.pt  train.log\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the 'model_root' directory\n",
    "!ls '{model_root}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRou9WfsDbh"
   },
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42619,
     "status": "ok",
     "timestamp": 1688946107097,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tStHVtbNmipI",
    "outputId": "c54d115d-1110-4dcc-bf68-991177eca55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 23:41:05,563 INFO] Loading checkpoint from /content/nmt/nmtmodel/model_step_1200.pt\n",
      "[2023-07-09 23:41:06,162 INFO] Loading data into the model\n",
      "[2023-07-09 23:41:46,604 INFO] PRED SCORE: -0.2086, PRED PPL: 1.23 NB SENTENCES: 500\n"
     ]
    }
   ],
   "source": [
    "# Perform translation using the trained NMT model\n",
    "# --model specifies the path to the trained model checkpoint\n",
    "# --src specifies the path to the source text file to be translated\n",
    "# --output specifies the path to save the translated output\n",
    "# -beam_size specifies the beam size for beam search\n",
    "!onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src /content/nmt/lcquad/test.en --output /content/nmt/lcquad/trans_test.sparql -beam_size 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1688946107374,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "UYszCfIbssof",
    "outputId": "2bb3f5fb-c47f-4292-85f1-50998f7cddff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list the products of the company which published tweenies: game time .\n",
      "name the common sports played at polytechnic university of philippines san juan and islamic azad university ?\n",
      "which company developed both dart and go ?\n",
      "count the total number of launch site of the rockets which have been launched form cape canaveral air force station ?\n",
      "to which settlement does elliot bay belong to ?\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.en'\n",
    "!head -n 5 /content/nmt/lcquad/test.en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688946107374,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "nnTRp6eas_p7",
    "outputId": "5daffc40-d50a-4d55-d493-c849fc7e2362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_publisher> <unk> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <unk> <dbo_sport> var_uri sep_dot <dbr_Islamic_Azad_University_Central_Tehran_Branch> <dbo_sport> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Dart_ attr_open programming_language attr_close math_gt <dbp_developer> var_uri sep_dot <dbr_Dart_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_manufacturer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <unk> attr_open <unk> attr_close math_gt <dbp_state> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'trans_test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688946107375,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "azi6x-EstHYN",
    "outputId": "9734ccbc-f7eb-4f4b-e689-2dd47cac268f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Polytechnic_University_of_the_Philippines_San_Juan> <dbo_sport> var_uri sep_dot <dbr_Islamic_Azad_University_Central_Tehran_Branch> <dbo_sport> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Dart_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot <dbr_Go_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_launchSite> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Elliott_Bay> <dbp_cities> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/test.sparql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vANuMTJUtfUb"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1688946107730,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "kj1AtrSDtjfq",
    "outputId": "30cdcbb1-d934-4ef0-e990-75c9575a619f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1688946107929,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9OhvK6N3ecsm",
    "outputId": "52f3ec86-06fb-412f-cef9-fad96a9743bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_publisher> <unk> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Accuracy:  0.6912572152611572\n"
     ]
    }
   ],
   "source": [
    "# Copy the file \"compute-accuracy.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-accuracy.py ./\n",
    "\n",
    "# Evaluate the translation using the provided accuracy computation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-accuracy.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5689,
     "status": "ok",
     "timestamp": 1688946113615,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "CBxRt66trVw5",
    "outputId": "77f4bdd2-5bf3-4607-b6fc-10aadf2cf7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_publisher> <unk> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "BLEU:  67.49842044355714\n"
     ]
    }
   ],
   "source": [
    "# Install the sacrebleu library using pip\n",
    "!pip install sacrebleu > /dev/null\n",
    "\n",
    "# Copy the file \"compute-bleu.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-bleu.py ./\n",
    "\n",
    "# Evaluate the translation using BLEU score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-bleu.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4946,
     "status": "ok",
     "timestamp": 1688946118556,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "m8OCPRSAxQ_I",
    "outputId": "b69fb23d-9e3e-4ac2-87ca-57656a05d9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_publisher> <unk> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Rouge-L:  0.7767762628699074\n"
     ]
    }
   ],
   "source": [
    "# Install the rouge library using pip\n",
    "!pip install rouge > /dev/null\n",
    "\n",
    "# Copy the file \"compute-rouge-l.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-rouge-l.py ./\n",
    "\n",
    "# Evaluate the translation using Rouge-L score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-rouge-l.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSDN_iq-2LJ0"
   },
   "outputs": [],
   "source": [
    "# Copy the directory 'nmt' and its contents from '/content/nmt' to '/content/drive/MyDrive'\n",
    "!cp -r /content/nmt /content/drive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L_nKJQMr2nO"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu7lspbw1Gxw"
   },
   "outputs": [],
   "source": [
    "# Define a list of sentences\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here comes the second sentence.\"\n",
    "]\n",
    "\n",
    "# Open a file named 'questions.en' in write mode\n",
    "with open('questions.en', 'w') as fp:\n",
    "    # Combine the sentences into a single string\n",
    "    combined_sentences = [''.join(x) for x in sentences]\n",
    "    \n",
    "    # Combine the sentences using newline characters as separators\n",
    "    combined_sentences = '\\n'.join(combined_sentences)\n",
    "    \n",
    "    # Write the combined sentences to the file\n",
    "    fp.write(combined_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform translation using the trained NMT model\n",
    "# --model specifies the path to the trained model checkpoint\n",
    "# --src specifies the path to the source text file to be translated\n",
    "# --output specifies the path to save the translated output\n",
    "!onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src questions.en --output pred.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the contents of the file 'pred.sparql'\n",
    "!cat pred.sparql\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMU/TZ1QDHkhMxiwDqN2TZY",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1UpvkqP7xdeVFN64-pY1HhxP1Idlj1eay",
   "provenance": [
    {
     "file_id": "1qwBCXsrX0UD96jZU-P9vIhFRTnCZYO6m",
     "timestamp": 1686823688936
    },
    {
     "file_id": "1sYi4GQuLKBmhhiOJvNHDacHYPI39LsG-",
     "timestamp": 1686759059262
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
