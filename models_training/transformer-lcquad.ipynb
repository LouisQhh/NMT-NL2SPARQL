{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w2wNk0167DQ"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1688923776522,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "6ZH1UZrpjtPE",
    "outputId": "f687b141-08e0-4422-914c-467028ba5176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Create a directory named 'nmt'\n",
    "!mkdir nmt\n",
    "\n",
    "# Change the current working directory to the newly created 'nmt' directory\n",
    "%cd nmt\n",
    "\n",
    "# Create a subdirectory named 'nmtmodel' inside the 'nmt' directory\n",
    "!mkdir nmtmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126350,
     "status": "ok",
     "timestamp": 1688923904091,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "df6W9IIWmlQL",
    "outputId": "7a578078-94d6-4c4e-f760-d08ab7bef3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install OpenNMT-py along with specific versions of torchvision and torchaudio\n",
    "! pip install OpenNMT-py torchvision==0.14.1 torchaudio==0.13.1 > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1688923904091,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "_3WnsZ-1nUSj",
    "outputId": "f67db56f-bde4-429a-eeb2-b995c6646bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QypsRuOq7IiR"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1688923904726,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "Jsc7THqZnYCI",
    "outputId": "67f2fca7-a630-4cbc-963b-cc26e191411a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./lcquad.zip\n",
      "   creating: ./lcquad/\n",
      "  inflating: ./__MACOSX/._lcquad     \n",
      "  inflating: ./lcquad/dev.en         \n",
      "  inflating: ./__MACOSX/lcquad/._dev.en  \n",
      "  inflating: ./lcquad/dev.sparql     \n",
      "  inflating: ./__MACOSX/lcquad/._dev.sparql  \n",
      "  inflating: ./lcquad/.DS_Store      \n",
      "  inflating: ./__MACOSX/lcquad/._.DS_Store  \n",
      "  inflating: ./lcquad/train.sparql   \n",
      "  inflating: ./__MACOSX/lcquad/._train.sparql  \n",
      "  inflating: ./lcquad/train.en       \n",
      "  inflating: ./__MACOSX/lcquad/._train.en  \n",
      "  inflating: ./lcquad/test.sparql    \n",
      "  inflating: ./__MACOSX/lcquad/._test.sparql  \n",
      "  inflating: ./lcquad/test.en        \n",
      "  inflating: ./__MACOSX/lcquad/._test.en  \n"
     ]
    }
   ],
   "source": [
    "# Copy the file 'lcquad.zip' from '/content/drive/MyDrive/' to the current directory\n",
    "!cp /content/drive/MyDrive/lcquad.zip ./\n",
    "\n",
    "# Unzip the file 'lcquad.zip' and extract its contents to the current directory\n",
    "!unzip ./lcquad.zip -d ./\n",
    "\n",
    "# Remove the original zip file 'lcquad.zip' from '/content/nmt/'\n",
    "!rm /content/nmt/lcquad.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1688923905044,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "dbGofKann5xB",
    "outputId": "84d519ed-4b40-4efa-d3b0-ab13e7f214fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcquad\t__MACOSX  nmtmodel\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the current directory\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Nfw0l77P8e"
   },
   "source": [
    "# Create the Training Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1688923905044,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "9uBY92Mf8vE6"
   },
   "outputs": [],
   "source": [
    "# Define the root directory path for the NMT model as \"model_root\"\n",
    "model_root = '/content/nmt/nmtmodel'\n",
    "\n",
    "# Create the directory structure for the NMT model using the mkdir command with the -p option\n",
    "# This ensures that the entire directory path is created, including any necessary parent directories\n",
    "!mkdir -p '{model_root}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688923905045,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "xAJ686rIvLRn"
   },
   "outputs": [],
   "source": [
    "# Define the configuration as a formatted string\n",
    "config = f'''# config.yaml\n",
    "## Where the samples will be written\n",
    "save_data: {model_root}\n",
    "\n",
    "## Where the vocab(s) will be written\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: {model_root}/src.vocab\n",
    "tgt_vocab: {model_root}/src.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 11000\n",
    "tgt_vocab_size: 11000\n",
    "share_vocab: true\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    train:\n",
    "        path_src: /content/nmt/lcquad/train.en\n",
    "        path_tgt: /content/nmt/lcquad/train.sparql\n",
    "    valid:\n",
    "        path_src: /content/nmt/lcquad/dev.en\n",
    "        path_tgt: /content/nmt/lcquad/dev.sparql\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: {model_root}/model\n",
    "log_file: {model_root}/train.log\n",
    "save_checkpoint_steps: 100\n",
    "train_steps: 1200\n",
    "valid_steps: 400\n",
    "\n",
    "# Stop training if it does not imporve after n validations\n",
    "early_stopping: 4\n",
    "\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 4242\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "# queue_size: 100\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "# world_size: 1\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "# model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "# learning_rate: 2\n",
    "warmup_steps: 500\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.98\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "# dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "'''\n",
    "\n",
    "# Write the configuration to a \"config.yaml\" file\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1ZGev3N7ao3"
   },
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2471,
     "status": "ok",
     "timestamp": 1688923907512,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "F4jDz0Dr7sne",
    "outputId": "8b46dcea-ac52-4635-b1b2-bbeaae8fe115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 17:31:46,273 INFO] Counter vocab from -1 samples.\n",
      "[2023-07-09 17:31:46,273 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2023-07-09 17:31:46,451 INFO] Counters src: 6405\n",
      "[2023-07-09 17:31:46,451 INFO] Counters tgt: 4412\n",
      "[2023-07-09 17:31:46,452 INFO] Counters after share:10760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the source vocabulary file doesn't exist in the 'model_root' directory\n",
    "if not os.path.exists(os.path.join(model_root, 'src.vocab')):\n",
    "    # Build the source vocabulary using the onmt_build_vocab command and the provided config.yaml\n",
    "    # The --n_sample option is used to indicate the number of samples to consider for building the vocabulary\n",
    "    # The \"|| true\" at the end ensures that the command won't stop the script even if there's an error\n",
    "    !onmt_build_vocab -config config.yaml --n_sample -1 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V312rtxK7pd7"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4146,
     "status": "ok",
     "timestamp": 1688923911651,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tJ4dYNPtvQ5Q",
    "outputId": "a963d36f-80bd-4e72-9a67-1c63cc46cb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  9 17:31:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "GPU:\n",
      "GPU 0: Tesla T4 (UUID: GPU-33886c72-7447-6059-4711-64d752494947)\n",
      "$*****************************************************************************$\n",
      "\n",
      "\n",
      "$*****************************************************************************$\n",
      "True\n",
      "Tesla T4\n",
      "Free GPU memory: 14998.8125 out of: 15101.8125\n",
      "$*****************************************************************************$\n"
     ]
    }
   ],
   "source": [
    "# Check NVIDIA GPU information using the 'nvidia-smi' command\n",
    "!nvidia-smi\n",
    "\n",
    "# Print a separator line and heading for the GPU information\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "print('GPU:')\n",
    "\n",
    "# Check and display the GPU devices using 'nvidia-smi -L'\n",
    "!nvidia-smi -L\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print a separator line and heading for GPU-related checks\n",
    "print('\\n\\n$*****************************************************************************$')\n",
    "\n",
    "# Check if CUDA-enabled GPU is available for PyTorch\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Print the name of the first CUDA-enabled GPU\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Get GPU memory information\n",
    "gpu_memory = torch.cuda.mem_get_info(0)\n",
    "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print system information using 'lsb_release -a'\n",
    "!lsb_release -a\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print kernel version using 'uname -r'\n",
    "!uname -r\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Print NVCC (NVIDIA CUDA Compiler) version using 'nvcc --version'\n",
    "!nvcc --version\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Check Torch version using Python import\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display CPU information using 'cat /proc/cpuinfo | grep model\\ name'\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Print a separator line\n",
    "print('$*****************************************************************************$')\n",
    "\n",
    "# Display total memory information using 'cat /proc/meminfo | grep MemTotal'\n",
    "!cat /proc/meminfo | grep MemTotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmaMw_sc7gvP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2071539,
     "status": "ok",
     "timestamp": 1688925983188,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "5iMmswvHvta_",
    "outputId": "3e79e875-7de3-45a5-9f1d-60717eaadc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 17:31:54,023 INFO] Missing transforms field for train data, set to default: [].\n",
      "[2023-07-09 17:31:54,023 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
      "[2023-07-09 17:31:54,023 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2023-07-09 17:31:54,023 INFO] Parsed 2 corpora from -data.\n",
      "[2023-07-09 17:31:54,024 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2023-07-09 17:31:54,056 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'var_uri', 'sep_dot', 'WHERE', 'brack_open', 'brack_close', 'the']\n",
      "[2023-07-09 17:31:54,056 INFO] The decoder start token is: <s>\n",
      "[2023-07-09 17:31:54,056 INFO] Building model...\n",
      "[2023-07-09 17:31:54,690 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2023-07-09 17:31:54,691 INFO] Non quantized layer compute is fp32\n",
      "[2023-07-09 17:31:55,676 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10768, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10768, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=10768, bias=True)\n",
      ")\n",
      "[2023-07-09 17:31:55,753 INFO] encoder: 24400896\n",
      "[2023-07-09 17:31:55,753 INFO] decoder: 36222480\n",
      "[2023-07-09 17:31:55,754 INFO] * number of parameters: 60623376\n",
      "[2023-07-09 17:31:55,754 INFO] Trainable parameters = {'torch.float32': 60623376, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 17:31:55,754 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-09 17:31:55,754 INFO]  * src vocab size = 10768\n",
      "[2023-07-09 17:31:55,754 INFO]  * tgt vocab size = 10768\n",
      "[2023-07-09 17:31:55,756 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 1\n",
      "[2023-07-09 17:31:55,757 INFO] Starting training on GPU: [0]\n",
      "[2023-07-09 17:31:55,757 INFO] Start training loop and validate every 400 steps...\n",
      "[2023-07-09 17:31:55,757 INFO] Scoring with: TransformPipe()\n",
      "[2023-07-09 17:31:55,820 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 2\n",
      "[2023-07-09 17:31:55,883 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 3\n",
      "[2023-07-09 17:31:55,948 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 4\n",
      "[2023-07-09 17:31:56,081 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 5\n",
      "[2023-07-09 17:31:56,147 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 6\n",
      "[2023-07-09 17:31:56,213 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 7\n",
      "[2023-07-09 17:31:56,280 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 8\n",
      "[2023-07-09 17:31:56,462 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 9\n",
      "[2023-07-09 17:31:56,530 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 10\n",
      "[2023-07-09 17:31:56,597 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 11\n",
      "[2023-07-09 17:31:56,662 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 12\n",
      "[2023-07-09 17:31:56,896 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 13\n",
      "[2023-07-09 17:31:56,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 14\n",
      "[2023-07-09 17:31:57,029 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 15\n",
      "[2023-07-09 17:31:57,094 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 16\n",
      "[2023-07-09 17:31:57,386 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 17\n",
      "[2023-07-09 17:31:57,455 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 18\n",
      "[2023-07-09 17:31:57,520 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 19\n",
      "[2023-07-09 17:31:57,583 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 20\n",
      "[2023-07-09 17:31:57,674 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 21\n",
      "[2023-07-09 17:31:58,024 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 22\n",
      "[2023-07-09 17:31:58,088 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 23\n",
      "[2023-07-09 17:31:58,149 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 24\n",
      "[2023-07-09 17:31:58,217 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 25\n",
      "[2023-07-09 17:31:58,284 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 26\n",
      "[2023-07-09 17:31:58,349 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 27\n",
      "[2023-07-09 17:31:58,414 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 28\n",
      "[2023-07-09 17:31:58,864 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 29\n",
      "[2023-07-09 17:31:58,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 30\n",
      "[2023-07-09 17:31:58,994 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 31\n",
      "[2023-07-09 17:31:59,059 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 32\n",
      "[2023-07-09 17:31:59,127 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 33\n",
      "[2023-07-09 17:31:59,193 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 34\n",
      "[2023-07-09 17:31:59,257 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 35\n",
      "[2023-07-09 17:31:59,322 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 36\n",
      "[2023-07-09 17:31:59,876 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 37\n",
      "[2023-07-09 17:31:59,941 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 38\n",
      "[2023-07-09 17:32:00,005 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 39\n",
      "[2023-07-09 17:32:00,070 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 40\n",
      "[2023-07-09 17:32:00,136 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 41\n",
      "[2023-07-09 17:32:00,203 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 42\n",
      "[2023-07-09 17:32:00,266 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 43\n",
      "[2023-07-09 17:32:00,329 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 44\n",
      "[2023-07-09 17:32:00,393 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 45\n",
      "[2023-07-09 17:32:00,459 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 46\n",
      "[2023-07-09 17:32:00,529 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 47\n",
      "[2023-07-09 17:32:01,212 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 48\n",
      "[2023-07-09 17:32:01,278 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 49\n",
      "[2023-07-09 17:32:01,342 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 50\n",
      "[2023-07-09 17:32:01,407 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 51\n",
      "[2023-07-09 17:32:01,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 52\n",
      "[2023-07-09 17:32:01,539 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 53\n",
      "[2023-07-09 17:32:01,604 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 54\n",
      "[2023-07-09 17:32:01,667 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 55\n",
      "[2023-07-09 17:32:01,741 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 56\n",
      "[2023-07-09 17:32:01,811 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 57\n",
      "[2023-07-09 17:32:01,877 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 58\n",
      "[2023-07-09 17:32:01,946 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 59\n",
      "[2023-07-09 17:32:02,013 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 60\n",
      "[2023-07-09 17:32:02,867 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 61\n",
      "[2023-07-09 17:32:02,936 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 62\n",
      "[2023-07-09 17:32:03,003 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 63\n",
      "[2023-07-09 17:32:03,069 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 64\n",
      "[2023-07-09 17:32:03,154 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 65\n",
      "[2023-07-09 17:32:03,265 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 66\n",
      "[2023-07-09 17:33:32,106 INFO] Step 50/ 1200; acc: 23.9; ppl: 1069.7; xent: 7.0; lr: 0.00020; sents:   49691; bsz: 2987/3809/248; 6200/7906 tok/s;     96 sec;\n",
      "[2023-07-09 17:34:50,942 INFO] Step 100/ 1200; acc: 65.7; ppl:  27.9; xent: 3.3; lr: 0.00040; sents:   51329; bsz: 3008/3808/257; 7632/9661 tok/s;    175 sec;\n",
      "[2023-07-09 17:34:50,947 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_100.pt\n",
      "[2023-07-09 17:36:12,156 INFO] Step 150/ 1200; acc: 75.8; ppl:  13.5; xent: 2.6; lr: 0.00060; sents:   50756; bsz: 2998/3786/254; 7382/9324 tok/s;    256 sec;\n",
      "[2023-07-09 17:37:13,755 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 67\n",
      "[2023-07-09 17:37:13,821 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 68\n",
      "[2023-07-09 17:37:13,887 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 69\n",
      "[2023-07-09 17:37:13,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 70\n",
      "[2023-07-09 17:37:14,019 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 71\n",
      "[2023-07-09 17:37:14,082 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 72\n",
      "[2023-07-09 17:37:14,146 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 73\n",
      "[2023-07-09 17:37:14,218 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 74\n",
      "[2023-07-09 17:37:14,283 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 75\n",
      "[2023-07-09 17:37:14,347 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 76\n",
      "[2023-07-09 17:37:14,410 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 77\n",
      "[2023-07-09 17:37:14,474 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 78\n",
      "[2023-07-09 17:37:14,540 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 79\n",
      "[2023-07-09 17:37:14,606 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 80\n",
      "[2023-07-09 17:37:14,670 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 81\n",
      "[2023-07-09 17:37:14,732 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 82\n",
      "[2023-07-09 17:37:14,793 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 83\n",
      "[2023-07-09 17:37:14,863 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 84\n",
      "[2023-07-09 17:37:14,930 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 85\n",
      "[2023-07-09 17:37:14,994 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 86\n",
      "[2023-07-09 17:37:15,057 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 87\n",
      "[2023-07-09 17:37:15,123 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 88\n",
      "[2023-07-09 17:37:15,188 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 89\n",
      "[2023-07-09 17:37:17,448 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 90\n",
      "[2023-07-09 17:37:17,512 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 91\n",
      "[2023-07-09 17:37:17,575 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 92\n",
      "[2023-07-09 17:37:17,639 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 93\n",
      "[2023-07-09 17:37:17,705 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 94\n",
      "[2023-07-09 17:37:17,769 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 95\n",
      "[2023-07-09 17:37:17,832 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 96\n",
      "[2023-07-09 17:37:17,896 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 97\n",
      "[2023-07-09 17:37:17,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 98\n",
      "[2023-07-09 17:37:18,029 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 99\n",
      "[2023-07-09 17:37:18,095 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 100\n",
      "[2023-07-09 17:37:18,161 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 101\n",
      "[2023-07-09 17:37:18,227 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 102\n",
      "[2023-07-09 17:37:18,303 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 103\n",
      "[2023-07-09 17:37:18,368 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 104\n",
      "[2023-07-09 17:37:18,434 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 105\n",
      "[2023-07-09 17:37:18,498 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 106\n",
      "[2023-07-09 17:37:18,568 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 107\n",
      "[2023-07-09 17:37:18,639 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 108\n",
      "[2023-07-09 17:37:18,705 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 109\n",
      "[2023-07-09 17:37:18,768 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 110\n",
      "[2023-07-09 17:37:18,836 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 111\n",
      "[2023-07-09 17:37:18,903 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 112\n",
      "[2023-07-09 17:37:18,971 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 113\n",
      "[2023-07-09 17:37:19,038 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 114\n",
      "[2023-07-09 17:37:19,103 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 115\n",
      "[2023-07-09 17:37:21,935 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 116\n",
      "[2023-07-09 17:37:22,044 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 117\n",
      "[2023-07-09 17:37:22,154 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 118\n",
      "[2023-07-09 17:37:22,268 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 119\n",
      "[2023-07-09 17:37:22,383 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 120\n",
      "[2023-07-09 17:37:22,502 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 121\n",
      "[2023-07-09 17:37:22,623 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 122\n",
      "[2023-07-09 17:37:22,740 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 123\n",
      "[2023-07-09 17:37:22,855 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 124\n",
      "[2023-07-09 17:37:22,982 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 125\n",
      "[2023-07-09 17:37:23,099 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 126\n",
      "[2023-07-09 17:37:23,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 127\n",
      "[2023-07-09 17:37:23,345 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 128\n",
      "[2023-07-09 17:37:23,466 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 129\n",
      "[2023-07-09 17:37:23,595 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 130\n",
      "[2023-07-09 17:37:23,717 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 131\n",
      "[2023-07-09 17:37:23,830 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 132\n",
      "[2023-07-09 17:37:49,807 INFO] Step 200/ 1200; acc: 80.5; ppl:   9.5; xent: 2.3; lr: 0.00079; sents:   52013; bsz: 3013/3799/260; 6170/7781 tok/s;    354 sec;\n",
      "[2023-07-09 17:37:49,809 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_200.pt\n",
      "[2023-07-09 17:39:11,415 INFO] Step 250/ 1200; acc: 91.7; ppl:   5.6; xent: 1.7; lr: 0.00099; sents:   49398; bsz: 2988/3797/247; 7322/9305 tok/s;    436 sec;\n",
      "[2023-07-09 17:40:30,370 INFO] Step 300/ 1200; acc: 99.4; ppl:   3.8; xent: 1.3; lr: 0.00119; sents:   50742; bsz: 3009/3808/254; 7622/9645 tok/s;    515 sec;\n",
      "[2023-07-09 17:40:30,373 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_300.pt\n",
      "[2023-07-09 17:41:51,632 INFO] Step 350/ 1200; acc: 96.1; ppl:   4.5; xent: 1.5; lr: 0.00139; sents:   50411; bsz: 2987/3811/252; 7351/9378 tok/s;    596 sec;\n",
      "[2023-07-09 17:42:34,214 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 133\n",
      "[2023-07-09 17:42:34,276 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 134\n",
      "[2023-07-09 17:42:34,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 135\n",
      "[2023-07-09 17:42:34,407 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 136\n",
      "[2023-07-09 17:42:34,494 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 137\n",
      "[2023-07-09 17:42:34,573 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 138\n",
      "[2023-07-09 17:42:34,650 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 139\n",
      "[2023-07-09 17:42:34,718 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 140\n",
      "[2023-07-09 17:42:36,920 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 141\n",
      "[2023-07-09 17:42:36,986 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 142\n",
      "[2023-07-09 17:42:37,049 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 143\n",
      "[2023-07-09 17:42:37,109 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 144\n",
      "[2023-07-09 17:42:37,186 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 145\n",
      "[2023-07-09 17:42:37,249 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 146\n",
      "[2023-07-09 17:42:37,312 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 147\n",
      "[2023-07-09 17:42:37,381 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 148\n",
      "[2023-07-09 17:42:37,446 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 149\n",
      "[2023-07-09 17:42:37,510 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 150\n",
      "[2023-07-09 17:42:37,580 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 151\n",
      "[2023-07-09 17:42:37,651 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 152\n",
      "[2023-07-09 17:42:37,730 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 153\n",
      "[2023-07-09 17:42:37,831 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 154\n",
      "[2023-07-09 17:42:37,947 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 155\n",
      "[2023-07-09 17:42:38,062 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 156\n",
      "[2023-07-09 17:42:38,169 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 157\n",
      "[2023-07-09 17:42:38,273 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 158\n",
      "[2023-07-09 17:42:38,380 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 159\n",
      "[2023-07-09 17:42:38,490 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 160\n",
      "[2023-07-09 17:42:38,603 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 161\n",
      "[2023-07-09 17:42:38,715 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 162\n",
      "[2023-07-09 17:42:38,823 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 163\n",
      "[2023-07-09 17:42:41,839 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 164\n",
      "[2023-07-09 17:42:41,938 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 165\n",
      "[2023-07-09 17:42:42,041 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 166\n",
      "[2023-07-09 17:42:42,144 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 167\n",
      "[2023-07-09 17:42:42,247 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 168\n",
      "[2023-07-09 17:42:42,350 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 169\n",
      "[2023-07-09 17:42:42,420 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 170\n",
      "[2023-07-09 17:42:42,481 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 171\n",
      "[2023-07-09 17:42:42,540 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 172\n",
      "[2023-07-09 17:42:42,600 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 173\n",
      "[2023-07-09 17:42:42,666 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 174\n",
      "[2023-07-09 17:42:42,726 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 175\n",
      "[2023-07-09 17:42:42,795 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 176\n",
      "[2023-07-09 17:42:42,855 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 177\n",
      "[2023-07-09 17:42:42,921 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 178\n",
      "[2023-07-09 17:42:42,983 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 179\n",
      "[2023-07-09 17:42:43,043 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 180\n",
      "[2023-07-09 17:42:43,103 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 181\n",
      "[2023-07-09 17:42:43,163 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 182\n",
      "[2023-07-09 17:42:43,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 183\n",
      "[2023-07-09 17:42:43,292 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 184\n",
      "[2023-07-09 17:42:43,353 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 185\n",
      "[2023-07-09 17:42:43,411 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 186\n",
      "[2023-07-09 17:42:43,472 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 187\n",
      "[2023-07-09 17:42:43,536 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 188\n",
      "[2023-07-09 17:42:43,599 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 189\n",
      "[2023-07-09 17:42:43,659 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 190\n",
      "[2023-07-09 17:42:43,718 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 191\n",
      "[2023-07-09 17:42:43,788 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 192\n",
      "[2023-07-09 17:42:46,795 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 193\n",
      "[2023-07-09 17:42:46,864 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 194\n",
      "[2023-07-09 17:42:46,924 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 195\n",
      "[2023-07-09 17:42:46,985 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 196\n",
      "[2023-07-09 17:42:47,045 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 197\n",
      "[2023-07-09 17:43:33,338 INFO] Step 400/ 1200; acc: 98.9; ppl:   3.9; xent: 1.4; lr: 0.00159; sents:   50562; bsz: 2985/3780/253; 5870/7433 tok/s;    698 sec;\n",
      "[2023-07-09 17:43:33,782 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.44309186935424805 s.\n",
      "[2023-07-09 17:43:33,783 INFO] Train perplexity: 13.9714\n",
      "[2023-07-09 17:43:33,784 INFO] Train accuracy: 78.9841\n",
      "[2023-07-09 17:43:33,784 INFO] Sentences processed: 404902\n",
      "[2023-07-09 17:43:33,784 INFO] Average bsz: 2997/3800/253\n",
      "[2023-07-09 17:43:33,784 INFO] Validation perplexity: 13.6869\n",
      "[2023-07-09 17:43:33,784 INFO] Validation accuracy: 77.9102\n",
      "[2023-07-09 17:43:33,784 INFO] Model is improving ppl: inf --> 13.6869.\n",
      "[2023-07-09 17:43:33,784 INFO] Model is improving acc: -inf --> 77.9102.\n",
      "[2023-07-09 17:43:33,787 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_400.pt\n",
      "[2023-07-09 17:44:54,811 INFO] Step 450/ 1200; acc: 99.7; ppl:   3.7; xent: 1.3; lr: 0.00178; sents:   49962; bsz: 2993/3808/250; 7347/9349 tok/s;    779 sec;\n",
      "[2023-07-09 17:46:13,402 INFO] Step 500/ 1200; acc: 99.5; ppl:   3.7; xent: 1.3; lr: 0.00197; sents:   51049; bsz: 3000/3804/255; 7635/9680 tok/s;    858 sec;\n",
      "[2023-07-09 17:46:13,407 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_500.pt\n",
      "[2023-07-09 17:47:34,734 INFO] Step 550/ 1200; acc: 98.7; ppl:   3.9; xent: 1.4; lr: 0.00188; sents:   51418; bsz: 3006/3804/257; 7391/9355 tok/s;    939 sec;\n",
      "[2023-07-09 17:47:59,878 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 198\n",
      "[2023-07-09 17:47:59,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 199\n",
      "[2023-07-09 17:48:00,025 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 200\n",
      "[2023-07-09 17:48:00,091 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 201\n",
      "[2023-07-09 17:48:00,155 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 202\n",
      "[2023-07-09 17:48:00,219 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 203\n",
      "[2023-07-09 17:48:00,282 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 204\n",
      "[2023-07-09 17:48:00,345 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 205\n",
      "[2023-07-09 17:48:00,409 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 206\n",
      "[2023-07-09 17:48:00,477 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 207\n",
      "[2023-07-09 17:48:00,540 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 208\n",
      "[2023-07-09 17:48:00,599 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 209\n",
      "[2023-07-09 17:48:00,662 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 210\n",
      "[2023-07-09 17:48:00,731 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 211\n",
      "[2023-07-09 17:48:00,798 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 212\n",
      "[2023-07-09 17:48:00,861 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 213\n",
      "[2023-07-09 17:48:00,924 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 214\n",
      "[2023-07-09 17:48:00,994 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 215\n",
      "[2023-07-09 17:48:01,063 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 216\n",
      "[2023-07-09 17:48:01,126 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 217\n",
      "[2023-07-09 17:48:01,189 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 218\n",
      "[2023-07-09 17:48:01,252 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 219\n",
      "[2023-07-09 17:48:01,315 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 220\n",
      "[2023-07-09 17:48:01,380 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 221\n",
      "[2023-07-09 17:48:01,447 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 222\n",
      "[2023-07-09 17:48:01,509 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 223\n",
      "[2023-07-09 17:48:04,050 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 224\n",
      "[2023-07-09 17:48:04,116 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 225\n",
      "[2023-07-09 17:48:04,178 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 226\n",
      "[2023-07-09 17:48:04,239 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 227\n",
      "[2023-07-09 17:48:04,303 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 228\n",
      "[2023-07-09 17:48:04,368 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 229\n",
      "[2023-07-09 17:48:04,433 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 230\n",
      "[2023-07-09 17:48:04,510 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 231\n",
      "[2023-07-09 17:48:04,577 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 232\n",
      "[2023-07-09 17:48:04,639 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 233\n",
      "[2023-07-09 17:48:04,706 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 234\n",
      "[2023-07-09 17:48:04,766 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 235\n",
      "[2023-07-09 17:48:04,827 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 236\n",
      "[2023-07-09 17:48:04,892 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 237\n",
      "[2023-07-09 17:48:04,961 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 238\n",
      "[2023-07-09 17:48:05,026 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 239\n",
      "[2023-07-09 17:48:05,097 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 240\n",
      "[2023-07-09 17:48:05,160 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 241\n",
      "[2023-07-09 17:48:05,224 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 242\n",
      "[2023-07-09 17:48:05,289 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 243\n",
      "[2023-07-09 17:48:05,349 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 244\n",
      "[2023-07-09 17:48:05,409 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 245\n",
      "[2023-07-09 17:48:05,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 246\n",
      "[2023-07-09 17:48:05,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 247\n",
      "[2023-07-09 17:48:05,598 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 248\n",
      "[2023-07-09 17:48:05,659 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 249\n",
      "[2023-07-09 17:48:05,720 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 250\n",
      "[2023-07-09 17:48:08,685 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 251\n",
      "[2023-07-09 17:48:08,746 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 252\n",
      "[2023-07-09 17:48:08,807 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 253\n",
      "[2023-07-09 17:48:08,867 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 254\n",
      "[2023-07-09 17:48:08,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 255\n",
      "[2023-07-09 17:48:08,991 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 256\n",
      "[2023-07-09 17:48:09,050 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 257\n",
      "[2023-07-09 17:48:09,112 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 258\n",
      "[2023-07-09 17:48:09,190 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 259\n",
      "[2023-07-09 17:48:09,253 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 260\n",
      "[2023-07-09 17:48:09,328 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 261\n",
      "[2023-07-09 17:48:09,426 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 262\n",
      "[2023-07-09 17:48:09,526 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 263\n",
      "[2023-07-09 17:49:13,701 INFO] Step 600/ 1200; acc: 99.4; ppl:   3.7; xent: 1.3; lr: 0.00180; sents:   51661; bsz: 3009/3798/258; 6081/7676 tok/s;   1038 sec;\n",
      "[2023-07-09 17:49:13,707 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_600.pt\n",
      "[2023-07-09 17:50:35,296 INFO] Step 650/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00173; sents:   50003; bsz: 2979/3792/250; 7303/9294 tok/s;   1120 sec;\n",
      "[2023-07-09 17:51:53,650 INFO] Step 700/ 1200; acc: 99.6; ppl:   3.7; xent: 1.3; lr: 0.00167; sents:   50102; bsz: 3003/3810/251; 7666/9724 tok/s;   1198 sec;\n",
      "[2023-07-09 17:51:53,654 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_700.pt\n",
      "[2023-07-09 17:53:16,165 INFO] Step 750/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00161; sents:   51090; bsz: 3004/3809/255; 7282/9232 tok/s;   1280 sec;\n",
      "[2023-07-09 17:53:24,083 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 264\n",
      "[2023-07-09 17:53:24,150 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 265\n",
      "[2023-07-09 17:53:24,215 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 266\n",
      "[2023-07-09 17:53:24,281 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 267\n",
      "[2023-07-09 17:53:24,350 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 268\n",
      "[2023-07-09 17:53:24,425 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 269\n",
      "[2023-07-09 17:53:24,491 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 270\n",
      "[2023-07-09 17:53:24,554 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 271\n",
      "[2023-07-09 17:53:24,621 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 272\n",
      "[2023-07-09 17:53:24,690 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 273\n",
      "[2023-07-09 17:53:24,754 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 274\n",
      "[2023-07-09 17:53:24,823 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 275\n",
      "[2023-07-09 17:53:24,891 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 276\n",
      "[2023-07-09 17:53:27,476 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 277\n",
      "[2023-07-09 17:53:27,588 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 278\n",
      "[2023-07-09 17:53:27,700 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 279\n",
      "[2023-07-09 17:53:27,817 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 280\n",
      "[2023-07-09 17:53:27,931 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 281\n",
      "[2023-07-09 17:53:28,049 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 282\n",
      "[2023-07-09 17:53:28,167 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 283\n",
      "[2023-07-09 17:53:28,279 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 284\n",
      "[2023-07-09 17:53:28,386 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 285\n",
      "[2023-07-09 17:53:28,499 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 286\n",
      "[2023-07-09 17:53:28,619 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 287\n",
      "[2023-07-09 17:53:28,729 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 288\n",
      "[2023-07-09 17:53:28,842 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 289\n",
      "[2023-07-09 17:53:28,949 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 290\n",
      "[2023-07-09 17:53:29,058 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 291\n",
      "[2023-07-09 17:53:29,163 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 292\n",
      "[2023-07-09 17:53:29,272 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 293\n",
      "[2023-07-09 17:53:29,379 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 294\n",
      "[2023-07-09 17:53:29,491 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 295\n",
      "[2023-07-09 17:53:29,602 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 296\n",
      "[2023-07-09 17:53:29,710 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 297\n",
      "[2023-07-09 17:53:29,818 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 298\n",
      "[2023-07-09 17:53:29,922 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 299\n",
      "[2023-07-09 17:53:30,028 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 300\n",
      "[2023-07-09 17:53:32,825 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 301\n",
      "[2023-07-09 17:53:32,887 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 302\n",
      "[2023-07-09 17:53:32,949 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 303\n",
      "[2023-07-09 17:53:33,011 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 304\n",
      "[2023-07-09 17:53:33,071 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 305\n",
      "[2023-07-09 17:53:33,137 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 306\n",
      "[2023-07-09 17:53:33,199 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 307\n",
      "[2023-07-09 17:53:33,274 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 308\n",
      "[2023-07-09 17:53:33,345 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 309\n",
      "[2023-07-09 17:53:33,408 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 310\n",
      "[2023-07-09 17:53:33,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 311\n",
      "[2023-07-09 17:53:33,530 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 312\n",
      "[2023-07-09 17:53:33,597 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 313\n",
      "[2023-07-09 17:53:33,668 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 314\n",
      "[2023-07-09 17:53:33,730 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 315\n",
      "[2023-07-09 17:53:33,791 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 316\n",
      "[2023-07-09 17:53:33,853 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 317\n",
      "[2023-07-09 17:53:33,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 318\n",
      "[2023-07-09 17:53:33,989 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 319\n",
      "[2023-07-09 17:53:34,051 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 320\n",
      "[2023-07-09 17:53:34,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 321\n",
      "[2023-07-09 17:53:34,179 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 322\n",
      "[2023-07-09 17:53:34,245 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 323\n",
      "[2023-07-09 17:53:34,310 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 324\n",
      "[2023-07-09 17:53:34,371 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 325\n",
      "[2023-07-09 17:53:34,430 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 326\n",
      "[2023-07-09 17:53:34,497 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 327\n",
      "[2023-07-09 17:53:34,560 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 328\n",
      "[2023-07-09 17:54:58,230 INFO] Step 800/ 1200; acc: 99.7; ppl:   3.6; xent: 1.3; lr: 0.00156; sents:   51229; bsz: 3002/3803/256; 5883/7452 tok/s;   1382 sec;\n",
      "[2023-07-09 17:54:58,714 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.4820249080657959 s.\n",
      "[2023-07-09 17:54:58,716 INFO] Train perplexity: 7.16102\n",
      "[2023-07-09 17:54:58,716 INFO] Train accuracy: 89.2684\n",
      "[2023-07-09 17:54:58,716 INFO] Sentences processed: 811416\n",
      "[2023-07-09 17:54:58,716 INFO] Average bsz: 2998/3802/254\n",
      "[2023-07-09 17:54:58,716 INFO] Validation perplexity: 13.8851\n",
      "[2023-07-09 17:54:58,716 INFO] Validation accuracy: 78.9697\n",
      "[2023-07-09 17:54:58,716 INFO] Stalled patience: 3/4\n",
      "[2023-07-09 17:54:58,720 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_800.pt\n",
      "[2023-07-09 17:56:19,412 INFO] Step 850/ 1200; acc: 99.7; ppl:   3.6; xent: 1.3; lr: 0.00151; sents:   50505; bsz: 2996/3809/253; 7381/9385 tok/s;   1464 sec;\n",
      "[2023-07-09 17:57:37,709 INFO] Step 900/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00147; sents:   50615; bsz: 3007/3810/253; 7681/9732 tok/s;   1542 sec;\n",
      "[2023-07-09 17:57:37,712 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_900.pt\n",
      "[2023-07-09 17:58:48,904 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 329\n",
      "[2023-07-09 17:58:48,975 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 330\n",
      "[2023-07-09 17:58:49,038 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 331\n",
      "[2023-07-09 17:58:49,102 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 332\n",
      "[2023-07-09 17:58:49,164 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 333\n",
      "[2023-07-09 17:58:49,232 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 334\n",
      "[2023-07-09 17:58:49,306 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 335\n",
      "[2023-07-09 17:58:49,386 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 336\n",
      "[2023-07-09 17:58:49,457 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 337\n",
      "[2023-07-09 17:58:49,527 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 338\n",
      "[2023-07-09 17:58:49,591 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 339\n",
      "[2023-07-09 17:58:49,653 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 340\n",
      "[2023-07-09 17:58:49,717 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 341\n",
      "[2023-07-09 17:58:49,789 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 342\n",
      "[2023-07-09 17:58:49,852 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 343\n",
      "[2023-07-09 17:58:49,916 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 344\n",
      "[2023-07-09 17:58:49,980 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 345\n",
      "[2023-07-09 17:58:50,049 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 346\n",
      "[2023-07-09 17:58:50,117 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 347\n",
      "[2023-07-09 17:58:50,182 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 348\n",
      "[2023-07-09 17:58:50,242 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 349\n",
      "[2023-07-09 17:58:50,311 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 350\n",
      "[2023-07-09 17:58:50,373 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 351\n",
      "[2023-07-09 17:58:50,439 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 352\n",
      "[2023-07-09 17:58:50,505 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 353\n",
      "[2023-07-09 17:58:50,568 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 354\n",
      "[2023-07-09 17:58:50,637 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 355\n",
      "[2023-07-09 17:58:50,704 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 356\n",
      "[2023-07-09 17:58:50,774 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 357\n",
      "[2023-07-09 17:58:50,837 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 358\n",
      "[2023-07-09 17:58:53,563 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 359\n",
      "[2023-07-09 17:58:53,631 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 360\n",
      "[2023-07-09 17:58:53,699 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 361\n",
      "[2023-07-09 17:58:53,762 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 362\n",
      "[2023-07-09 17:58:53,833 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 363\n",
      "[2023-07-09 17:58:53,898 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 364\n",
      "[2023-07-09 17:58:53,966 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 365\n",
      "[2023-07-09 17:58:54,033 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 366\n",
      "[2023-07-09 17:58:54,108 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 367\n",
      "[2023-07-09 17:58:54,179 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 368\n",
      "[2023-07-09 17:58:54,246 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 369\n",
      "[2023-07-09 17:58:54,313 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 370\n",
      "[2023-07-09 17:58:54,381 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 371\n",
      "[2023-07-09 17:58:54,447 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 372\n",
      "[2023-07-09 17:58:54,515 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 373\n",
      "[2023-07-09 17:58:54,584 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 374\n",
      "[2023-07-09 17:58:54,656 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 375\n",
      "[2023-07-09 17:58:54,722 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 376\n",
      "[2023-07-09 17:58:54,783 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 377\n",
      "[2023-07-09 17:58:54,857 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 378\n",
      "[2023-07-09 17:58:54,924 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 379\n",
      "[2023-07-09 17:58:54,990 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 380\n",
      "[2023-07-09 17:58:55,054 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 381\n",
      "[2023-07-09 17:58:55,116 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 382\n",
      "[2023-07-09 17:58:55,177 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 383\n",
      "[2023-07-09 17:58:55,250 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 384\n",
      "[2023-07-09 17:58:55,314 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 385\n",
      "[2023-07-09 17:58:55,375 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 386\n",
      "[2023-07-09 17:58:58,712 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 387\n",
      "[2023-07-09 17:58:58,821 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 388\n",
      "[2023-07-09 17:58:58,940 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 389\n",
      "[2023-07-09 17:58:59,047 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 390\n",
      "[2023-07-09 17:58:59,158 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 391\n",
      "[2023-07-09 17:58:59,288 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 392\n",
      "[2023-07-09 17:58:59,396 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 393\n",
      "[2023-07-09 17:58:59,501 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 394\n",
      "[2023-07-09 17:59:18,914 INFO] Step 950/ 1200; acc: 99.6; ppl:   3.6; xent: 1.3; lr: 0.00143; sents:   50138; bsz: 2987/3786/251; 5903/7483 tok/s;   1643 sec;\n",
      "[2023-07-09 18:00:37,535 INFO] Step 1000/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00140; sents:   50683; bsz: 3006/3807/253; 7646/9686 tok/s;   1722 sec;\n",
      "[2023-07-09 18:00:37,538 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1000.pt\n",
      "[2023-07-09 18:01:58,443 INFO] Step 1050/ 1200; acc: 99.8; ppl:   3.6; xent: 1.3; lr: 0.00136; sents:   49981; bsz: 2992/3809/250; 7395/9415 tok/s;   1803 sec;\n",
      "[2023-07-09 18:03:16,749 INFO] Step 1100/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00133; sents:   51013; bsz: 3001/3806/255; 7666/9722 tok/s;   1881 sec;\n",
      "[2023-07-09 18:03:16,754 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1100.pt\n",
      "[2023-07-09 18:04:10,800 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 395\n",
      "[2023-07-09 18:04:10,872 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 396\n",
      "[2023-07-09 18:04:10,950 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 397\n",
      "[2023-07-09 18:04:11,015 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 398\n",
      "[2023-07-09 18:04:11,082 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 399\n",
      "[2023-07-09 18:04:11,145 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 400\n",
      "[2023-07-09 18:04:11,210 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 401\n",
      "[2023-07-09 18:04:11,279 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 402\n",
      "[2023-07-09 18:04:11,343 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 403\n",
      "[2023-07-09 18:04:11,406 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 404\n",
      "[2023-07-09 18:04:11,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 405\n",
      "[2023-07-09 18:04:11,541 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 406\n",
      "[2023-07-09 18:04:11,609 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 407\n",
      "[2023-07-09 18:04:11,672 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 408\n",
      "[2023-07-09 18:04:11,738 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 409\n",
      "[2023-07-09 18:04:11,816 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 410\n",
      "[2023-07-09 18:04:11,881 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 411\n",
      "[2023-07-09 18:04:11,945 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 412\n",
      "[2023-07-09 18:04:12,011 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 413\n",
      "[2023-07-09 18:04:12,080 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 414\n",
      "[2023-07-09 18:04:12,145 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 415\n",
      "[2023-07-09 18:04:12,213 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 416\n",
      "[2023-07-09 18:04:14,855 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 417\n",
      "[2023-07-09 18:04:14,927 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 418\n",
      "[2023-07-09 18:04:15,032 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 419\n",
      "[2023-07-09 18:04:15,145 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 420\n",
      "[2023-07-09 18:04:15,259 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 421\n",
      "[2023-07-09 18:04:15,381 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 422\n",
      "[2023-07-09 18:04:15,495 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 423\n",
      "[2023-07-09 18:04:15,606 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 424\n",
      "[2023-07-09 18:04:15,715 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 425\n",
      "[2023-07-09 18:04:15,833 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 426\n",
      "[2023-07-09 18:04:15,953 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 427\n",
      "[2023-07-09 18:04:16,078 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 428\n",
      "[2023-07-09 18:04:16,193 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 429\n",
      "[2023-07-09 18:04:16,308 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 430\n",
      "[2023-07-09 18:04:16,424 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 431\n",
      "[2023-07-09 18:04:16,535 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 432\n",
      "[2023-07-09 18:04:16,646 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 433\n",
      "[2023-07-09 18:04:16,767 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 434\n",
      "[2023-07-09 18:04:16,879 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 435\n",
      "[2023-07-09 18:04:16,997 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 436\n",
      "[2023-07-09 18:04:17,108 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 437\n",
      "[2023-07-09 18:04:17,219 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 438\n",
      "[2023-07-09 18:04:17,327 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 439\n",
      "[2023-07-09 18:04:17,440 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 440\n",
      "[2023-07-09 18:04:17,551 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 441\n",
      "[2023-07-09 18:04:17,675 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 442\n",
      "[2023-07-09 18:04:17,788 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 443\n",
      "[2023-07-09 18:04:21,217 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 444\n",
      "[2023-07-09 18:04:21,276 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 445\n",
      "[2023-07-09 18:04:21,339 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 446\n",
      "[2023-07-09 18:04:21,403 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 447\n",
      "[2023-07-09 18:04:21,468 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 448\n",
      "[2023-07-09 18:04:21,536 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 449\n",
      "[2023-07-09 18:04:21,598 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 450\n",
      "[2023-07-09 18:04:21,660 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 451\n",
      "[2023-07-09 18:04:21,721 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 452\n",
      "[2023-07-09 18:04:21,786 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 453\n",
      "[2023-07-09 18:04:21,855 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 454\n",
      "[2023-07-09 18:04:21,919 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 455\n",
      "[2023-07-09 18:04:21,982 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 456\n",
      "[2023-07-09 18:04:22,049 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 457\n",
      "[2023-07-09 18:04:22,123 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 458\n",
      "[2023-07-09 18:04:22,183 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* train: 459\n",
      "[2023-07-09 18:04:58,354 INFO] Step 1150/ 1200; acc: 99.6; ppl:   3.6; xent: 1.3; lr: 0.00130; sents:   51923; bsz: 3000/3777/260; 5906/7435 tok/s;   1983 sec;\n",
      "[2023-07-09 18:06:16,999 INFO] Step 1200/ 1200; acc: 99.9; ppl:   3.6; xent: 1.3; lr: 0.00128; sents:   49673; bsz: 2997/3813/248; 7621/9697 tok/s;   2061 sec;\n",
      "[2023-07-09 18:06:17,466 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 0.46442294120788574 s.\n",
      "[2023-07-09 18:06:17,468 INFO] Train perplexity: 5.69321\n",
      "[2023-07-09 18:06:17,468 INFO] Train accuracy: 92.7726\n",
      "[2023-07-09 18:06:17,468 INFO] Sentences processed: 1.21595e+06\n",
      "[2023-07-09 18:06:17,468 INFO] Average bsz: 2998/3802/253\n",
      "[2023-07-09 18:06:17,468 INFO] Validation perplexity: 13.6271\n",
      "[2023-07-09 18:06:17,468 INFO] Validation accuracy: 79.5921\n",
      "[2023-07-09 18:06:17,468 INFO] Model is improving ppl: 13.6869 --> 13.6271.\n",
      "[2023-07-09 18:06:17,468 INFO] Model is improving acc: 77.9102 --> 79.5921.\n",
      "[2023-07-09 18:06:17,474 INFO] Saving checkpoint /content/nmt/nmtmodel/model_step_1200.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the NMT model using the configuration defined in 'config.yaml'\n",
    "!onmt_train -config config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1688926049656,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "ro-21ODLnBNU",
    "outputId": "8f2079ac-d999-4de7-c63e-f33deb81fb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step_1000.pt  model_step_200.pt  model_step_600.pt  src.vocab\n",
      "model_step_100.pt   model_step_300.pt  model_step_700.pt  train.log\n",
      "model_step_1100.pt  model_step_400.pt  model_step_800.pt\n",
      "model_step_1200.pt  model_step_500.pt  model_step_900.pt\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the 'model_root' directory\n",
    "!ls '{model_root}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRou9WfsDbh"
   },
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66472,
     "status": "ok",
     "timestamp": 1688926049656,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "tStHVtbNmipI",
    "outputId": "31f9c190-ef47-41d1-8033-971778130c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-09 18:06:24,314 INFO] Loading checkpoint from /content/nmt/nmtmodel/model_step_1200.pt\n",
      "[2023-07-09 18:06:25,023 INFO] Loading data into the model\n",
      "[2023-07-09 18:07:29,066 INFO] PRED SCORE: -0.3570, PRED PPL: 1.43 NB SENTENCES: 500\n"
     ]
    }
   ],
   "source": [
    "# Perform translation using the trained NMT model\n",
    "# --model specifies the path to the trained model checkpoint\n",
    "# --src specifies the path to the source text file to be translated\n",
    "# --output specifies the path to save the translated output\n",
    "# -beam_size specifies the beam size for beam search\n",
    "! onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src /content/nmt/lcquad/test.en --output /content/nmt/lcquad/trans_test.sparql -beam_size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1688926050345,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "UYszCfIbssof",
    "outputId": "c8f14df9-8694-4272-b7ff-091ddeea48d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list the products of the company which published tweenies: game time .\n",
      "name the common sports played at polytechnic university of philippines san juan and islamic azad university ?\n",
      "which company developed both dart and go ?\n",
      "count the total number of launch site of the rockets which have been launched form cape canaveral air force station ?\n",
      "to which settlement does elliot bay belong to ?\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.en'\n",
    "!head -n 5 /content/nmt/lcquad/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1688926050346,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "nnTRp6eas_p7",
    "outputId": "6af08395-c5f7-49bc-e89c-ae1fc33864b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_product> <dbr_Postbanken> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Advocate_Nasiruddin> <dbo_knownFor> var_uri sep_dot <dbr_Polytechnic_University_of_the_Philippines_San_Juan> <dbo_sport> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Dart_ attr_open programming_language attr_close math_gt <dbp_developer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_manufacturer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Clark_Daniel_Stearns> <dbo_militaryBranch> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'trans_test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/trans_test.sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1688926050346,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "azi6x-EstHYN",
    "outputId": "d1e6e291-c150-46be-faa6-e742770c2660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Polytechnic_University_of_the_Philippines_San_Juan> <dbo_sport> var_uri sep_dot <dbr_Islamic_Azad_University_Central_Tehran_Branch> <dbo_sport> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Dart_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot <dbr_Go_ attr_open programming_language attr_close math_gt <dbo_developer> var_uri sep_dot brack_close\n",
      "SELECT DISTINCT COUNT attr_open var_uri attr_close WHERE brack_open var_x <dbo_launchSite> <dbr_Cape_Canaveral_Air_Force_Station> sep_dot var_x <dbo_launchSite> var_uri brack_close\n",
      "SELECT DISTINCT var_uri WHERE brack_open <dbr_Elliott_Bay> <dbp_cities> var_uri brack_close\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 lines of the file 'test.sparql'\n",
    "!head -n 5 /content/nmt/lcquad/test.sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vANuMTJUtfUb"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688926050346,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "kj1AtrSDtjfq",
    "outputId": "df550af9-2372-4208-94c5-c4aaf2041893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1688926050833,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "CBxRt66trVw5",
    "outputId": "4cc90602-0e8f-4b03-dbd4-b812fe6408d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_product> <dbr_Postbanken> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Accuracy:  0.63043784316486\n"
     ]
    }
   ],
   "source": [
    "# Copy the file \"compute-accuracy.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-accuracy.py ./\n",
    "\n",
    "# Evaluate the translation using the provided accuracy computation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-accuracy.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5487,
     "status": "ok",
     "timestamp": 1688926056318,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "p2M4CDSltiuL",
    "outputId": "4f90dead-8a8b-4967-c00b-7ce5e05028a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_product> <dbr_Postbanken> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "BLEU:  67.0482631851078\n"
     ]
    }
   ],
   "source": [
    "# Install the sacrebleu library using pip\n",
    "!pip install sacrebleu > /dev/null\n",
    "\n",
    "# Copy the file \"compute-bleu.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-bleu.py ./\n",
    "\n",
    "# Evaluate the translation using BLEU score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-bleu.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6618,
     "status": "ok",
     "timestamp": 1688926062932,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "YDcNrZm3MCO1",
    "outputId": "c58366de-79f4-4afd-d1b8-34a4cc642684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: SELECT DISTINCT var_uri WHERE brack_open <dbr_Tweenies:_Game_Time> <dbp_publisher> var_x sep_dot var_x <dbp_products> var_uri sep_dot brack_close\n",
      "MTed 1st sentence: SELECT DISTINCT var_uri WHERE brack_open var_x <dbo_product> <dbr_Postbanken> sep_dot var_x <dbp_products> var_uri sep_dot var_x <rdf_type> <dbo_Company> brack_close\n",
      "Rouge-L:  0.7283995887106266\n"
     ]
    }
   ],
   "source": [
    "# Install the rouge library using pip\n",
    "!pip install rouge > /dev/null\n",
    "\n",
    "# Copy the file \"compute-rouge-l.py\" from \"/content/drive/MyDrive/\" to the current directory\n",
    "!cp /content/drive/MyDrive/compute-rouge-l.py ./\n",
    "\n",
    "# Evaluate the translation using Rouge-L score calculation script\n",
    "# - The first argument is the path to the reference (gold standard) sparql file\n",
    "# - The second argument is the path to the translated sparql file\n",
    "!python compute-rouge-l.py /content/nmt/lcquad/test.sparql /content/nmt/lcquad/trans_test.sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 97848,
     "status": "ok",
     "timestamp": 1688926311974,
     "user": {
      "displayName": "Honghao Qiu",
      "userId": "12770867871229673910"
     },
     "user_tz": -60
    },
    "id": "PV4ziD4Ku6Za"
   },
   "outputs": [],
   "source": [
    "# Copy the directory 'nmt' and its contents from '/content/nmt' to '/content/drive/MyDrive'\n",
    "!cp -r /content/nmt /content/drive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L_nKJQMr2nO"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu7lspbw1Gxw"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ''' ''',\n",
    "    ''' '''\n",
    "]\n",
    "with open('questions.en', 'w') as fp:\n",
    "    t = [''.join(x) for x in sentences]\n",
    "    t = '\\n'.join(t)\n",
    "    fp.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_translate --model '/content/nmt/nmtmodel/model_step_1200.pt' --src questions.en --output pred.sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat pred.sparql"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOh1aUsRSXTx2hQv9hC3wIt",
   "gpuType": "T4",
   "mount_file_id": "11WBXhQhUEbHJhHMLIEZleBFcrRhUEGPc",
   "provenance": [
    {
     "file_id": "1qwBCXsrX0UD96jZU-P9vIhFRTnCZYO6m",
     "timestamp": 1686823688936
    },
    {
     "file_id": "1sYi4GQuLKBmhhiOJvNHDacHYPI39LsG-",
     "timestamp": 1686759059262
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
